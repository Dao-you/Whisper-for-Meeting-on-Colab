 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Dao-you/Whisper-for-Meeting-on-Colab/blob/main/Whisper_for_Meeting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  {
   "cell_type": "code",
   "metadata": {
    "id": "95849644-392d-48b8-8058-d84b741259a3",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =========================================================\n",
    "# Whisper Automatic Subtitle Generation: GPU Transcription + CPU Denoising + OpenCC Post-processing (Traditional/Simplified Conversion)\n",
    "# And LLM Summarization (GPT-OSS-20B / llama.cpp / CUDA)\n",
    "# - Transcription: faster-whisper (CUDA, compute: int8_float16\u2192float16\u2192int8)\n",
    "# - Denoising: ffmpeg afftdn (CPU)\n",
    "# - Progress: Real-time printing of \"current sentence + video total length percentage\"\n",
    "# - Network source download and output: MyDrive/whisper; Files in Drive: Output to the same folder\n",
    "# - LLM Summary: llama.cpp + GPT-OSS-20B GGUF for summarizing transcription\n",
    "# - Prompts \"Delete runtime and restart\" if download is blocked or abnormal\n",
    "# =========================================================\n",
    "\n",
    "# Restrict multithreading (more stable)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# [1/8] Mount Google Drive\n",
    "from google.colab import drive\n",
    "try:\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "except:\n",
    "    drive.mount(\"/content/gdrive\", force_remount=True)\n",
    "\n",
    "# Consolidated Imports\n",
    "import sys, gc, shutil, datetime, subprocess as sp\n",
    "from pathlib import Path\n",
    "import re, math, time, importlib, textwrap\n",
    "from typing import List, Tuple\n",
    "from IPython.display import display, Markdown\n",
    "import soundfile as sf\n",
    "from faster_whisper import WhisperModel\n",
    "from opencc import OpenCC\n",
    "import srt as _srt # Import srt as _srt to avoid name conflict later with the module itself\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "ROOT = Path(\"/content/gdrive/MyDrive\")\n",
    "WHISPER_DIR = ROOT / \"whisper\"\n",
    "WHISPER_DIR.mkdir(exist_ok=True, parents=True)\n",
    "os.chdir(ROOT)\n",
    "print(f\"\u2192 \u7576\u524d\u5de5\u4f5c\u76ee\u9304\uff1a{os.getcwd()}\")\n",
    "\n",
    "# [2/8] User Form Parameters (Unified)\n",
    "#@markdown # Whisper Transcription & LLM Summary Pipeline\n",
    "\n",
    "#@markdown ## Input & Transcription Settings\n",
    "#@markdown **Input Source:** Google Drive file (relative to MyDrive) or video URL (YouTube/HTTP).\n",
    "filename = \"whisper/jcz-mfkq-frc (2025-08-08 10_00 GMT+8).mp4\"  #@param {type:\"string\"}\n",
    "#@markdown **Download Option:** Check to save network source files to `MyDrive/whisper`.\n",
    "save_video_to_google_drive = True  #@param {type:\"boolean\"}\n",
    "#@markdown **Whisper Model Size:** Choose a model size. `large-v3` requires more GPU VRAM; `medium` is a good alternative if VRAM is limited.\n",
    "model_size = \"medium\"  #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v2\", \"large-v3\"] # Changed model_size to \"medium\"\n",
    "#@markdown **Language:** Select the language for transcription. \"\u81ea\u52d5\u5075\u6e2c\" (Auto-detect) is usually sufficient.\n",
    "language = \"\u81ea\u52d5\u5075\u6e2c\"  #@param [\"\u81ea\u52d5\u5075\u6e2c\", \"\u4e2d\u6587\", \"\u82f1\u6587\"]\n",
    "#@markdown **Denoising:** Apply CPU-based denoising to the audio before transcription. `afftdn` is recommended.\n",
    "denoise_method = \"afftdn (\u5efa\u8b70)\"  #@param [\"afftdn (\u5efa\u8b70)\", \"none\"]\n",
    "#@markdown **Text Post-processing (OpenCC):** Convert the transcribed text (SRT/TXT output) between Simplified and Traditional Chinese variants.\n",
    "text_postprocess = \"\u81fa\u7063\u7e41\u9ad4\u4e2d\u6587\uff08\u9810\u8a2d\uff09\"  #@param [\"\u81fa\u7063\u7e41\u9ad4\u4e2d\u6587\uff08\u9810\u8a2d\uff09\",\"\u9999\u6e2f\u7e41\u9ad4\u4e2d\u6587\",\"\u5927\u9678\u7c21\u9ad4\u4e2d\u6587\",\"\u95dc\u9589\"]\n",
    "#@markdown **YouTube Cookies (Optional):** Path to a Netscape-format cookies file (relative to MyDrive) for accessing age-restricted or member-only YouTube videos (e.g., `cookies/youtube.txt`).\n",
    "youtube_cookies_txt_path = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ## Summarization Settings\n",
    "#@markdown **Topic Hint (Optional):** Provide a brief hint about the topic to guide the summarization process.\n",
    "topic_hint = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "language_code_map = {\"\u81ea\u52d5\u5075\u6e2c\": None, \"\u4e2d\u6587\":\"zh\", \"\u82f1\u6587\":\"en\"}\n",
    "language_code = language_code_map[language]\n",
    "\n",
    "# =========================================================\n",
    "# Developer Options\n",
    "# Advanced users can fine-tune parameters in this section.\n",
    "# Modify only if you understand the impact.\n",
    "# =========================================================\n",
    "DEBUG_MODE = False # Set to True for more detailed logging\n",
    "\n",
    "# --- Transcription Parameters ---\n",
    "TRANSCRIPTION_BEAM_SIZE_PRIMARY = 3\n",
    "TRANSCRIPTION_CHUNK_LENGTH_PRIMARY = 20\n",
    "TRANSCRIPTION_BEAM_SIZE_FALLBACK = 1 # Used if primary fails\n",
    "TRANSCRIPTION_CHUNK_LENGTH_FALLBACK = 15 # Used if primary fails\n",
    "\n",
    "# --- Denoising Parameters ---\n",
    "DENOISE_NOISE_FLOOR_DB = -25\n",
    "\n",
    "# --- Filtering Parameters ---\n",
    "FILTER_MIN_DURATION_SHORT = 1.5 # Minimum duration for short segments\n",
    "FILTER_AVG_LOGPROB_THRESHOLD = -1.0 # Avg log probability threshold for short segments\n",
    "FILTER_MIN_DURATION_SPEECH_PROB = 2.0 # Minimum duration for speech probability filtering\n",
    "FILTER_NO_SPEECH_PROB_THRESHOLD = 0.6 # No speech probability threshold\n",
    "\n",
    "# --- Summary Model Parameters ---\n",
    "REPO_ID   = \"unsloth/gpt-oss-20b-GGUF\"   # GGUF Model Repository\n",
    "GGUF_FILE = \"gpt-oss-20b-Q4_K_M.gguf\"    # Approx. 10.8GiB, T4 can run\n",
    "\n",
    "# --- Summary Inference Parameters (Increase available generation space to avoid truncation) ---\n",
    "ctx_window            = 8192\n",
    "map_max_new_tokens    = 512   # Segment output: original 256 -> 512 (approx. 350-450 chars)\n",
    "reduce_max_new_tokens = 1024  # Summary output: original 512 -> 1024 (approx. 700-900+ chars)\n",
    "temperature           = 0.2\n",
    "top_p                 = 0.9\n",
    "repeat_penalty        = 1.05\n",
    "# =========================================================\n",
    "# End of Developer Options\n",
    "# =========================================================\n",
    "\n",
    "\n",
    "# [3/8] Install Dependencies\n",
    "# Combine installation steps from both original cells\n",
    "if DEBUG_MODE: print(\"[Install] faster-whisper / yt-dlp / soundfile / opencc / srt / huggingface_hub / llama-cpp-python ...\")\n",
    "\n",
    "def pip_install(pkgs, extra_args=None, env=None):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\"]\n",
    "    if extra_args:\n",
    "        cmd += extra_args\n",
    "    cmd += pkgs\n",
    "    return sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True, env=env)\n",
    "\n",
    "# Install common dependencies first\n",
    "common_missing = []\n",
    "try: import srt # check srt module directly after import as _srt\n",
    "except ModuleNotFoundError: common_missing.append(\"srt>=3.5.3\")\n",
    "try: from huggingface_hub import snapshot_download # check huggingface_hub module directly\n",
    "except ModuleNotFoundError: common_missing.append(\"huggingface_hub>=0.23.0\")\n",
    "try: import soundfile # check soundfile\n",
    "except ModuleNotFoundError: common_missing.append(\"soundfile\")\n",
    "try: import opencc # check opencc\n",
    "except ModuleNotFoundError: common_missing.append(\"opencc-python-reimplemented\")\n",
    "\n",
    "if common_missing:\n",
    "    if DEBUG_MODE: print(\"\u2192 Installing common missing packages:\", \", \".join(common_missing))\n",
    "    r = pip_install(common_missing)\n",
    "    if r.returncode != 0:\n",
    "        if DEBUG_MODE: print(r.stdout)\n",
    "        raise RuntimeError(\"\u57fa\u790e\u4f9d\u8cf4\u5b89\u88dd\u5931\u6557\uff0c\u8acb\u91cd\u555f\u57f7\u884c\u968e\u6bb5\u5f8c\u91cd\u8a66\u3002\")\n",
    "\n",
    "# Install faster-whisper and yt-dlp separately as they were in the first cell\n",
    "try: from faster_whisper import WhisperModel # check faster_whisper\n",
    "except ModuleNotFoundError:\n",
    "    if DEBUG_MODE: print(\"\u2192 Installing missing package: faster-whisper yt-dlp\")\n",
    "    r = pip_install([\"faster-whisper\", \"yt-dlp\"])\n",
    "    if r.returncode != 0:\n",
    "        if DEBUG_MODE: print(r.stdout)\n",
    "        raise RuntimeError(\"faster-whisper / yt-dlp \u5b89\u88dd\u5931\u6557\uff0c\u8acb\u91cd\u555f\u57f7\u884c\u968e\u6bb5\u5f8c\u91cd\u8a66\u3002\")\n",
    "\n",
    "\n",
    "def suggest_runtime_reset():\n",
    "    print(\"\\n\ud83e\uddf9 \u5efa\u8b70\u52d5\u4f5c\uff08Colab\uff09\")\n",
    "    print(\"1) \u4f9d\u5e8f\uff1a\u300e\u57f7\u884c\u968e\u6bb5 Runtime\u300f \u2192 \u300e\u522a\u9664\u57f7\u884c\u968e\u6bb5/\u9084\u539f\u51fa\u5ee0\u8a2d\u5b9a Factory reset runtime\u300f\")\n",
    "    print(\"2) \u91cd\u65b0\u57f7\u884c\u672c Notebook\uff08\u5f9e\u639b\u8f09\u96f2\u7aef\u786c\u789f\u90a3\u683c\u958b\u59cb\uff09\\n\", flush=True)\n",
    "\n",
    "def run_cmd(cmd:list, check=True):\n",
    "    if DEBUG_MODE: print(\"  $\", \" \".join(cmd))\n",
    "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
    "    if DEBUG_MODE and p.stdout: sys.stdout.write(p.stdout)\n",
    "    if check and p.returncode != 0:\n",
    "        raise RuntimeError(f\"\u547d\u4ee4\u5931\u6557\uff1a{' '.join(cmd)}\")\n",
    "    return p\n",
    "\n",
    "def is_youtube_url(s:str)->bool:\n",
    "    return isinstance(s, str) and (\"youtu.be\" in s or \"youtube.com\" in s)\n",
    "def is_http_url(s:str)->bool:\n",
    "    return isinstance(s, str) and s.lower().startswith(\"http\")\n",
    "def to_abs_mydrive(p:str)->Path:\n",
    "    return (Path(p) if p.startswith(\"/\") else (ROOT / p)).resolve()\n",
    "def fmt_ts_srt(t:float)->str:\n",
    "    h = int(t//3600); m = int((t%3600)//60); s = t - h*3600 - m*60\n",
    "    return f\"{h:02d}:{m:02d}:{int(s):02d},{int(round((s-int(s))*1000)):03d}\"\n",
    "def verify_wav_ok(path: Path)->bool:\n",
    "    try:\n",
    "        info = sf.info(str(path))\n",
    "        return info.samplerate > 0 and info.channels in (1, 2)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# OpenCC converter setup\n",
    "def build_opencc_pipeline(choice:str):\n",
    "    if choice.startswith(\"\u81fa\u7063\"):\n",
    "        return [OpenCC('s2t'), OpenCC('t2tw')]\n",
    "    if choice.startswith(\"\u9999\u6e2f\"):\n",
    "        return [OpenCC('s2t'), OpenCC('t2hk')]\n",
    "    if choice.startswith(\"\u5927\u9678\"):\n",
    "        return [OpenCC('t2s')]\n",
    "    return []  # Disable\n",
    "\n",
    "def apply_opencc(text:str, pipeline)->str:\n",
    "    for cc in pipeline:\n",
    "        text = cc.convert(text)\n",
    "    return text\n",
    "\n",
    "def ytdl(yturl:str)->Path:\n",
    "    tmp = Path(\"/tmp/dl\"); tmp.mkdir(parents=True, exist_ok=True)\n",
    "    for x in tmp.glob(\"*\"):\n",
    "        try: x.unlink()\n",
    "        except: shutil.rmtree(x, ignore_errors=True)\n",
    "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    out = tmp / f\"downloaded_{ts}.mp4\"\n",
    "    if DEBUG_MODE: print(\"[Download] Getting YouTube video ...\")\n",
    "    # Use sp.run instead of subprocess.run directly\n",
    "    cmd = [\"yt-dlp\", \"-f\", \"mp4\", \"-o\", str(tmp / \"%(title)s.%(ext)s\")]\n",
    "    if youtube_cookies_txt_path.strip():\n",
    "        cookies_abs = to_abs_mydrive(youtube_cookies_txt_path.strip())\n",
    "        if cookies_abs.exists():\n",
    "            cmd += [\"--cookies\", str(cookies_abs)]\n",
    "        else:\n",
    "            if DEBUG_MODE: print(f\"\u26a0\ufe0f \u627e\u4e0d\u5230 cookies \u6a94\uff1a{cookies_abs}\uff08\u6539\u70ba\u4e0d\u5e36 cookies\uff09\")\n",
    "    cmd.append(yturl)\n",
    "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
    "    if DEBUG_MODE and p.stdout: sys.stdout.write(p.stdout)\n",
    "    if p.returncode != 0:\n",
    "        if \"Sign in to confirm\" in (p.stdout or \"\"):\n",
    "            print(\"\\n\u2757YouTube \u8981\u6c42\u767b\u5165/\u9a57\u8b49\uff0c\u8acb\u63d0\u4f9b cookies \u6216\u5148\u81ea\u884c\u4e0b\u8f09\u5230\u96f2\u7aef\u786c\u789f\u3002\")\n",
    "        print(\"\ud83d\udd04 \u82e5\u591a\u6b21\u5931\u6557\uff0c\u8acb\u522a\u9664\u57f7\u884c\u968e\u6bb5\u4e26\u91cd\u555f\u5f8c\u91cd\u8a66\u3002\")\n",
    "        suggest_runtime_reset()\n",
    "        raise RuntimeError(\"yt-dlp \u4e0b\u8f09\u5931\u6557\")\n",
    "    files = list(tmp.glob(\"*\"))\n",
    "    if not files:\n",
    "        print(\"\ud83d\udd04 \u4e0b\u8f09\u70ba\u7a7a\uff0c\u5efa\u8b70\u522a\u9664\u57f7\u884c\u968e\u6bb5\u518d\u91cd\u8a66\u3002\")\n",
    "        suggest_runtime_reset()\n",
    "        raise FileNotFoundError(\"YouTube \u4e0b\u8f09\u5931\u6557\uff1a/tmp/dl \u70ba\u7a7a\")\n",
    "    f = files[0]\n",
    "    if save_video_to_google_drive:\n",
    "        shutil.copy2(f, WHISPER_DIR / f.name)\n",
    "    return f\n",
    "\n",
    "def http_dl(url:str)->Path:\n",
    "    tmp = Path(\"/tmp/dl\"); tmp.mkdir(parents=True, exist_ok=True)\n",
    "    for x in tmp.glob(\"*\"):\n",
    "        try: x.unlink()\n",
    "        except: shutil.rmtree(x, ignore_errors=True)\n",
    "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    out = tmp / f\"downloaded_{ts}.mp4\"\n",
    "    if DEBUG_MODE: print(\"[Download] Getting HTTP(S) video ...\")\n",
    "    run_cmd([\"curl\", \"-L\", \"-o\", str(out), url])\n",
    "    if save_video_to_google_drive:\n",
    "        shutil.copy2(out, WHISPER_DIR / out.name)\n",
    "    return out\n",
    "\n",
    "# Extract audio: ffmpeg -> 16k/mono WAV\n",
    "def ffmpeg_extract_wav(in_path:Path, out_wav:Path, sr=16000):\n",
    "    cmd = [\"ffmpeg\",\"-y\",\"-i\",str(in_path),\"-vn\",\"-ac\",\"1\",\"-ar\",str(sr),\"-f\",\"wav\",str(out_wav)]\n",
    "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
    "    if p.returncode != 0:\n",
    "        if DEBUG_MODE: print(p.stdout)\n",
    "        raise RuntimeError(\"ffmpeg \u8f49 WAV \u5931\u6557\")\n",
    "\n",
    "# CPU Denoising: ffmpeg afftdn\n",
    "def ffmpeg_afftdn(in_wav: Path, out_wav: Path, noise_floor_db=DENOISE_NOISE_FLOOR_DB):\n",
    "    cmd = [\"ffmpeg\",\"-y\",\"-i\",str(in_wav),\"-af\",f\"afftdn=nf={noise_floor_db}\",\n",
    "           \"-ac\",\"1\",\"-ar\",\"16000\",\"-f\",\"wav\",str(out_wav)]\n",
    "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
    "    if p.returncode != 0:\n",
    "        if DEBUG_MODE: print(p.stdout)\n",
    "        raise RuntimeError(\"ffmpeg afftdn \u5931\u6557\")\n",
    "\n",
    "# Safeguard: Repack WAV header if format is strange\n",
    "def ffmpeg_repack_wav(in_wav: Path, out_wav: Path, sr=16000):\n",
    "    cmd = [\"ffmpeg\",\"-y\",\"-i\",str(in_wav),\"-acodec\",\"pcm_s16le\",\"-ac\",\"1\",\"-ar\",str(sr),str(out_wav)]\n",
    "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
    "    if p.returncode != 0:\n",
    "        if DEBUG_MODE: print(p.stdout)\n",
    "        raise RuntimeError(\"ffmpeg \u91cd\u5305 WAV \u5931\u6557\")\n",
    "\n",
    "# [4/8] Parse Source (Transcription) - Uses 'filename' and 'save_video_to_google_drive'\n",
    "if DEBUG_MODE: print(\"[4/8] Parsing input source ...\")\n",
    "try:\n",
    "    if is_youtube_url(filename):\n",
    "        src_path = ytdl(filename); out_base_dir = WHISPER_DIR\n",
    "    elif is_http_url(filename):\n",
    "        src_path = http_dl(filename); out_base_dir = WHISPER_DIR\n",
    "    else:\n",
    "        src_path = to_abs_mydrive(filename)\n",
    "        if not src_path.exists(): raise FileNotFoundError(f\"\u627e\u4e0d\u5230\u6a94\u6848\uff1a{src_path}\")\n",
    "        out_base_dir = src_path.parent\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u26d4 \u4f86\u6e90\u89e3\u6790/\u4e0b\u8f09\u5931\u6557\uff1a{e}\")\n",
    "    print(\"\ud83d\udd04 \u8acb\u522a\u9664\u57f7\u884c\u968e\u6bb5\u4e26\u91cd\u65b0\u555f\u52d5\u5f8c\u91cd\u8dd1\u3002\"); suggest_runtime_reset(); raise\n",
    "\n",
    "print(f\"\u2192 \u4f86\u6e90\u6a94\uff1a{src_path}\")\n",
    "print(f\"\u2192 \u8f38\u51fa\u8cc7\u6599\u593e\uff1a{out_base_dir}\")\n",
    "\n",
    "# [5/8] Extract Audio & CPU Denoising (Transcription) - Uses 'denoise_method' and 'DENOISE_NOISE_FLOOR_DB'\n",
    "AUDIO_16K = Path(\"/tmp/audio_16k.wav\")\n",
    "if DEBUG_MODE: print(\"[5/8] Extracting audio (ffmpeg \u2192 16k/mono WAV) ...\")\n",
    "ffmpeg_extract_wav(src_path, AUDIO_16K, sr=16000)\n",
    "\n",
    "if denoise_method.startswith(\"afftdn\"):\n",
    "    if DEBUG_MODE: print(\"[5.5/8] Denoising (ffmpeg afftdn, CPU) ...\")\n",
    "    DENOISED = Path(\"/tmp/audio_16k_denoised.wav\")\n",
    "    ffmpeg_afftdn(AUDIO_16K, DENOISED, noise_floor_db=DENOISE_NOISE_FLOOR_DB)\n",
    "    denoised_audio = DENOISED if verify_wav_ok(DENOISED) else AUDIO_16K\n",
    "else:\n",
    "    denoised_audio = AUDIO_16K\n",
    "\n",
    "if not verify_wav_ok(denoised_audio):\n",
    "    if DEBUG_MODE: print(\"  - \u97f3\u8a0a\u683c\u5f0f\u7570\u5e38\uff1b\u5617\u8a66\u91cd\u5305 WAV ...\")\n",
    "    FIXED = Path(\"/tmp/audio_16k_fixed.wav\")\n",
    "    ffmpeg_repack_wav(denoised_audio, FIXED, sr=16000)\n",
    "    denoised_audio = FIXED\n",
    "\n",
    "if DEBUG_MODE: print(f\"\u2192 \u6700\u7d42\u8f38\u5165\u97f3\u8a0a\uff1a{denoised_audio}\")\n",
    "\n",
    "# [6/8] Load faster-whisper (GPU enforced) - Uses 'model_size'\n",
    "if DEBUG_MODE: print(\"[6/8] Loading faster-whisper model (GPU) ...\")\n",
    "device = \"cuda\"  # Enforce GPU\n",
    "model = None; last_err = None\n",
    "for ctype in [\"int8_float16\", \"float16\", \"int8\"]:\n",
    "    try:\n",
    "        if DEBUG_MODE: print(f\"  - Trying compute_type={ctype}\")\n",
    "        model = WhisperModel(model_size, device=device, compute_type=ctype)\n",
    "        if DEBUG_MODE: print(\"  - Model loaded successfully\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "        if DEBUG_MODE: print(f\"  - Load failed: {e}\")\n",
    "if model is None:\n",
    "    print(\"\\n\u26d4 GPU \u6a21\u578b\u8f09\u5165\u5931\u6557\u3002\u8acb\u78ba\u8a8d\u300e\u8b8a\u66f4\u57f7\u884c\u968e\u6bb5\u985e\u578b\u300f\u9078\u4e86 GPU\uff08T4/A100\uff09\uff0c\u6216\u522a\u9664\u57f7\u884c\u968e\u6bb5\u5f8c\u91cd\u8a66\u3002\")\n",
    "    suggest_runtime_reset()\n",
    "    raise RuntimeError(f\"\u7121\u6cd5\u8f09\u5165\u6a21\u578b\uff1a{last_err}\")\n",
    "\n",
    "gc.collect()  # Clean up before transcription (safety)\n",
    "\n",
    "# [7/8] Transcribe (GPU; real-time progress per segment) - Uses 'language_code', 'TRANSCRIPTION_BEAM_SIZE_PRIMARY', 'TRANSCRIPTION_CHUNK_LENGTH_PRIMARY', 'TRANSCRIPTION_BEAM_SIZE_FALLBACK', 'TRANSCRIPTION_CHUNK_LENGTH_FALLBACK'\n",
    "if DEBUG_MODE: print(f\"[7/8] Starting transcription (GPU: beam={TRANSCRIPTION_BEAM_SIZE_PRIMARY} / chunk={TRANSCRIPTION_CHUNK_LENGTH_PRIMARY}s / no VAD) ...\")\n",
    "\n",
    "def transcribe_gpu(_beam=TRANSCRIPTION_BEAM_SIZE_PRIMARY, _chunk=TRANSCRIPTION_CHUNK_LENGTH_PRIMARY):\n",
    "    return model.transcribe(\n",
    "        str(denoised_audio),\n",
    "        task=\"transcribe\",\n",
    "        language=language_code,\n",
    "        temperature=0.0,\n",
    "        condition_on_previous_text=False,\n",
    "        compression_ratio_threshold=2.4,\n",
    "        log_prob_threshold=-1.0,\n",
    "        no_speech_threshold=0.6,\n",
    "        beam_size=_beam,\n",
    "        chunk_length=_chunk,\n",
    "        vad_filter=False,\n",
    "        word_timestamps=False\n",
    "    )\n",
    "\n",
    "try:\n",
    "    seg_iter, info = transcribe_gpu(_beam=TRANSCRIPTION_BEAM_SIZE_PRIMARY, _chunk=TRANSCRIPTION_CHUNK_LENGTH_PRIMARY)\n",
    "except Exception as e:\n",
    "    if DEBUG_MODE: print(f\"  - First transcription failed: {e}\\n    \u2192 Trying more conservative (beam={TRANSCRIPTION_BEAM_SIZE_FALLBACK}, chunk={TRANSCRIPTION_CHUNK_LENGTH_FALLBACK}) ...\")\n",
    "    seg_iter, info = transcribe_gpu(_beam=TRANSCRIPTION_BEAM_SIZE_FALLBACK, _chunk=TRANSCRIPTION_CHUNK_LENGTH_FALLBACK)\n",
    "\n",
    "# Display percentage based on total video duration\n",
    "duration = float(getattr(info, \"duration\", 0.0) or 0.0)\n",
    "if duration <= 0: duration = 1.0\n",
    "\n",
    "segments = []\n",
    "filtered = []\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    print(f\"  - Detected language: {getattr(info,'language','\u672a\u77e5')} (p={getattr(info,'language_probability',0):.2f})\")\n",
    "    print(f\"  - Audio length: {duration:.2f}s\")\n",
    "\n",
    "for s in seg_iter:\n",
    "    pct = int(min(100, round((s.end / duration) * 100)))\n",
    "    print(f\"[{pct:3d}%] {fmt_ts_srt(s.start)} \u2192 {fmt_ts_srt(s.end)}  {s.text.strip()}\", flush=True)\n",
    "    segments.append(s)\n",
    "\n",
    "    # Low confidence/high no-speech short segment filtering (no blacklist) - Uses FILTER_* parameters\n",
    "    keep = True\n",
    "    seg_dur = float(s.end - s.start)\n",
    "    if seg_dur < FILTER_MIN_DURATION_SHORT and getattr(s, \"avg_logprob\", None) is not None and s.avg_logprob < FILTER_AVG_LOGPROB_THRESHOLD:\n",
    "        keep = False\n",
    "    if seg_dur < FILTER_MIN_DURATION_SPEECH_PROB and getattr(s, \"no_speech_prob\", None) is not None and s.no_speech_prob > FILTER_NO_SPEECH_PROB_THRESHOLD:\n",
    "        keep = False\n",
    "    if keep:\n",
    "        filtered.append(s)\n",
    "\n",
    "if DEBUG_MODE: print(f\"  - Number of segments: Before filtering {len(segments)} \u2192 After filtering {len(filtered)}\")\n",
    "\n",
    "# ---- OpenCC Normalization (for output text) ---- - Uses 'text_postprocess'\n",
    "pipeline = build_opencc_pipeline(text_postprocess)\n",
    "def norm(txt: str) -> str:\n",
    "    return apply_opencc(txt, pipeline) if pipeline else txt\n",
    "\n",
    "# [8/8] Output (text after OpenCC) - Uses 'out_base_dir' (derived from 'filename')\n",
    "print(\"[8/8] \u8f38\u51fa SRT / TXT ...\")\n",
    "# Determine the output directory for transcription based on input type\n",
    "# If input is a network source, output to WHISPER_DIR\n",
    "# If input is a local file, output to the same directory as the input file\n",
    "if is_youtube_url(filename) or is_http_url(filename):\n",
    "    out_base_dir = WHISPER_DIR\n",
    "else:\n",
    "    src_path_abs = to_abs_mydrive(filename)\n",
    "    out_base_dir = src_path_abs.parent\n",
    "\n",
    "# Create the transcription output directory if it doesn't exist\n",
    "out_dir = out_base_dir\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Determine the stem from the original source file path\n",
    "stem = Path(src_path).stem\n",
    "SRT = out_dir / f\"{stem}.srt\"\n",
    "TXT = out_dir / f\"{stem}.txt\"\n",
    "\n",
    "with open(SRT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, s in enumerate(filtered, 1):\n",
    "        text_out = norm(s.text.strip())\n",
    "        f.write(f\"{i}\\n{fmt_ts_srt(s.start)} --> {fmt_ts_srt(s.end)}\\n{text_out}\\n\\n\")\n",
    "\n",
    "with open(TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in filtered:\n",
    "        f.write(norm(s.text.strip()) + \"\\n\")  # Each segment on a new line\n",
    "\n",
    "print(f\"\u2192 \u5b8c\u6210\uff01\\n  SRT: {SRT}\\n  TXT: {TXT}\")\n",
    "\n",
    "# Release model (release GPU memory)\n",
    "try: del model\n",
    "except: pass\n",
    "gc.collect()\n",
    "if DEBUG_MODE: print(\"\u2192 Model released; can run again directly if needed.\")\n",
    "\n",
    "\n",
    "# ===== Summarization Logic Starts Here =====\n",
    "# Use SRT from transcription step for summarization\n",
    "summary_srt_path_abs = SRT\n",
    "assert summary_srt_path_abs.exists(), f\"SRT \u6a94\u4e0d\u5b58\u5728\uff1a{summary_srt_path_abs}\"\n",
    "\n",
    "# ===== Summary 1/6) Check GPU and Install Dependencies (llama-cpp-python specific) =====\n",
    "# llama-cpp-python installation logic - Keep this separate as it has specific CUDA requirements\n",
    "# Moved this section to just before reading the SRT for summarization\n",
    "if DEBUG_MODE: print(\"[Summary 1/6] Checking GPU and installing llama-cpp-python ...\")\n",
    "\n",
    "def detect_cuda_tag():\n",
    "    try:\n",
    "        out = sp.check_output([\"nvidia-smi\"], text=True)\n",
    "        m = re.search(r\"CUDA Version:\\s*([\\d.]+)\", out)\n",
    "        if not m:\n",
    "            return \"cu124\"\n",
    "        major, minor = [int(x) for x in m.group(1).split(\".\")[:2]]\n",
    "        if major > 12 or (major == 12 and minor >= 5):\n",
    "            return \"cu125\"\n",
    "        return \"cu124\"\n",
    "    except Exception:\n",
    "        return \"cu124\"\n",
    "\n",
    "cuda_tag = detect_cuda_tag()\n",
    "if DEBUG_MODE: print(f\"GPU 0: Detected CUDA version tag {cuda_tag}\")\n",
    "\n",
    "def try_import_llama():\n",
    "    try:\n",
    "        from llama_cpp import Llama\n",
    "        return Llama\n",
    "    except ModuleNotFoundError:\n",
    "        return None\n",
    "\n",
    "Llama = try_import_llama()\n",
    "if Llama is None:\n",
    "    # Keep your existing installation strategy: extra-index -> fallback to source compilation on failure\n",
    "    candidates = [cuda_tag, \"cu125\", \"cu124\", \"cu122\", \"cu121\"]\n",
    "    ok = False\n",
    "    for tag in candidates:\n",
    "        idx = f\"https://abetlen.github.io/llama-cpp-python/whl/{tag}\"\n",
    "        if DEBUG_MODE: print(f\"\u2192 Attempting to install llama-cpp-python ({tag}) ...\")\n",
    "        r = pip_install([\"llama-cpp-python\"], extra_args=[\"--extra-index-url\", idx])\n",
    "        if r.returncode == 0:\n",
    "            Llama = try_import_llama()\n",
    "            if Llama is not None:\n",
    "                ok = True\n",
    "                break\n",
    "        else:\n",
    "            if DEBUG_MODE: print(\"  \u2717 Installation failed (summary):\", \"\\n\".join(r.stdout.splitlines()[-5:]))\n",
    "    if not ok:\n",
    "        if DEBUG_MODE: print(\"\u2192 Pre-compiled wheels not available, switching to 'source compilation (CUDA=ON)' ... (takes longer)\")\n",
    "        try:\n",
    "            import ninja # noqa: F401 # Import ninja to check if installed\n",
    "        except ModuleNotFoundError:\n",
    "            if DEBUG_MODE: print(\"\u2192 Installing missing package: ninja\")\n",
    "            r = pip_install([\"ninja\"])\n",
    "            if r.returncode != 0:\n",
    "                if DEBUG_MODE: print(r.stdout)\n",
    "                raise RuntimeError(\"\u5b89\u88dd ninja \u5931\u6557\u3002\u8acb\u91cd\u555f\u5f8c\u91cd\u8a66\u3002\")\n",
    "        env = os.environ.copy()\n",
    "        env[\"CMAKE_ARGS\"] = \"-DGGML_CUDA=on -DLLAMA_CUBLAS=on\"\n",
    "        env[\"FORCE_CMAKE\"] = \"1\"\n",
    "        r = pip_install([\"llama-cpp-python\"], env=env)\n",
    "        if r.returncode != 0:\n",
    "            if DEBUG_MODE: print(r.stdout)\n",
    "            raise RuntimeError(\"\u7121\u6cd5\u5b89\u88dd GPU \u7248 llama-cpp-python\u3002\")\n",
    "        Llama = try_import_llama()\n",
    "\n",
    "\n",
    "if DEBUG_MODE: print(\"[Summary 2/6] Reading SRT ...\")\n",
    "with open(summary_srt_path_abs, \"r\", encoding=\"utf-8\") as f:\n",
    "    srt_text = f.read()\n",
    "subs = list(_srt.parse(srt_text)) # Use _srt as srt module was imported as _srt\n",
    "def td2s(td): return td.total_seconds()\n",
    "segments = []\n",
    "for it in subs:\n",
    "    txt = it.content.strip()\n",
    "    if not txt: continue\n",
    "    segments.append((td2s(it.start), td2s(it.end), txt))\n",
    "total_secs = (segments[-1][1] - segments[0][0]) if segments else 0\n",
    "if DEBUG_MODE: print(f\"\u2192 Number of subtitle segments: {len(segments)}\uff1bVideo length (est): {total_secs/60:.1f} minutes\")\n",
    "\n",
    "\n",
    "# ===== Summary 3/6) Download and Load GGUF Model (Summary) - Uses summary model parameters (REPO_ID, GGUF_FILE, ctx_window, etc.)\n",
    "# Moved this section to just after installing llama-cpp-python\n",
    "if DEBUG_MODE: print(\"[Summary 3/6] Loading GPT-OSS-20B (GGUF, CUDA) ...\")\n",
    "local_repo = snapshot_download(REPO_ID, allow_patterns=[GGUF_FILE])\n",
    "gguf_path = str(Path(local_repo)/GGUF_FILE)\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=gguf_path,\n",
    "    n_ctx=ctx_window,\n",
    "    n_gpu_layers=-1,\n",
    "    seed=0,\n",
    "    logits_all=False,\n",
    "    verbose=True,          # Display the actual chat format used\n",
    "    chat_format=\"chatml\",  # Directly override the GGUF built-in Unsloth template to avoid outputting <|channel|> tags\n",
    ")\n",
    "if DEBUG_MODE: print(\"\u2192 Model loaded successfully (GPU)\")\n",
    "\n",
    "\n",
    "# ===== Summary 4/6) Token-aware Segmentation (Summary) - Uses ctx_window, map_max_new_tokens, prompt_overhead\n",
    "if DEBUG_MODE: print(\"[Summary 4/6] Generating segments (token-aware; single segment \u2264 safety limit) ...\")\n",
    "\n",
    "def count_tokens_text(text: str) -> int:\n",
    "    # Check if llm is initialized before using it\n",
    "    if 'llm' not in locals() or llm is None:\n",
    "         raise RuntimeError(\"LLM model is not loaded. Cannot count tokens.\")\n",
    "    return len(llm.tokenize(text.encode(\"utf-8\")))\n",
    "\n",
    "SYSTEM_INSTR = (\n",
    "  \"\u4f60\u662f\u4e00\u500b\u6703\u8b70\u7e3d\u7d50\u6a5f\u5668\u4eba\u3002\u6839\u64da\u4f7f\u7528\u8005\u63d0\u4f9b\u7684\u9010\u5b57\u7a3f\uff08\u53ef\u80fd\u96dc\u8a0a\u3001\u91cd\u8907\u3001\u932f\u5b57\uff09\uff0c\"\n",
    "  \"\u8acb\u53bb\u9664\u96dc\u8a0a\u8207\u91cd\u8907\u3001\u56b4\u5b88\u4e8b\u5be6\u3001\u4e0d\u8166\u88dc\u3002\u9047\u5230\u4e0d\u660e\u78ba\u8cc7\u8a0a\u4ee5\u300c\u5f85\u88dc\u5145\uff0f\u672a\u660e\u78ba\u300d\u6a19\u8a3b\u3002\"\n",
    "  \"\u8f38\u51fa\u70ba Markdown\uff08\u7e41\u9ad4\u4e2d\u6587\uff09\uff0c\u4e0d\u8981\u8f38\u51fa\u4efb\u4f55\u7cfb\u7d71\uff0f\u601d\u8003\u6a19\u8a18\u3002\"\n",
    ")\n",
    "\n",
    "# \u2014 Segment Summary Prompt: More concise request, avoid verbosity and system language - Uses 'topic_hint'\n",
    "MAP_USER_TMPL = textwrap.dedent(\"\"\"\\\n",
    "\u4e3b\u984c\uff08\u53ef\u7559\u7a7a\uff09\uff1a{topic}\n",
    "\n",
    "\u4ee5\u4e0b\u662f\u9010\u5b57\u7a3f\u7247\u6bb5\uff08\u975e\u5b8c\u6574\u5168\u6587\uff09\uff1a\n",
    "{chunk}\n",
    "\n",
    "\u8acb\u5c31\u6b64\u7247\u6bb5\u8f38\u51fa\u300c\u689d\u5217\u5f0f\u91cd\u9ede\u6458\u8981\u300d\uff08500\u2013900 \u5b57\uff0c\u7e41\u9ad4\u4e2d\u6587\uff09\uff0c\u6ce8\u610f\uff1a\n",
    "- \u53ea\u5beb\u6700\u7d42\u5167\u5bb9\uff0c\u4e0d\u8981\u5beb\u89e3\u984c\u60f3\u6cd5\u3001\u4e0d\u8981\u51fa\u73fe\u4efb\u4f55\u7cfb\u7d71\u63d0\u793a\u6216\u4e2d\u82f1\u62ec\u865f\u6a19\u8a18\u3002\n",
    "- \u805a\u7126\u53ef\u9a57\u8b49\u4e8b\u5be6\uff08\u6642\u9593\u3001\u4eba\u7269\u3001\u4efb\u52d9\u3001\u7d50\u8ad6\u3001\u672a\u6c7a\u4e8b\u9805\u3001\u884c\u52d5\uff09\u3002\n",
    "- \u7d50\u69cb\uff1a\u53ef\u7528\u5c0f\u6a19\u984c\uff0b\u9805\u76ee\u7b26\u865f\uff0c\u8a9e\u53e5\u52d9\u5fc5\u77ed\u3001\u6e96\u78ba\u3001\u7121\u8d05\u8a5e\u3002\n",
    "\"\"\")\n",
    "\n",
    "# \u2014 Summary Prompt: Maintain your three-section output structure - Uses 'topic_hint'\n",
    "REDUCE_USER_TMPL = textwrap.dedent(\"\"\"\\\n",
    "\u4e3b\u984c\uff08\u53ef\u7559\u7a7a\uff09\uff1a{topic}\n",
    "\n",
    "\u4ee5\u4e0b\u662f\u6240\u6709\u7247\u6bb5\u7684\u91cd\u9ede\u6458\u8981\u5f59\u6574\uff08\u4ecd\u53ef\u80fd\u6709\u91cd\u758a\uff09\uff1a\n",
    "{maps}\n",
    "\n",
    "\u8acb\u6574\u5408\u70ba\u4e00\u4efd\u6703\u8b70\u7b46\u8a18\uff08Markdown\uff0c\u7e41\u9ad4\uff09\uff1a\n",
    "1) **\u6574\u9ad4\u63d0\u8981**\uff083\u20136 \u53e5\uff0c\u907f\u514d\u5197\u8a00\uff09\n",
    "2) **\u7ae0\u7bc0\u8981\u9ede\uff08\u542b\u6642\u9593\u8108\u7d61\uff09**\uff1a\u689d\u5217\u5448\u73fe\uff0c\u6bcf\u9ede\u4e00\u884c\uff0c\u53ef\u9644\u7c97\u7565\u6642\u9593\n",
    "3) **\u53ef\u57f7\u884c\u91cd\u9ede**\uff1a\u5177\u9ad4\u5f85\u8fa6\uff08\u6bcf\u689d\u4ee5\u52d5\u8a5e\u958b\u982d\uff09\n",
    "\u8acb\u53ea\u8f38\u51fa\u6700\u7d42\u7b46\u8a18\uff0c\u4e0d\u8981\u51fa\u73fe\u7cfb\u7d71\u6216\u601d\u8003\u6a19\u8a18\uff0c\u4e0d\u8981\u52a0\u5165\u672a\u51fa\u73fe\u7684\u65b0\u8cc7\u8a0a\u3002\n",
    "\"\"\")\n",
    "\n",
    "# Single segment token budget (reserve space for prompt and generation)\n",
    "prompt_overhead = 700\n",
    "chunk_target    = max(1024, min(3072, ctx_window - prompt_overhead - map_max_new_tokens))\n",
    "\n",
    "chunks: List[Tuple[float,float,str]] = []\n",
    "buf, t0, t1, cur = [], None, None, 0\n",
    "for (s, e, txt) in segments:\n",
    "    t = count_tokens_text(txt)\n",
    "    if not buf:\n",
    "        buf, t0, t1, cur = [txt], s, e, t\n",
    "        continue\n",
    "    if cur + t <= chunk_target:\n",
    "        buf.append(txt); t1 = e; cur += t\n",
    "    else:\n",
    "        chunks.append((t0, t1, \"\\n\".join(buf)))\n",
    "        buf, t0, t1, cur = [txt], s, e, t\n",
    "if buf:\n",
    "    chunks.append((t0, t1, \"\\n\".join(buf)))\n",
    "\n",
    "if DEBUG_MODE: print(f\"\u2192 Generated {len(chunks)} segments (target ~{chunk_target} tokens per segment)\")\n",
    "\n",
    "# ===== Common: Streaming Tools (No regex cleaning; use correct stop sequence) - Uses temperature, top_p, repeat_penalty, map_max_new_tokens, reduce_max_new_tokens\n",
    "def llm_stream(messages, max_tokens):\n",
    "    # Check if llm is initialized before using it\n",
    "    if 'llm' not in locals() or llm is None:\n",
    "         raise RuntimeError(\"LLM model is not loaded. Cannot stream generation.\")\n",
    "    # ChatML messages end with <|im_end|>; use stop to cut off, preventing the closing tag from being written to the file\n",
    "    gen = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        temperature=float(temperature),\n",
    "        top_p=float(top_p),\n",
    "        repeat_penalty=float(repeat_penalty),\n",
    "        max_tokens=int(max_tokens),\n",
    "        stream=True,\n",
    "        stop=[\"<|im_end|>\"],  # Key: Prevent outputting the ending template\n",
    "    )\n",
    "    for ev in gen:\n",
    "        # Compatible with different fields\n",
    "        piece = \"\"\n",
    "        try:\n",
    "            piece = ev[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
    "        except Exception:\n",
    "            piece = ev[\"choices\"][0].get(\"text\", \"\")\n",
    "        if piece:\n",
    "            yield piece\n",
    "\n",
    "# ===== Summary 5/6) Segment Summary (map) - Uses map_max_new_tokens, ctx_window, prompt_overhead, topic_hint\n",
    "if DEBUG_MODE: print(\"[Summary 5/6] Segment summarization (map) ...\")\n",
    "live = display(Markdown(\"\"), display_id=True)\n",
    "maps: List[str] = []\n",
    "\n",
    "for i, (s, e, body) in enumerate(chunks, 1):\n",
    "    pct = i / max(len(chunks),1) * 100\n",
    "    sys.stdout.write(f\"  - \u8655\u7406\u5206\u6bb5 {i}/{len(chunks)}\uff08~{pct:.1f}%\uff09\\n\"); sys.stdout.flush()\n",
    "\n",
    "    # Shrink to safe budget before sending (prevent prompt+segment from exceeding window and causing model to terminate early)\n",
    "    budget_tokens = max(512, ctx_window - map_max_new_tokens - prompt_overhead)\n",
    "    def shrink_to_budget(text: str, budget_tokens: int) -> str:\n",
    "        cur = text\n",
    "        for _ in range(6):\n",
    "            if count_tokens_text(cur) <= budget_tokens:\n",
    "                return cur\n",
    "            keep = max(800, int(len(cur) * 0.85))\n",
    "            cur = cur[:keep]\n",
    "        return cur\n",
    "    body2 = shrink_to_budget(body, budget_tokens)\n",
    "\n",
    "    user_txt = MAP_USER_TMPL.format(topic=(topic_hint or \"\uff08\u7121\uff09\"), chunk=body2)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_INSTR},\n",
    "        {\"role\": \"user\",   \"content\": user_txt},\n",
    "    ]\n",
    "\n",
    "    part_buf = [] # Reset part_buf for each segment\n",
    "    for token in llm_stream(messages, map_max_new_tokens):\n",
    "        part_buf.append(token)\n",
    "        # Update live display and terminal character count periodically\n",
    "        if len(part_buf) % 24 == 0:\n",
    "            cur_txt = \"\".join(part_buf)\n",
    "            live.update(Markdown(cur_txt))\n",
    "            sys.stdout.write(f\"    \u21b3 \u5206\u6bb5 {i} \u5df2\u7522\u751f\u5b57\u5143\uff1a{len(cur_txt)}\\n\"); sys.stdout.flush()\n",
    "    cur_txt = \"\".join(part_buf)\n",
    "    live.update(Markdown(cur_txt))\n",
    "    sys.stdout.write(f\"    \u21b3 \u5206\u6bb5 {i} \u5df2\u7522\u751f\u5b57\u5143\uff1a{len(cur_txt)}\\n\"); sys.stdout.flush()\n",
    "\n",
    "    # Include the model's final output directly, no regex cleaning\n",
    "    maps.append(cur_txt.strip())\n",
    "\n",
    "if DEBUG_MODE: print(\"\u2192 Segment summarization complete\")\n",
    "\n",
    "if DEBUG_MODE: print(\"[Summary 6/6] Consolidating summary (reduce) ...\")\n",
    "maps_md = \"\\n\\n---\\n\\n\".join(f\"### \u7247\u6bb5 {i+1} \u8981\u9ede\\n\\n{m}\" for i, m in enumerate(maps))\n",
    "\n",
    "# If combined text exceeds window, truncate proportionally first (without changing text within segments to avoid breaking meaning)\n",
    "def fit_reduce_payload(md_text: str, max_ctx_tokens: int) -> str:\n",
    "    for _ in range(8):\n",
    "        need = count_tokens_text(md_text)\n",
    "        if need + reduce_max_new_tokens + 400 <= max_ctx_tokens:\n",
    "            return md_text\n",
    "        md_text = md_text[: int(len(md_text) * 0.9)]\n",
    "    return md_text\n",
    "\n",
    "md_cur = fit_reduce_payload(maps_md, ctx_window)\n",
    "\n",
    "user_txt = REDUCE_USER_TMPL.format(topic=(topic_hint or \"\uff08\u7121\uff09\"), maps=md_cur)\n",
    "messages = [{\"role\":\"system\",\"content\":SYSTEM_INSTR},\n",
    "            {\"role\":\"user\",\"content\":user_txt}]\n",
    "\n",
    "live2 = display(Markdown(\"\"), display_id=True)\n",
    "final_buf = []\n",
    "for token in llm_stream(messages, reduce_max_new_tokens):\n",
    "    final_buf.append(token)\n",
    "    if len(final_buf) % 24 == 0:\n",
    "        live2.update(Markdown(\"\".join(final_buf)))\n",
    "        sys.stdout.write(f\"    \u21b3 \u5f59\u6574 \u5df2\u7522\u751f\u5b57\u5143\uff1a{len(''.join(final_buf))}\\n\"); sys.stdout.flush()\n",
    "live2.update(Markdown(\"\".join(final_buf)))\n",
    "sys.stdout.write(f\"    \u21b3 \u5f59\u6574 \u5df2\u7522\u751f\u5b57\u5143\uff1a{len(''.join(final_buf))}\\n\"); sys.stdout.flush()\n",
    "\n",
    "final_text = \"\".join(final_buf).strip()\n",
    "\n",
    "# Determine and create the summary output directory\n",
    "summary_output_dir_abs = out_base_dir\n",
    "summary_output_dir_abs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Determine the summary output file path using the stem of the input SRT\n",
    "out_md = summary_output_dir_abs / f\"{Path(summary_srt_path_abs).stem}_summary.md\"\n",
    "\n",
    "\n",
    "with open(out_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_text)\n",
    "\n",
    "print(f\"\u2192 \u5b8c\u6210 \u2705  {out_md}\")\n",
    "try:\n",
    "    del llm\n",
    "except Exception:\n",
    "    pass\n",
    "gc.collect()\n",
    "if DEBUG_MODE: print(\"\uff08\u986f\u5b58\u5df2\u91cb\u653e\uff0c\u5982\u9700\u91cd\u8dd1\u53ef\u76f4\u63a5\u518d\u6b21\u57f7\u884c\uff09\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2