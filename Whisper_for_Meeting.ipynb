{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Dao-you/Whisper-for-Meeting-on-Colab/blob/main/Whisper_for_Meeting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "95849644-392d-48b8-8058-d84b741259a3"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =========================================================\n",
    "# Whisper Automatic Subtitle Generation: GPU Transcription + CPU Denoising + OpenCC Post-processing (Traditional/Simplified Conversion)\n",
    "# And LLM Summarization (GPT-OSS-20B / llama.cpp / CUDA)\n",
    "# - Transcription: faster-whisper (CUDA, compute: int8_float16â†’float16â†’int8)\n",
    "# - Denoising: ffmpeg afftdn (CPU)\n",
    "# - Progress: Real-time printing of \"current sentence + video total length percentage\"\n",
    "# - Network source download and output: MyDrive/whisper; Files in Drive: Output to the same folder\n",
    "# - LLM Summary: llama.cpp + GPT-OSS-20B GGUF for summarizing transcription\n",
    "# - Prompts \"Delete runtime and restart\" if download is blocked or abnormal\n",
    "# =========================================================\n",
    "\n",
    "# Restrict multithreading (more stable)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# [1/8] Mount Google Drive\n",
    "from google.colab import drive\n",
    "print(\"è«‹æˆæ¬Šä»¥æ›è¼‰ Google Drive\")\n",
    "drive.mount(\"/content/gdrive\", force_remount=True)\n",
    "\n",
    "# Built-in Imports\n",
    "import sys, gc, shutil, datetime, subprocess as sp\n",
    "from pathlib import Path\n",
    "import re, math, time, textwrap\n",
    "from typing import List, Tuple, Optional, Iterable, Iterator\n",
    "from IPython.display import display, Markdown\n",
    "import json\n",
    "\n",
    "\n",
    "ROOT = Path(\"/content/gdrive/MyDrive\")\n",
    "WHISPER_DIR = ROOT / \"whisper\"\n",
    "WHISPER_DIR.mkdir(exist_ok=True, parents=True)\n",
    "os.chdir(ROOT)\n",
    "print(f\"â†’ ç•¶å‰å·¥ä½œç›®éŒ„ï¼š{os.getcwd()}\")\n",
    "\n",
    "# [2/8] User Form Parameters (Unified)\n",
    "#@markdown # Whisper Transcription & LLM Summary Pipeline\n",
    "\n",
    "#@markdown ## Input & Transcription Settings\n",
    "#@markdown **Input Source:** Google Drive file (relative to MyDrive) or video URL (YouTube/HTTP).\n",
    "filename = \"whisper/jcz-mfkq-frc (2025-08-08 10_00 GMT+8).mp4\"  #@param {type:\"string\"}\n",
    "#@markdown **Download Option:** Check to save network source files to `MyDrive/whisper`.\n",
    "save_video_to_google_drive = True  #@param {type:\"boolean\"}\n",
    "#@markdown **Whisper Model Size:** Choose a model size. `large-v3` requires more GPU VRAM; `medium` is a good alternative if VRAM is limited.\n",
    "model_size = \"medium\"  #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v2\", \"large-v3\"] # Changed model_size to \"medium\"\n",
    "#@markdown **Language:** Select the language for transcription. \"è‡ªå‹•åµæ¸¬\" (Auto-detect) is usually sufficient.\n",
    "language = \"è‡ªå‹•åµæ¸¬\"  #@param [\"è‡ªå‹•åµæ¸¬\", \"ä¸­æ–‡\", \"è‹±æ–‡\"]\n",
    "#@markdown **Denoising:** Apply CPU-based denoising to the audio before transcription. `afftdn` is recommended.\n",
    "denoise_method = \"afftdn (å»ºè­°)\"  #@param [\"afftdn (å»ºè­°)\", \"none\"]\n",
    "#@markdown **Text Post-processing (OpenCC):** Convert the transcribed text (SRT/TXT output) between Simplified and Traditional Chinese variants.\n",
    "text_postprocess = \"è‡ºç£ç¹é«”ä¸­æ–‡ï¼ˆé è¨­ï¼‰\"  #@param [\"è‡ºç£ç¹é«”ä¸­æ–‡ï¼ˆé è¨­ï¼‰\",\"é¦™æ¸¯ç¹é«”ä¸­æ–‡\",\"å¤§é™¸ç°¡é«”ä¸­æ–‡\",\"é—œé–‰\"]\n",
    "#@markdown **YouTube Cookies (Optional):** Path to a Netscape-format cookies file (relative to MyDrive) for accessing age-restricted or member-only YouTube videos (e.g., `cookies/youtube.txt`).\n",
    "youtube_cookies_txt_path = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ## Summarization Settings\n",
    "#@markdown **Topic Hint (Optional):** Provide a brief hint about the topic to guide the summarization process.\n",
    "topic_hint = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "language_code_map = {\"è‡ªå‹•åµæ¸¬\": None, \"ä¸­æ–‡\":\"zh\", \"è‹±æ–‡\":\"en\"}\n",
    "language_code = language_code_map[language]\n",
    "\n",
    "# =========================================================\n",
    "# Developer Options\n",
    "# Advanced users can fine-tune parameters in this section.\n",
    "# Modify only if you understand the impact.\n",
    "# =========================================================\n",
    "DEBUG_MODE = False # Set to True for more detailed logging\n",
    "\n",
    "# --- Transcription Parameters ---\n",
    "TRANSCRIPTION_BEAM_SIZE_PRIMARY = 3\n",
    "TRANSCRIPTION_CHUNK_LENGTH_PRIMARY = 20\n",
    "TRANSCRIPTION_BEAM_SIZE_FALLBACK = 1 # Used if primary fails\n",
    "TRANSCRIPTION_CHUNK_LENGTH_FALLBACK = 15 # Used if primary fails\n",
    "\n",
    "# --- Denoising Parameters ---\n",
    "DENOISE_NOISE_FLOOR_DB = -25\n",
    "\n",
    "# --- Filtering Parameters ---\n",
    "FILTER_MIN_DURATION_SHORT = 1.5 # Minimum duration for short segments\n",
    "FILTER_AVG_LOGPROB_THRESHOLD = -1.0 # Avg log probability threshold for short segments\n",
    "FILTER_MIN_DURATION_SPEECH_PROB = 2.0 # Minimum duration for speech probability filtering\n",
    "FILTER_NO_SPEECH_PROB_THRESHOLD = 0.6 # No speech probability threshold\n",
    "\n",
    "# --- Summary Model Parameters ---\n",
    "REPO_ID   = \"unsloth/gpt-oss-20b-GGUF\"   # GGUF Model Repository\n",
    "GGUF_FILE = \"gpt-oss-20b-Q4_K_M.gguf\"    # Approx. 10.8GiB, T4 can run\n",
    "\n",
    "# --- Summary Inference Parameters (Increase available generation space to avoid truncation) ---\n",
    "ctx_window            = 8192\n",
    "map_max_new_tokens    = 512   # Segment output: original 256 -> 512 (approx. 350-450 chars)\n",
    "reduce_max_new_tokens = 1024  # Summary output: original 512 -> 1024 (approx. 700-900+ chars)\n",
    "temperature           = 0.2\n",
    "top_p                 = 0.9\n",
    "repeat_penalty        = 1.05\n",
    "\n",
    "tokenizer_config_data = None\n",
    "harmony_chat_formatter = None\n",
    "\n",
    "def ensure_harmony_formatter():\n",
    "    \"\"\"Ensure the Harmony chat formatter is available for GPT-OSS prompts.\"\"\"\n",
    "    global harmony_chat_formatter\n",
    "    if harmony_chat_formatter is not None:\n",
    "        return harmony_chat_formatter\n",
    "    try:\n",
    "        from llama_cpp.llama_chat_format import (\n",
    "            hf_tokenizer_config_to_chat_formatter,\n",
    "            Jinja2ChatFormatter,\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(\"llama_cpp Harmony chat helpers are unavailable\") from exc\n",
    "\n",
    "    formatter = None\n",
    "    if tokenizer_config_data:\n",
    "        try:\n",
    "            formatter = hf_tokenizer_config_to_chat_formatter(\n",
    "                tokenizer_config_data,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "        except Exception as exc:\n",
    "            if DEBUG_MODE:\n",
    "                print(\"  âœ— Failed to initialize Harmony formatter from tokenizer_config.json:\", exc)\n",
    "\n",
    "    if formatter is None:\n",
    "        template = None\n",
    "        if 'llm' in globals() and hasattr(llm, 'metadata'):\n",
    "            template = llm.metadata.get('tokenizer.chat_template')\n",
    "        if template:\n",
    "            try:\n",
    "                bos_token = (\n",
    "                    llm.metadata.get('tokenizer.bos_token')\n",
    "                    or llm.metadata.get('tokenizer.ggml.bos_token')\n",
    "                    or llm.detokenize([llm.token_bos()], special=True).decode('utf-8', errors='ignore')\n",
    "                )\n",
    "                eos_token = (\n",
    "                    llm.metadata.get('tokenizer.eos_token')\n",
    "                    or llm.metadata.get('tokenizer.ggml.eos_token')\n",
    "                    or llm.detokenize([llm.token_eos()], special=True).decode('utf-8', errors='ignore')\n",
    "                )\n",
    "                formatter = Jinja2ChatFormatter(\n",
    "                    template,\n",
    "                    eos_token=eos_token,\n",
    "                    bos_token=bos_token,\n",
    "                    stop_token_ids=[llm.token_eos()],\n",
    "                )\n",
    "            except Exception as exc:\n",
    "                if DEBUG_MODE:\n",
    "                    print(\"  âœ— Failed to build Harmony formatter from GGUF metadata:\", exc)\n",
    "\n",
    "    if formatter is None:\n",
    "        raise RuntimeError(\"Harmony chat formatter could not be prepared\")\n",
    "\n",
    "    harmony_chat_formatter = formatter\n",
    "    return harmony_chat_formatter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# End of Developer Options\n",
    "# =========================================================\n",
    "\n",
    "\n",
    "# [3/8] Install Dependencies\n",
    "# Combine installation steps from both original cells\n",
    "if DEBUG_MODE: print(\"[Install] faster-whisper / yt-dlp / soundfile / opencc / srt / huggingface_hub / llama-cpp-python ...\")\n",
    "\n",
    "def pip_install(pkgs, extra_args=None, env=None):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\"]\n",
    "    if extra_args:\n",
    "        cmd += extra_args\n",
    "    cmd += pkgs\n",
    "    return sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True, env=env)\n",
    "\n",
    "import importlib.util\n",
    "# Install common dependencies first\n",
    "common_missing = []\n",
    "if importlib.util.find_spec(\"srt\") is None:\n",
    "    common_missing.append(\"srt>=3.5.3\")\n",
    "if importlib.util.find_spec(\"huggingface_hub\") is None:\n",
    "    common_missing.append(\"huggingface_hub>=0.23.0\")\n",
    "if importlib.util.find_spec(\"soundfile\") is None:\n",
    "    common_missing.append(\"soundfile\")\n",
    "if importlib.util.find_spec(\"opencc\") is None:\n",
    "    common_missing.append(\"opencc-python-reimplemented\")\n",
    "if importlib.util.find_spec(\"jinja2\") is None:\n",
    "    common_missing.append(\"jinja2>=3.1.0\")\n",
    "\n",
    "if common_missing:\n",
    "    if DEBUG_MODE: print(\"â†’ Installing common missing packages:\", \", \".join(common_missing))\n",
    "    r = pip_install(common_missing)\n",
    "    if r.returncode != 0:\n",
    "        if DEBUG_MODE: print(r.stdout)\n",
    "        raise RuntimeError(\"åŸºç¤ä¾è³´å®‰è£å¤±æ•—ï¼Œè«‹é‡å•ŸåŸ·è¡Œéšæ®µå¾Œé‡è©¦ã€‚\")\n",
    "\n",
    "# Install faster-whisper and yt-dlp separately as they were in the first cell\n",
    "if importlib.util.find_spec(\"faster_whisper\") is None:\n",
    "    if DEBUG_MODE: print(\"â†’ Installing missing package: faster-whisper yt-dlp\")\n",
    "    r = pip_install([\"faster-whisper\", \"yt-dlp\"])\n",
    "    if r.returncode != 0:\n",
    "        if DEBUG_MODE: print(r.stdout)\n",
    "        raise RuntimeError(\"faster-whisper / yt-dlp å®‰è£å¤±æ•—ï¼Œè«‹é‡å•ŸåŸ·è¡Œéšæ®µå¾Œé‡è©¦ã€‚\")\n",
    "\n",
    "# Import external packages after ensuring installation\n",
    "import soundfile as sf\n",
    "from faster_whisper import WhisperModel\n",
    "from opencc import OpenCC\n",
    "import srt as _srt  # Import srt as _srt to avoid name conflict later with the module itself\n",
    "from huggingface_hub import snapshot_download\n",
    "def suggest_runtime_reset():\n",
    "    print(\"\\nğŸ§¹ å»ºè­°å‹•ä½œï¼ˆColabï¼‰\")\n",
    "    print(\"1) ä¾åºï¼šã€åŸ·è¡Œéšæ®µ Runtimeã€ â†’ ã€åˆªé™¤åŸ·è¡Œéšæ®µ/é‚„åŸå‡ºå» è¨­å®š Factory reset runtimeã€\")\n",
    "    print(\"2) é‡æ–°åŸ·è¡Œæœ¬ Notebookï¼ˆå¾æ›è¼‰é›²ç«¯ç¡¬ç¢Ÿé‚£æ ¼é–‹å§‹ï¼‰\\n\", flush=True)\n",
    "\n",
    "def run_cmd(cmd:list, check=True):\n",
    "    if DEBUG_MODE: print(\"  $\", \" \".join(cmd))\n",
    "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
    "    if DEBUG_MODE and p.stdout: sys.stdout.write(p.stdout)\n",
    "    if check and p.returncode != 0:\n",
    "        raise RuntimeError(f\"å‘½ä»¤å¤±æ•—ï¼š{' '.join(cmd)}\")\n",
    "    return p\n",
    "\n",
    "def is_youtube_url(s:str)->bool:\n",
    "    return isinstance(s, str) and (\"youtu.be\" in s or \"youtube.com\" in s)\n",
    "def is_http_url(s:str)->bool:\n",
    "    return isinstance(s, str) and s.lower().startswith(\"http\")\n",
    "def to_abs_mydrive(p:str)->Path:\n",
    "    return (Path(p) if p.startswith(\"/\") else (ROOT / p)).resolve()\n",
    "def fmt_ts_srt(t:float)->str:\n",
    "    h = int(t//3600); m = int((t%3600)//60); s = t - h*3600 - m*60\n",
    "    return f\"{h:02d}:{m:02d}:{int(s):02d},{int(round((s-int(s))*1000)):03d}\"\n",
    "def verify_wav_ok(path: Path)->bool:\n",
    "    try:\n",
    "        info = sf.info(str(path))\n",
    "        return info.samplerate > 0 and info.channels in (1, 2)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# OpenCC converter setup\n",
    "def build_opencc_pipeline(choice:str):\n",
    "    if choice.startswith(\"è‡ºç£\"):\n",
    "        return [OpenCC('s2t'), OpenCC('t2tw')]\n",
    "    if choice.startswith(\"é¦™æ¸¯\"):\n",
    "        return [OpenCC('s2t'), OpenCC('t2hk')]\n",
    "    if choice.startswith(\"å¤§é™¸\"):\n",
    "        return [OpenCC('t2s')]\n",
    "    return []  # Disable\n",
    "\n",
    "def apply_opencc(text:str, pipeline)->str:\n",
    "    for cc in pipeline:\n",
    "        text = cc.convert(text)\n",
    "    return text\n",
    "\n",
    "def ytdl(yturl:str)->Path:\n",
    "    tmp = Path(\"/tmp/dl\"); tmp.mkdir(parents=True, exist_ok=True)\n",
    "    for x in tmp.glob(\"*\"):\n",
    "        try: x.unlink()\n",
    "        except: shutil.rmtree(x, ignore_errors=True)\n",
    "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    out = tmp / f\"downloaded_{ts}.mp4\"\n",
    "    if DEBUG_MODE: print(\"[Download] Getting YouTube video ...\")\n",
    "    # Use sp.run instead of subprocess.run directly\n",
    "    cmd = [\"yt-dlp\", \"-f\", \"mp4\", \"-o\", str(tmp / \"%(title)s.%(ext)s\")]\n",
    "    if youtube_cookies_txt_path.strip():\n",
    "        cookies_abs = to_abs_mydrive(youtube_cookies_txt_path.strip())\n",
    "        if cookies_abs.exists():\n",
    "            cmd += [\"--cookies\", str(cookies_abs)]\n",
    "        else:\n",
    "            if DEBUG_MODE: print(f\"âš ï¸ æ‰¾ä¸åˆ° cookies æª”ï¼š{cookies_abs}ï¼ˆæ”¹ç‚ºä¸å¸¶ cookiesï¼‰\")\n",
    "    cmd.append(yturl)\n",
    "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
    "    if DEBUG_MODE and p.stdout: sys.stdout.write(p.stdout)\n",
    "    if p.returncode != 0:\n",
    "        if \"Sign in to confirm\" in (p.stdout or \"\"):\n",
    "            print(\"\\nâ—YouTube è¦æ±‚ç™»å…¥/é©—è­‰ï¼Œè«‹æä¾› cookies æˆ–å…ˆè‡ªè¡Œä¸‹è¼‰åˆ°é›²ç«¯ç¡¬ç¢Ÿã€‚\")\n",
    "        print(\"ğŸ”„ è‹¥å¤šæ¬¡å¤±æ•—ï¼Œè«‹åˆªé™¤åŸ·è¡Œéšæ®µä¸¦é‡å•Ÿå¾Œé‡è©¦ã€‚\")\n",
    "        suggest_runtime_reset()\n",
    "        raise RuntimeError(\"yt-dlp ä¸‹è¼‰å¤±æ•—\")\n",
    "    files = list(tmp.glob(\"*\"))\n",
    "    if not files:\n",
    "        print(\"ğŸ”„ ä¸‹è¼‰ç‚ºç©ºï¼Œå»ºè­°åˆªé™¤åŸ·è¡Œéšæ®µå†é‡è©¦ã€‚\")\n",
    "        suggest_runtime_reset()\n",
    "        raise FileNotFoundError(\"YouTube ä¸‹è¼‰å¤±æ•—ï¼š/tmp/dl ç‚ºç©º\")\n",
    "    f = files[0]\n",
    "    if save_video_to_google_drive:\n",
    "        shutil.copy2(f, WHISPER_DIR / f.name)\n",
    "    return f\n",
    "\n",
    "def http_dl(url:str)->Path:\n",
    "    tmp = Path(\"/tmp/dl\"); tmp.mkdir(parents=True, exist_ok=True)\n",
    "    for x in tmp.glob(\"*\"):\n",
    "        try: x.unlink()\n",
    "        except: shutil.rmtree(x, ignore_errors=True)\n",
    "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    out = tmp / f\"downloaded_{ts}.mp4\"\n",
    "    if DEBUG_MODE: print(\"[Download] Getting HTTP(S) video ...\")\n",
    "    run_cmd([\"curl\", \"-L\", \"-o\", str(out), url])\n",
    "    if save_video_to_google_drive:\n",
    "        shutil.copy2(out, WHISPER_DIR / out.name)\n",
    "    return out\n",
    "\n",
    "# Extract audio: ffmpeg -> 16k/mono WAV\n",
    "def ffmpeg_extract_wav(in_path:Path, out_wav:Path, sr=16000):\n",
    "    cmd = [\"ffmpeg\",\"-y\",\"-i\",str(in_path),\"-vn\",\"-ac\",\"1\",\"-ar\",str(sr),\"-f\",\"wav\",str(out_wav)]\n",
    "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
    "    if p.returncode != 0:\n",
    "        if DEBUG_MODE: print(p.stdout)\n",
    "        raise RuntimeError(\"ffmpeg è½‰ WAV å¤±æ•—\")\n",
    "\n",
    "# CPU Denoising: ffmpeg afftdn\n",
    "def ffmpeg_afftdn(in_wav: Path, out_wav: Path, noise_floor_db=DENOISE_NOISE_FLOOR_DB):\n",
    "    cmd = [\"ffmpeg\",\"-y\",\"-i\",str(in_wav),\"-af\",f\"afftdn=nf={noise_floor_db}\",\n",
    "           \"-ac\",\"1\",\"-ar\",\"16000\",\"-f\",\"wav\",str(out_wav)]\n",
    "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
    "    if p.returncode != 0:\n",
    "        if DEBUG_MODE: print(p.stdout)\n",
    "        raise RuntimeError(\"ffmpeg afftdn å¤±æ•—\")\n",
    "\n",
    "# Safeguard: Repack WAV header if format is strange\n",
    "def ffmpeg_repack_wav(in_wav: Path, out_wav: Path, sr=16000):\n",
    "    cmd = [\"ffmpeg\",\"-y\",\"-i\",str(in_wav),\"-acodec\",\"pcm_s16le\",\"-ac\",\"1\",\"-ar\",str(sr),str(out_wav)]\n",
    "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
    "    if p.returncode != 0:\n",
    "        if DEBUG_MODE: print(p.stdout)\n",
    "        raise RuntimeError(\"ffmpeg é‡åŒ… WAV å¤±æ•—\")\n",
    "\n",
    "# [4/8] Parse Source (Transcription) - Uses 'filename' and 'save_video_to_google_drive'\n",
    "if DEBUG_MODE: print(\"[4/8] Parsing input source ...\")\n",
    "try:\n",
    "    if is_youtube_url(filename):\n",
    "        src_path = ytdl(filename); out_base_dir = WHISPER_DIR\n",
    "    elif is_http_url(filename):\n",
    "        src_path = http_dl(filename); out_base_dir = WHISPER_DIR\n",
    "    else:\n",
    "        src_path = to_abs_mydrive(filename)\n",
    "        if not src_path.exists(): raise FileNotFoundError(f\"æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{src_path}\")\n",
    "        out_base_dir = src_path.parent\n",
    "except Exception as e:\n",
    "    print(f\"\\nâ›” ä¾†æºè§£æ/ä¸‹è¼‰å¤±æ•—ï¼š{e}\")\n",
    "    print(\"ğŸ”„ è«‹åˆªé™¤åŸ·è¡Œéšæ®µä¸¦é‡æ–°å•Ÿå‹•å¾Œé‡è·‘ã€‚\"); suggest_runtime_reset(); raise\n",
    "\n",
    "print(f\"â†’ ä¾†æºæª”ï¼š{src_path}\")\n",
    "print(f\"â†’ è¼¸å‡ºè³‡æ–™å¤¾ï¼š{out_base_dir}\")\n",
    "\n",
    "# [5/8] Extract Audio & CPU Denoising (Transcription) - Uses 'denoise_method' and 'DENOISE_NOISE_FLOOR_DB'\n",
    "AUDIO_16K = Path(\"/tmp/audio_16k.wav\")\n",
    "if DEBUG_MODE: print(\"[5/8] Extracting audio (ffmpeg â†’ 16k/mono WAV) ...\")\n",
    "ffmpeg_extract_wav(src_path, AUDIO_16K, sr=16000)\n",
    "\n",
    "if denoise_method.startswith(\"afftdn\"):\n",
    "    if DEBUG_MODE: print(\"[5.5/8] Denoising (ffmpeg afftdn, CPU) ...\")\n",
    "    DENOISED = Path(\"/tmp/audio_16k_denoised.wav\")\n",
    "    ffmpeg_afftdn(AUDIO_16K, DENOISED, noise_floor_db=DENOISE_NOISE_FLOOR_DB)\n",
    "    denoised_audio = DENOISED if verify_wav_ok(DENOISED) else AUDIO_16K\n",
    "else:\n",
    "    denoised_audio = AUDIO_16K\n",
    "\n",
    "if not verify_wav_ok(denoised_audio):\n",
    "    if DEBUG_MODE: print(\"  - éŸ³è¨Šæ ¼å¼ç•°å¸¸ï¼›å˜—è©¦é‡åŒ… WAV ...\")\n",
    "    FIXED = Path(\"/tmp/audio_16k_fixed.wav\")\n",
    "    ffmpeg_repack_wav(denoised_audio, FIXED, sr=16000)\n",
    "    denoised_audio = FIXED\n",
    "\n",
    "if DEBUG_MODE: print(f\"â†’ æœ€çµ‚è¼¸å…¥éŸ³è¨Šï¼š{denoised_audio}\")\n",
    "\n",
    "# [6/8] Load faster-whisper (GPU enforced) - Uses 'model_size'\n",
    "if DEBUG_MODE: print(\"[6/8] Loading faster-whisper model (GPU) ...\")\n",
    "device = \"cuda\"  # Enforce GPU\n",
    "model = None; last_err = None\n",
    "for ctype in [\"int8_float16\", \"float16\", \"int8\"]:\n",
    "    try:\n",
    "        if DEBUG_MODE: print(f\"  - Trying compute_type={ctype}\")\n",
    "        model = WhisperModel(model_size, device=device, compute_type=ctype)\n",
    "        if DEBUG_MODE: print(\"  - Model loaded successfully\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "        if DEBUG_MODE: print(f\"  - Load failed: {e}\")\n",
    "if model is None:\n",
    "    print(\"\\nâ›” GPU æ¨¡å‹è¼‰å…¥å¤±æ•—ã€‚è«‹ç¢ºèªã€è®Šæ›´åŸ·è¡Œéšæ®µé¡å‹ã€é¸äº† GPUï¼ˆT4/A100ï¼‰ï¼Œæˆ–åˆªé™¤åŸ·è¡Œéšæ®µå¾Œé‡è©¦ã€‚\")\n",
    "    suggest_runtime_reset()\n",
    "    raise RuntimeError(f\"ç„¡æ³•è¼‰å…¥æ¨¡å‹ï¼š{last_err}\")\n",
    "\n",
    "gc.collect()  # Clean up before transcription (safety)\n",
    "\n",
    "# [7/8] Transcribe (GPU; real-time progress per segment) - Uses 'language_code', 'TRANSCRIPTION_BEAM_SIZE_PRIMARY', 'TRANSCRIPTION_CHUNK_LENGTH_PRIMARY', 'TRANSCRIPTION_BEAM_SIZE_FALLBACK', 'TRANSCRIPTION_CHUNK_LENGTH_FALLBACK'\n",
    "if DEBUG_MODE: print(f\"[7/8] Starting transcription (GPU: beam={TRANSCRIPTION_BEAM_SIZE_PRIMARY} / chunk={TRANSCRIPTION_CHUNK_LENGTH_PRIMARY}s / no VAD) ...\")\n",
    "\n",
    "def transcribe_gpu(_beam=TRANSCRIPTION_BEAM_SIZE_PRIMARY, _chunk=TRANSCRIPTION_CHUNK_LENGTH_PRIMARY):\n",
    "    return model.transcribe(\n",
    "        str(denoised_audio),\n",
    "        task=\"transcribe\",\n",
    "        language=language_code,\n",
    "        temperature=0.0,\n",
    "        condition_on_previous_text=True,\n",
    "        compression_ratio_threshold=2.4,\n",
    "        log_prob_threshold=-1.0,\n",
    "        no_speech_threshold=0.6,\n",
    "        beam_size=_beam,\n",
    "        chunk_length=_chunk,\n",
    "        vad_filter=False,\n",
    "        word_timestamps=False\n",
    "    )\n",
    "\n",
    "try:\n",
    "    seg_iter, info = transcribe_gpu(_beam=TRANSCRIPTION_BEAM_SIZE_PRIMARY, _chunk=TRANSCRIPTION_CHUNK_LENGTH_PRIMARY)\n",
    "except Exception as e:\n",
    "    if DEBUG_MODE: print(f\"  - First transcription failed: {e}\\n    â†’ Trying more conservative (beam={TRANSCRIPTION_BEAM_SIZE_FALLBACK}, chunk={TRANSCRIPTION_CHUNK_LENGTH_FALLBACK}) ...\")\n",
    "    seg_iter, info = transcribe_gpu(_beam=TRANSCRIPTION_BEAM_SIZE_FALLBACK, _chunk=TRANSCRIPTION_CHUNK_LENGTH_FALLBACK)\n",
    "\n",
    "# Display percentage based on total video duration\n",
    "duration = float(getattr(info, \"duration\", 0.0) or 0.0)\n",
    "if duration <= 0: duration = 1.0\n",
    "\n",
    "segments = []\n",
    "filtered = []\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    print(f\"  - Detected language: {getattr(info,'language','æœªçŸ¥')} (p={getattr(info,'language_probability',0):.2f})\")\n",
    "    print(f\"  - Audio length: {duration:.2f}s\")\n",
    "\n",
    "for s in seg_iter:\n",
    "    pct = int(min(100, round((s.end / duration) * 100)))\n",
    "    print(f\"[{pct:3d}%] {fmt_ts_srt(s.start)} â†’ {fmt_ts_srt(s.end)}  {s.text.strip()}\", flush=True)\n",
    "    segments.append(s)\n",
    "\n",
    "    # Low confidence/high no-speech short segment filtering (no blacklist) - Uses FILTER_* parameters\n",
    "    keep = True\n",
    "    seg_dur = float(s.end - s.start)\n",
    "    if seg_dur < FILTER_MIN_DURATION_SHORT and getattr(s, \"avg_logprob\", None) is not None and s.avg_logprob < FILTER_AVG_LOGPROB_THRESHOLD:\n",
    "        keep = False\n",
    "    if seg_dur < FILTER_MIN_DURATION_SPEECH_PROB and getattr(s, \"no_speech_prob\", None) is not None and s.no_speech_prob > FILTER_NO_SPEECH_PROB_THRESHOLD:\n",
    "        keep = False\n",
    "    if keep:\n",
    "        filtered.append(s)\n",
    "\n",
    "if DEBUG_MODE: print(f\"  - Number of segments: Before filtering {len(segments)} â†’ After filtering {len(filtered)}\")\n",
    "\n",
    "# ---- OpenCC Normalization (for output text) ---- - Uses 'text_postprocess'\n",
    "pipeline = build_opencc_pipeline(text_postprocess)\n",
    "def norm(txt: str) -> str:\n",
    "    return apply_opencc(txt, pipeline) if pipeline else txt\n",
    "\n",
    "# [8/8] Output (text after OpenCC) - Uses 'out_base_dir' (derived from 'filename')\n",
    "print(\"[8/8] è¼¸å‡º SRT / TXT ...\")\n",
    "# Determine the output directory for transcription based on input type\n",
    "# If input is a network source, output to WHISPER_DIR\n",
    "# If input is a local file, output to the same directory as the input file\n",
    "if is_youtube_url(filename) or is_http_url(filename):\n",
    "    out_base_dir = WHISPER_DIR\n",
    "else:\n",
    "    src_path_abs = to_abs_mydrive(filename)\n",
    "    out_base_dir = src_path_abs.parent\n",
    "\n",
    "# Create the transcription output directory if it doesn't exist\n",
    "out_dir = out_base_dir\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Determine the stem from the original source file path\n",
    "stem = Path(src_path).stem\n",
    "SRT = out_dir / f\"{stem}.srt\"\n",
    "TXT = out_dir / f\"{stem}.txt\"\n",
    "\n",
    "with open(SRT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, s in enumerate(filtered, 1):\n",
    "        text_out = norm(s.text.strip())\n",
    "        f.write(f\"{i}\\n{fmt_ts_srt(s.start)} --> {fmt_ts_srt(s.end)}\\n{text_out}\\n\\n\")\n",
    "\n",
    "with open(TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in filtered:\n",
    "        f.write(norm(s.text.strip()) + \"\\n\")  # Each segment on a new line\n",
    "\n",
    "print(f\"â†’ å®Œæˆï¼\\n  SRT: {SRT}\\n  TXT: {TXT}\")\n",
    "\n",
    "# Release model (release GPU memory)\n",
    "try: del model\n",
    "except: pass\n",
    "gc.collect()\n",
    "if DEBUG_MODE: print(\"â†’ Model released; can run again directly if needed.\")\n",
    "\n",
    "\n",
    "# ===== Summarization Logic Starts Here =====\n",
    "# Use SRT from transcription step for summarization\n",
    "summary_srt_path_abs = SRT\n",
    "assert summary_srt_path_abs.exists(), f\"SRT æª”ä¸å­˜åœ¨ï¼š{summary_srt_path_abs}\"\n",
    "\n",
    "# ===== Summary 1/6) Check GPU and Install Dependencies (llama-cpp-python specific) =====\n",
    "# llama-cpp-python installation logic - Keep this separate as it has specific CUDA requirements\n",
    "# Moved this section to just before reading the SRT for summarization\n",
    "if DEBUG_MODE: print(\"[Summary 1/6] Checking GPU and installing llama-cpp-python ...\")\n",
    "\n",
    "def detect_cuda_tag():\n",
    "    try:\n",
    "        out = sp.check_output([\"nvidia-smi\"], text=True)\n",
    "        m = re.search(r\"CUDA Version:\\s*([\\d.]+)\", out)\n",
    "        if not m:\n",
    "            return \"cu124\"\n",
    "        major, minor = [int(x) for x in m.group(1).split(\".\")[:2]]\n",
    "        if major > 12 or (major == 12 and minor >= 5):\n",
    "            return \"cu125\"\n",
    "        return \"cu124\"\n",
    "    except Exception:\n",
    "        return \"cu124\"\n",
    "\n",
    "cuda_tag = detect_cuda_tag()\n",
    "if DEBUG_MODE: print(f\"GPU 0: Detected CUDA version tag {cuda_tag}\")\n",
    "\n",
    "def try_import_llama():\n",
    "    try:\n",
    "        from llama_cpp import Llama\n",
    "        return Llama\n",
    "    except ModuleNotFoundError:\n",
    "        return None\n",
    "\n",
    "Llama = try_import_llama()\n",
    "if Llama is None:\n",
    "    # Keep your existing installation strategy: extra-index -> fallback to source compilation on failure\n",
    "    candidates = [cuda_tag, \"cu125\", \"cu124\", \"cu122\", \"cu121\"]\n",
    "    ok = False\n",
    "    for tag in candidates:\n",
    "        idx = f\"https://abetlen.github.io/llama-cpp-python/whl/{tag}\"\n",
    "        if DEBUG_MODE: print(f\"â†’ Attempting to install llama-cpp-python ({tag}) ...\")\n",
    "        r = pip_install([\"llama-cpp-python\"], extra_args=[\"--extra-index-url\", idx])\n",
    "        if r.returncode == 0:\n",
    "            Llama = try_import_llama()\n",
    "            if Llama is not None:\n",
    "                ok = True\n",
    "                break\n",
    "        else:\n",
    "            if DEBUG_MODE: print(\"  âœ— Installation failed (summary):\", \"\\n\".join(r.stdout.splitlines()[-5:]))\n",
    "    if not ok:\n",
    "        if DEBUG_MODE: print(\"â†’ Pre-compiled wheels not available, switching to 'source compilation (CUDA=ON)' ... (takes longer)\")\n",
    "        try:\n",
    "            import ninja # noqa: F401 # Import ninja to check if installed\n",
    "        except ModuleNotFoundError:\n",
    "            if DEBUG_MODE: print(\"â†’ Installing missing package: ninja\")\n",
    "            r = pip_install([\"ninja\"])\n",
    "            if r.returncode != 0:\n",
    "                if DEBUG_MODE: print(r.stdout)\n",
    "                raise RuntimeError(\"å®‰è£ ninja å¤±æ•—ã€‚è«‹é‡å•Ÿå¾Œé‡è©¦ã€‚\")\n",
    "        env = os.environ.copy()\n",
    "        env[\"CMAKE_ARGS\"] = \"-DGGML_CUDA=on -DLLAMA_CUBLAS=on\"\n",
    "        env[\"FORCE_CMAKE\"] = \"1\"\n",
    "        r = pip_install([\"llama-cpp-python\"], env=env)\n",
    "        if r.returncode != 0:\n",
    "            if DEBUG_MODE: print(r.stdout)\n",
    "            raise RuntimeError(\"ç„¡æ³•å®‰è£ GPU ç‰ˆ llama-cpp-pythonã€‚\")\n",
    "        Llama = try_import_llama()\n",
    "\n",
    "\n",
    "if DEBUG_MODE: print(\"[Summary 2/6] Reading SRT ...\")\n",
    "with open(summary_srt_path_abs, \"r\", encoding=\"utf-8\") as f:\n",
    "    srt_text = f.read()\n",
    "subs = list(_srt.parse(srt_text)) # Use _srt as srt module was imported as _srt\n",
    "def td2s(td): return td.total_seconds()\n",
    "segments = []\n",
    "for it in subs:\n",
    "    txt = it.content.strip()\n",
    "    if not txt: continue\n",
    "    segments.append((td2s(it.start), td2s(it.end), txt))\n",
    "total_secs = (segments[-1][1] - segments[0][0]) if segments else 0\n",
    "if DEBUG_MODE: print(f\"â†’ Number of subtitle segments: {len(segments)}ï¼›Video length (est): {total_secs/60:.1f} minutes\")\n",
    "\n",
    "\n",
    "# ===== Summary 3/6) Download and Load GGUF Model (Summary) - Uses summary model parameters (REPO_ID, GGUF_FILE, ctx_window, etc.)\n",
    "# Moved this section to just after installing llama-cpp-python\n",
    "if DEBUG_MODE: print(\"[Summary 3/6] Loading GPT-OSS-20B (GGUF, CUDA) ...\")\n",
    "local_repo = snapshot_download(REPO_ID, allow_patterns=[GGUF_FILE, \"tokenizer_config.json\"])\n",
    "gguf_path = str(Path(local_repo)/GGUF_FILE)\n",
    "\n",
    "tokenizer_config_path = Path(local_repo)/\"tokenizer_config.json\"\n",
    "if tokenizer_config_path.exists():\n",
    "    try:\n",
    "        tokenizer_config_data = json.loads(tokenizer_config_path.read_text())\n",
    "        if DEBUG_MODE: print(\"â†’ Loaded tokenizer_config.json (Harmony template)\")\n",
    "    except Exception as exc:\n",
    "        if DEBUG_MODE: print(\"  âœ— Failed to parse tokenizer_config.json:\", exc)\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=gguf_path,\n",
    "    n_ctx=ctx_window,\n",
    "    n_gpu_layers=-1,\n",
    "    seed=0,\n",
    "    logits_all=False,\n",
    "    verbose=True          # Display the actual chat format used\n",
    ")\n",
    "if DEBUG_MODE: print(\"â†’ Model loaded successfully (GPU)\")\n",
    "\n",
    "def ensure_harmony_formatter():\n",
    "    \"\"\"Ensure the Harmony chat formatter is available for GPT-OSS prompts.\"\"\"\n",
    "    global harmony_chat_formatter\n",
    "    if harmony_chat_formatter is not None:\n",
    "        return harmony_chat_formatter\n",
    "    try:\n",
    "        from llama_cpp.llama_chat_format import (\n",
    "            hf_tokenizer_config_to_chat_formatter,\n",
    "            Jinja2ChatFormatter,\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(\"llama_cpp Harmony chat helpers are unavailable\") from exc\n",
    "\n",
    "    formatter = None\n",
    "    if tokenizer_config_data:\n",
    "        try:\n",
    "            formatter = hf_tokenizer_config_to_chat_formatter(\n",
    "                tokenizer_config_data,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "        except Exception as exc:\n",
    "            if DEBUG_MODE:\n",
    "                print(\"  âœ— Failed to initialize Harmony formatter from tokenizer_config.json:\", exc)\n",
    "\n",
    "    if formatter is None:\n",
    "        template = None\n",
    "        if 'llm' in globals() and hasattr(llm, 'metadata'):\n",
    "            template = llm.metadata.get('tokenizer.chat_template')\n",
    "        if template:\n",
    "            try:\n",
    "                bos_token = (\n",
    "                    llm.metadata.get('tokenizer.bos_token')\n",
    "                    or llm.metadata.get('tokenizer.ggml.bos_token')\n",
    "                    or llm.detokenize([llm.token_bos()], special=True).decode('utf-8', errors='ignore')\n",
    "                )\n",
    "                eos_token = (\n",
    "                    llm.metadata.get('tokenizer.eos_token')\n",
    "                    or llm.metadata.get('tokenizer.ggml.eos_token')\n",
    "                    or llm.detokenize([llm.token_eos()], special=True).decode('utf-8', errors='ignore')\n",
    "                )\n",
    "                formatter = Jinja2ChatFormatter(\n",
    "                    template,\n",
    "                    eos_token=eos_token,\n",
    "                    bos_token=bos_token,\n",
    "                    stop_token_ids=[llm.token_eos()],\n",
    "                )\n",
    "            except Exception as exc:\n",
    "                if DEBUG_MODE:\n",
    "                    print(\"  âœ— Failed to build Harmony formatter from GGUF metadata:\", exc)\n",
    "\n",
    "    if formatter is None:\n",
    "        raise RuntimeError(\"Harmony chat formatter could not be prepared\")\n",
    "\n",
    "    harmony_chat_formatter = formatter\n",
    "    return harmony_chat_formatter\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    ensure_harmony_formatter()\n",
    "    if DEBUG_MODE: print(\"â†’ Harmony formatter prepared\")\n",
    "except Exception as exc:\n",
    "    raise RuntimeError(f\"Failed to prepare Harmony formatter: {exc}\")\n",
    "\n",
    "\n",
    "# ===== Summary 4/6) Token-aware Segmentation (Summary) - Uses ctx_window, map_max_new_tokens, prompt_overhead\n",
    "if DEBUG_MODE: print(\"[Summary 4/6] Generating segments (token-aware; single segment â‰¤ safety limit) ...\")\n",
    "\n",
    "def count_tokens_text(text: str) -> int:\n",
    "    # Check if llm is initialized before using it\n",
    "    if 'llm' not in globals() or llm is None:\n",
    "         raise RuntimeError(\"LLM model is not loaded. Cannot count tokens.\")\n",
    "    return len(llm.tokenize(text.encode(\"utf-8\")))\n",
    "\n",
    "SYSTEM_INSTR = (\n",
    "  \"ä½ æ˜¯ä¸€å€‹æœƒè­°ç¸½çµæ©Ÿå™¨äººã€‚æ ¹æ“šä½¿ç”¨è€…æä¾›çš„é€å­—ç¨¿ï¼ˆå¯èƒ½é›œè¨Šã€é‡è¤‡ã€éŒ¯å­—ï¼‰ï¼Œ\"\n",
    "  \"è«‹å»é™¤é›œè¨Šèˆ‡é‡è¤‡ã€åš´å®ˆäº‹å¯¦ã€ä¸è…¦è£œã€‚é‡åˆ°ä¸æ˜ç¢ºè³‡è¨Šä»¥ã€Œå¾…è£œå……ï¼æœªæ˜ç¢ºã€æ¨™è¨»ã€‚\"\n",
    "  \"è¼¸å‡ºç‚º Markdownï¼ˆç¹é«”ä¸­æ–‡ï¼‰ï¼Œä¸è¦è¼¸å‡ºä»»ä½•ç³»çµ±ï¼æ€è€ƒæ¨™è¨˜ã€‚\"\n",
    ")\n",
    "\n",
    "# â€” Segment Summary Prompt: More concise request, avoid verbosity and system language - Uses 'topic_hint'\n",
    "MAP_USER_TMPL = textwrap.dedent(\"\"\"\\\n",
    "ä¸»é¡Œï¼ˆå¯ç•™ç©ºï¼‰ï¼š{topic}\n",
    "\n",
    "ä»¥ä¸‹æ˜¯é€å­—ç¨¿ç‰‡æ®µï¼ˆéå®Œæ•´å…¨æ–‡ï¼‰ï¼š\n",
    "{chunk}\n",
    "\n",
    "è«‹å°±æ­¤ç‰‡æ®µè¼¸å‡ºã€Œæ¢åˆ—å¼é‡é»æ‘˜è¦ã€ï¼ˆ500â€“900 å­—ï¼Œç¹é«”ä¸­æ–‡ï¼‰ï¼Œæ³¨æ„ï¼š\n",
    "- åªå¯«æœ€çµ‚å…§å®¹ï¼Œä¸è¦å¯«è§£é¡Œæƒ³æ³•ã€ä¸è¦å‡ºç¾ä»»ä½•ç³»çµ±æç¤ºæˆ–ä¸­è‹±æ‹¬è™Ÿæ¨™è¨˜ã€‚\n",
    "- èšç„¦å¯é©—è­‰äº‹å¯¦ï¼ˆæ™‚é–“ã€äººç‰©ã€ä»»å‹™ã€çµè«–ã€æœªæ±ºäº‹é …ã€è¡Œå‹•ï¼‰ã€‚\n",
    "- çµæ§‹ï¼šå¯ç”¨å°æ¨™é¡Œï¼‹é …ç›®ç¬¦è™Ÿï¼Œèªå¥å‹™å¿…çŸ­ã€æº–ç¢ºã€ç„¡è´…è©ã€‚\n",
    "\"\"\")\n",
    "\n",
    "# â€” Summary Prompt: Maintain your three-section output structure - Uses 'topic_hint'\n",
    "REDUCE_USER_TMPL = textwrap.dedent(\"\"\"\\\n",
    "ä¸»é¡Œï¼ˆå¯ç•™ç©ºï¼‰ï¼š{topic}\n",
    "\n",
    "ä»¥ä¸‹æ˜¯æ‰€æœ‰ç‰‡æ®µçš„é‡é»æ‘˜è¦å½™æ•´ï¼ˆä»å¯èƒ½æœ‰é‡ç–Šï¼‰ï¼š\n",
    "{maps}\n",
    "\n",
    "è«‹æ•´åˆç‚ºä¸€ä»½æœƒè­°ç­†è¨˜ï¼ˆMarkdownï¼Œç¹é«”ï¼‰ï¼š\n",
    "1) **æ•´é«”æè¦**ï¼ˆ3â€“6 å¥ï¼Œé¿å…å†—è¨€ï¼‰\n",
    "2) **ç« ç¯€è¦é»ï¼ˆå«æ™‚é–“è„ˆçµ¡ï¼‰**ï¼šæ¢åˆ—å‘ˆç¾ï¼Œæ¯é»ä¸€è¡Œï¼Œå¯é™„ç²—ç•¥æ™‚é–“\n",
    "3) **å¯åŸ·è¡Œé‡é»**ï¼šå…·é«”å¾…è¾¦ï¼ˆæ¯æ¢ä»¥å‹•è©é–‹é ­ï¼‰\n",
    "è«‹åªè¼¸å‡ºæœ€çµ‚ç­†è¨˜ï¼Œä¸è¦å‡ºç¾ç³»çµ±æˆ–æ€è€ƒæ¨™è¨˜ï¼Œä¸è¦åŠ å…¥æœªå‡ºç¾çš„æ–°è³‡è¨Šã€‚\n",
    "\"\"\")\n",
    "\n",
    "# Single segment token budget (reserve space for prompt and generation)\n",
    "prompt_overhead = 700\n",
    "chunk_target    = max(1024, min(3072, ctx_window - prompt_overhead - map_max_new_tokens))\n",
    "\n",
    "chunks: List[Tuple[float,float,str]] = []\n",
    "buf, t0, t1, cur = [], None, None, 0\n",
    "for (s, e, txt) in segments:\n",
    "    t = count_tokens_text(txt)\n",
    "    if not buf:\n",
    "        buf, t0, t1, cur = [txt], s, e, t\n",
    "        continue\n",
    "    if cur + t <= chunk_target:\n",
    "        buf.append(txt); t1 = e; cur += t\n",
    "    else:\n",
    "        chunks.append((t0, t1, \"\\n\".join(buf)))\n",
    "        buf, t0, t1, cur = [txt], s, e, t\n",
    "if buf:\n",
    "    chunks.append((t0, t1, \"\\n\".join(buf)))\n",
    "\n",
    "if DEBUG_MODE: print(f\"â†’ Generated {len(chunks)} segments (target ~{chunk_target} tokens per segment)\")\n",
    "\n",
    "# ===== Common: Streaming Tools (No regex cleaning; use correct stop sequence) - Uses temperature, top_p, repeat_penalty, map_max_new_tokens, reduce_max_new_tokens\n",
    "\n",
    "\n",
    "def stream_harmony_final_pieces(text_chunks: Iterable[str]) -> Iterator[str]:\n",
    "    \"\"\"Yield Harmony streamed text, preferring the final channel.\n",
    "\n",
    "    Some community models only emit the assistant channel; fall back to it\n",
    "    so we do not drop the actual content.\n",
    "    \"\"\"\n",
    "    buffer = \"\"\n",
    "    current_channel: Optional[str] = None\n",
    "    pending_channel = False\n",
    "    channel_name_buffer = \"\"\n",
    "    in_message = False\n",
    "    assistant_cache: List[str] = []\n",
    "    final_seen = False\n",
    "\n",
    "    def canonical_channel(name: Optional[str]) -> str:\n",
    "        if not name:\n",
    "            return \"\"\n",
    "        lowered = name.strip().lower()\n",
    "        if not lowered:\n",
    "            return \"\"\n",
    "        if \"final\" in lowered:\n",
    "            return \"final\"\n",
    "        if \"assistant\" in lowered:\n",
    "            return \"assistant\"\n",
    "        return lowered\n",
    "\n",
    "    def emit_text(text: str):\n",
    "        nonlocal final_seen, assistant_cache\n",
    "        if not text:\n",
    "            return\n",
    "        channel = canonical_channel(current_channel)\n",
    "        if channel == \"final\":\n",
    "            final_seen = True\n",
    "            if assistant_cache:\n",
    "                cached = assistant_cache[:]\n",
    "                assistant_cache.clear()\n",
    "                for cached_piece in cached:\n",
    "                    if cached_piece:\n",
    "                        yield cached_piece\n",
    "            yield text\n",
    "        elif channel == \"assistant\":\n",
    "            if final_seen:\n",
    "                yield text\n",
    "            else:\n",
    "                assistant_cache.append(text)\n",
    "        elif final_seen:\n",
    "            yield text\n",
    "\n",
    "    for piece in text_chunks:\n",
    "        if not piece:\n",
    "            continue\n",
    "        buffer += piece\n",
    "        while True:\n",
    "            if pending_channel:\n",
    "                idx = buffer.find(\"<|\")\n",
    "                if idx == -1:\n",
    "                    channel_name_buffer += buffer\n",
    "                    buffer = \"\"\n",
    "                    break\n",
    "                channel_name_buffer += buffer[:idx]\n",
    "                buffer = buffer[idx:]\n",
    "                channel = channel_name_buffer.strip()\n",
    "                channel_name_buffer = \"\"\n",
    "                pending_channel = False\n",
    "                current_channel = channel\n",
    "                channel_canonical = canonical_channel(current_channel)\n",
    "                in_message = bool(channel_canonical in {\"assistant\", \"final\"})\n",
    "                continue\n",
    "            tag_start = buffer.find(\"<|\")\n",
    "            if tag_start == -1:\n",
    "                if in_message or canonical_channel(current_channel) in {\"assistant\", \"final\"}:\n",
    "                    for out in emit_text(buffer):\n",
    "                        yield out\n",
    "                    buffer = \"\"\n",
    "                else:\n",
    "                    if len(buffer) > 128:\n",
    "                        buffer = buffer[-128:]\n",
    "                break\n",
    "            if tag_start > 0:\n",
    "                text = buffer[:tag_start]\n",
    "                if in_message or canonical_channel(current_channel) in {\"assistant\", \"final\"}:\n",
    "                    for out in emit_text(text):\n",
    "                        yield out\n",
    "                buffer = buffer[tag_start:]\n",
    "            tag_end = buffer.find(\"|>\")\n",
    "            if tag_end == -1:\n",
    "                break\n",
    "            tag = buffer[2:tag_end].strip().lower()\n",
    "            buffer = buffer[tag_end + 2:]\n",
    "            if tag == \"start\":\n",
    "                current_channel = None\n",
    "                in_message = False\n",
    "            elif tag == \"channel\":\n",
    "                pending_channel = True\n",
    "            elif tag == \"message\":\n",
    "                in_message = True\n",
    "            elif tag == \"end\":\n",
    "                in_message = False\n",
    "                current_channel = None\n",
    "            elif tag == \"return\":\n",
    "                if not final_seen and assistant_cache:\n",
    "                    for cached_piece in assistant_cache:\n",
    "                        if cached_piece:\n",
    "                            yield cached_piece\n",
    "                    assistant_cache.clear()\n",
    "                return\n",
    "            else:\n",
    "                continue\n",
    "    if (in_message or canonical_channel(current_channel) in {\"assistant\", \"final\"}) and buffer:\n",
    "        for out in emit_text(buffer):\n",
    "            yield out\n",
    "    if not final_seen and assistant_cache:\n",
    "        for cached_piece in assistant_cache:\n",
    "            if cached_piece:\n",
    "                yield cached_piece\n",
    "\n",
    "def llm_stream(messages, max_tokens):\n",
    "    \"\"\"Stream Harmony-formatted completions and yield only the final channel.\"\"\"\n",
    "    if 'llm' not in globals() or llm is None:\n",
    "        raise RuntimeError(\"LLM model is not loaded. Cannot stream generation.\")\n",
    "    formatter = ensure_harmony_formatter()\n",
    "    chat_response = formatter(\n",
    "        messages=messages,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    completion_kwargs = dict(\n",
    "        prompt=chat_response.prompt,\n",
    "        temperature=float(temperature),\n",
    "        top_p=float(top_p),\n",
    "        repeat_penalty=float(repeat_penalty),\n",
    "        max_tokens=int(max_tokens),\n",
    "        stream=True,\n",
    "    )\n",
    "    if chat_response.stop:\n",
    "        completion_kwargs[\"stop\"] = chat_response.stop\n",
    "    if chat_response.stopping_criteria is not None:\n",
    "        completion_kwargs[\"stopping_criteria\"] = chat_response.stopping_criteria\n",
    "\n",
    "    gen = llm.create_completion(**completion_kwargs)\n",
    "\n",
    "    def _iter_text_stream(events):\n",
    "        for ev in events:\n",
    "            yield ev.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "\n",
    "    text_stream = _iter_text_stream(gen)\n",
    "\n",
    "    for final_piece in stream_harmony_final_pieces(text_stream):\n",
    "        if final_piece:\n",
    "            yield final_piece\n",
    "\n",
    "# ===== Summary 5/6) Segment Summary (map) - Uses map_max_new_tokens, ctx_window, prompt_overhead, topic_hint\n",
    "if DEBUG_MODE: print(\"[Summary 5/6] Segment summarization (map) ...\")\n",
    "live = display(Markdown(\"\"), display_id=True)\n",
    "maps: List[str] = []\n",
    "\n",
    "\n",
    "def escape_braces(text: str) -> str:\n",
    "    \"\"\"Escape braces so str.format does not treat user content as placeholders.\"\"\"\n",
    "    return text.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "\n",
    "safe_topic_hint = escape_braces(topic_hint or \"ï¼ˆç„¡ï¼‰\")\n",
    "\n",
    "for i, (s, e, body) in enumerate(chunks, 1):\n",
    "    pct = i / max(len(chunks),1) * 100\n",
    "    sys.stdout.write(f\"  - è™•ç†åˆ†æ®µ {i}/{len(chunks)}ï¼ˆ~{pct:.1f}%ï¼‰\\n\"); sys.stdout.flush()\n",
    "\n",
    "    # Shrink to safe budget before sending (prevent prompt+segment from exceeding window and causing model to terminate early)\n",
    "    budget_tokens = max(512, ctx_window - map_max_new_tokens - prompt_overhead)\n",
    "    def shrink_to_budget(text: str, budget_tokens: int) -> str:\n",
    "        cur = text\n",
    "        for _ in range(6):\n",
    "            if count_tokens_text(cur) <= budget_tokens:\n",
    "                return cur\n",
    "            keep = max(800, int(len(cur) * 0.85))\n",
    "            cur = cur[:keep]\n",
    "        return cur\n",
    "    body2 = shrink_to_budget(body, budget_tokens)\n",
    "\n",
    "    safe_body = escape_braces(body2)\n",
    "    user_txt = MAP_USER_TMPL.format(topic=safe_topic_hint, chunk=safe_body)\n",
    "    user_txt = user_txt.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_INSTR},\n",
    "        {\"role\": \"user\",   \"content\": user_txt},\n",
    "    ]\n",
    "\n",
    "    part_buf = [] # Reset part_buf for each segment\n",
    "    for token in llm_stream(messages, map_max_new_tokens):\n",
    "        part_buf.append(token)\n",
    "        # Update live display and terminal character count periodically\n",
    "        if len(part_buf) % 24 == 0:\n",
    "            cur_txt = \"\".join(part_buf)\n",
    "            live.update(Markdown(cur_txt))\n",
    "            sys.stdout.write(f\"    â†³ åˆ†æ®µ {i} å·²ç”¢ç”Ÿå­—å…ƒï¼š{len(cur_txt)}\\n\"); sys.stdout.flush()\n",
    "    cur_txt = \"\".join(part_buf)\n",
    "    live.update(Markdown(cur_txt))\n",
    "    sys.stdout.write(f\"    â†³ åˆ†æ®µ {i} å·²ç”¢ç”Ÿå­—å…ƒï¼š{len(cur_txt)}\\n\"); sys.stdout.flush()\n",
    "\n",
    "    # Include the model's final output directly, no regex cleaning\n",
    "    maps.append(cur_txt.strip())\n",
    "\n",
    "if DEBUG_MODE: print(\"â†’ Segment summarization complete\")\n",
    "\n",
    "if DEBUG_MODE: print(\"[Summary 6/6] Consolidating summary (reduce) ...\")\n",
    "maps_md = \"\\n\\n---\\n\\n\".join(f\"### ç‰‡æ®µ {i+1} è¦é»\\n\\n{m}\" for i, m in enumerate(maps))\n",
    "\n",
    "# If combined text exceeds window, truncate proportionally first (without changing text within segments to avoid breaking meaning)\n",
    "def fit_reduce_payload(md_text: str, max_ctx_tokens: int) -> str:\n",
    "    for _ in range(8):\n",
    "        need = count_tokens_text(md_text)\n",
    "        if need + reduce_max_new_tokens + 400 <= max_ctx_tokens:\n",
    "            return md_text\n",
    "        md_text = md_text[: int(len(md_text) * 0.9)]\n",
    "    return md_text\n",
    "\n",
    "md_cur = fit_reduce_payload(maps_md, ctx_window)\n",
    "\n",
    "safe_md_cur = escape_braces(md_cur)\n",
    "user_txt = REDUCE_USER_TMPL.format(topic=safe_topic_hint, maps=safe_md_cur)\n",
    "user_txt = user_txt.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
    "messages = [{\"role\":\"system\",\"content\":SYSTEM_INSTR},\n",
    "            {\"role\":\"user\",\"content\":user_txt}]\n",
    "\n",
    "live2 = display(Markdown(\"\"), display_id=True)\n",
    "final_buf = []\n",
    "for token in llm_stream(messages, reduce_max_new_tokens):\n",
    "    final_buf.append(token)\n",
    "    if len(final_buf) % 24 == 0:\n",
    "        live2.update(Markdown(\"\".join(final_buf)))\n",
    "        sys.stdout.write(f\"    â†³ å½™æ•´ å·²ç”¢ç”Ÿå­—å…ƒï¼š{len(''.join(final_buf))}\\n\"); sys.stdout.flush()\n",
    "live2.update(Markdown(\"\".join(final_buf)))\n",
    "sys.stdout.write(f\"    â†³ å½™æ•´ å·²ç”¢ç”Ÿå­—å…ƒï¼š{len(''.join(final_buf))}\\n\"); sys.stdout.flush()\n",
    "\n",
    "final_text = \"\".join(final_buf).strip()\n",
    "\n",
    "# Determine and create the summary output directory\n",
    "summary_output_dir_abs = out_base_dir\n",
    "summary_output_dir_abs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Determine the summary output file path using the stem of the input SRT\n",
    "out_md = summary_output_dir_abs / f\"{Path(summary_srt_path_abs).stem}_summary.md\"\n",
    "\n",
    "\n",
    "with open(out_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_text)\n",
    "\n",
    "print(f\"â†’ å®Œæˆ âœ…  {out_md}\")\n",
    "try:\n",
    "    del llm\n",
    "except Exception:\n",
    "    pass\n",
    "gc.collect()\n",
    "if DEBUG_MODE: print(\"ï¼ˆé¡¯å­˜å·²é‡‹æ”¾ï¼Œå¦‚éœ€é‡è·‘å¯ç›´æ¥å†æ¬¡åŸ·è¡Œï¼‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
