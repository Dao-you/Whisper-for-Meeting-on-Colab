{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Dao-you/Whisper-for-Meeting-on-Colab/blob/main/Whisper_for_Meeting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "95849644-392d-48b8-8058-d84b741259a3"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =========================================================\n",
    "# Whisper Automatic Subtitle Generation: GPU Transcription + CPU Denoising + OpenCC Post-processing (Traditional/Simplified Conversion)\n",
    "# And LLM Summarization (GPT-OSS-20B / llama.cpp / CUDA)\n",
    "# - Transcription: faster-whisper (CUDA, compute: int8_float16→float16→int8)\n",
    "# - Denoising: ffmpeg afftdn (CPU)\n",
    "# - Progress: Real-time printing of \"current sentence + video total length percentage\"\n",
    "# - Network source download and output: MyDrive/whisper; Files in Drive: Output to the same folder\n",
    "# - LLM Summary: llama.cpp + GPT-OSS-20B GGUF for summarizing transcription\n",
    "# - Prompts \"Delete runtime and restart\" if download is blocked or abnormal\n",
    "# =========================================================\n",
    "\n",
    "# Restrict multithreading (more stable)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# [1/8] Mount Google Drive\n",
    "from google.colab import drive\n",
    "print(\"請授權以掛載 Google Drive\")\n",
    "drive.mount(\"/content/gdrive\", force_remount=True)\n",
    "\n",
    "# Built-in Imports\n",
    "import sys, gc, shutil, datetime, subprocess as sp\n",
    "from pathlib import Path\n",
    "import re, math, time, textwrap\n",
    "from typing import List, Tuple, Optional, Iterable, Iterator\n",
    "from IPython.display import display, Markdown\n",
    "import json\n",
    "\n",
    "\n",
    "ROOT = Path(\"/content/gdrive/MyDrive\")\n",
    "WHISPER_DIR = ROOT / \"whisper\"\n",
    "WHISPER_DIR.mkdir(exist_ok=True, parents=True)\n",
    "os.chdir(ROOT)\n",
    "print(f\"→ 當前工作目錄：{os.getcwd()}\")\n",
    "\n",
    "# [2/8] User Form Parameters (Unified)\n",
    "#@markdown # Whisper Transcription & LLM Summary Pipeline\n",
    "\n",
    "#@markdown ## Input & Transcription Settings\n",
    "#@markdown **Input Source:** Google Drive file (relative to MyDrive) or video URL (YouTube/HTTP).\n",
    "filename = \"whisper/jcz-mfkq-frc (2025-08-08 10_00 GMT+8).mp4\"  #@param {type:\"string\"}\n",
    "#@markdown **Download Option:** Check to save network source files to `MyDrive/whisper`.\n",
    "save_video_to_google_drive = True  #@param {type:\"boolean\"}\n",
    "#@markdown **Whisper Model Size:** Choose a model size. `large-v3` requires more GPU VRAM; `medium` is a good alternative if VRAM is limited.\n",
    "model_size = \"medium\"  #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v2\", \"large-v3\"] # Changed model_size to \"medium\"\n",
    "#@markdown **Language:** Select the language for transcription. \"自動偵測\" (Auto-detect) is usually sufficient.\n",
    "language = \"自動偵測\"  #@param [\"自動偵測\", \"中文\", \"英文\"]\n",
    "#@markdown **Denoising:** Apply CPU-based denoising to the audio before transcription. `afftdn` is recommended.\n",
    "denoise_method = \"afftdn (建議)\"  #@param [\"afftdn (建議)\", \"none\"]\n",
    "#@markdown **Text Post-processing (OpenCC):** Convert the transcribed text (SRT/TXT output) between Simplified and Traditional Chinese variants.\n",
    "text_postprocess = \"臺灣繁體中文（預設）\"  #@param [\"臺灣繁體中文（預設）\",\"香港繁體中文\",\"大陸簡體中文\",\"關閉\"]\n",
    "#@markdown **YouTube Cookies (Optional):** Path to a Netscape-format cookies file (relative to MyDrive) for accessing age-restricted or member-only YouTube videos (e.g., `cookies/youtube.txt`).\n",
    "youtube_cookies_txt_path = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ## Summarization Settings\n",
    "#@markdown **Topic Hint (Optional):** Provide a brief hint about the topic to guide the summarization process.\n",
    "topic_hint = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "language_code_map = {\"自動偵測\": None, \"中文\":\"zh\", \"英文\":\"en\"}\n",
    "language_code = language_code_map[language]\n",
    "\n",
    "# =========================================================\n",
    "# Developer Options\n",
    "# Advanced users can fine-tune parameters in this section.\n",
    "# Modify only if you understand the impact.\n",
    "# =========================================================\n",
    "DEBUG_MODE = False # Set to True for more detailed logging\n",
    "\n",
    "# --- Transcription Parameters ---\n",
    "TRANSCRIPTION_BEAM_SIZE_PRIMARY = 3\n",
    "TRANSCRIPTION_CHUNK_LENGTH_PRIMARY = 20\n",
    "TRANSCRIPTION_BEAM_SIZE_FALLBACK = 1 # Used if primary fails\n",
    "TRANSCRIPTION_CHUNK_LENGTH_FALLBACK = 15 # Used if primary fails\n",
    "\n",
    "# --- Denoising Parameters ---\n",
    "DENOISE_NOISE_FLOOR_DB = -25\n",
    "\n",
    "# --- Filtering Parameters ---\n",
    "FILTER_MIN_DURATION_SHORT = 1.5 # Minimum duration for short segments\n",
    "FILTER_AVG_LOGPROB_THRESHOLD = -1.0 # Avg log probability threshold for short segments\n",
    "FILTER_MIN_DURATION_SPEECH_PROB = 2.0 # Minimum duration for speech probability filtering\n",
    "FILTER_NO_SPEECH_PROB_THRESHOLD = 0.6 # No speech probability threshold\n",
    "\n",
    "# --- Summary Model Parameters ---\n",
    "REPO_ID   = \"unsloth/gpt-oss-20b-GGUF\"   # GGUF Model Repository\n",
    "GGUF_FILE = \"gpt-oss-20b-Q4_K_M.gguf\"    # Approx. 10.8GiB, T4 can run\n",
    "\n",
    "# --- Summary Inference Parameters (Increase available generation space to avoid truncation) ---\n",
    "ctx_window            = 8192\n",
    "map_max_new_tokens    = 512   # Segment output: original 256 -> 512 (approx. 350-450 chars)\n",
    "reduce_max_new_tokens = 1024  # Summary output: original 512 -> 1024 (approx. 700-900+ chars)\n",
    "temperature           = 0.2\n",
    "top_p                 = 0.9\n",
    "repeat_penalty        = 1.05\n",
    "\n",
    "tokenizer_config_data = None\n",
    "harmony_chat_formatter = None\n",
    "\n",
    "def ensure_harmony_formatter():\n",
    "    \"\"\"Ensure the Harmony chat formatter is available for GPT-OSS prompts.\"\"\"\n",
    "    global harmony_chat_formatter\n",
    "    if harmony_chat_formatter is not None:\n",
    "        return harmony_chat_formatter\n",
    "    try:\n",
    "        from llama_cpp.llama_chat_format import (\n",
    "            hf_tokenizer_config_to_chat_formatter,\n",
    "            Jinja2ChatFormatter,\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(\"llama_cpp Harmony chat helpers are unavailable\") from exc\n",
    "\n",
    "    formatter = None\n",
    "    if tokenizer_config_data:\n",
    "        try:\n",
    "            formatter = hf_tokenizer_config_to_chat_formatter(\n",
    "                tokenizer_config_data,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "        except Exception as exc:\n",
    "            if DEBUG_MODE:\n",
    "                print(\"  ✗ Failed to initialize Harmony formatter from tokenizer_config.json:\", exc)\n",
    "\n",
    "    if formatter is None:\n",
    "        template = None\n",
    "        if 'llm' in globals() and hasattr(llm, 'metadata'):\n",
    "            template = llm.metadata.get('tokenizer.chat_template')\n",
    "        if template:\n",
    "            try:\n",
    "                bos_token = (\n",
    "                    llm.metadata.get('tokenizer.bos_token')\n",
    "                    or llm.metadata.get('tokenizer.ggml.bos_token')\n",
    "                    or llm.detokenize([llm.token_bos()], special=True).decode('utf-8', errors='ignore')\n",
    "                )\n",
    "                eos_token = (\n",
    "                    llm.metadata.get('tokenizer.eos_token')\n",
    "                    or llm.metadata.get('tokenizer.ggml.eos_token')\n",
    "                    or llm.detokenize([llm.token_eos()], special=True).decode('utf-8', errors='ignore')\n",
    "                )\n",
    "                formatter = Jinja2ChatFormatter(\n",
    "                    template,\n",
    "                    eos_token=eos_token,\n",
    "                    bos_token=bos_token,\n",
    "                    stop_token_ids=[llm.token_eos()],\n",
    "                )\n",
    "            except Exception as exc:\n",
    "                if DEBUG_MODE:\n",
    "                    print(\"  ✗ Failed to build Harmony formatter from GGUF metadata:\", exc)\n",
    "\n",
    "    if formatter is None:\n",
    "        raise RuntimeError(\"Harmony chat formatter could not be prepared\")\n",
    "\n",
    "    harmony_chat_formatter = formatter\n",
    "    return harmony_chat_formatter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# End of Developer Options\n",
    "# =========================================================\n",
    "\n",
    "\n",
    "# [3/8] Install Dependencies\n",
    "# Combine installation steps from both original cells\n",
    "if DEBUG_MODE: print(\"[Install] faster-whisper / yt-dlp / soundfile / opencc / srt / huggingface_hub / llama-cpp-python ...\")\n",
    "\n",
    "def pip_install(pkgs, extra_args=None, env=None):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\"]\n",
    "    if extra_args:\n",
    "        cmd += extra_args\n",
    "    cmd += pkgs\n",
    "    return sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True, env=env)\n",
    "\n",
    "import importlib.util\n",
    "# Install common dependencies first\n",
    "common_missing = []\n",
    "if importlib.util.find_spec(\"srt\") is None:\n",
    "    common_missing.append(\"srt>=3.5.3\")\n",
    "if importlib.util.find_spec(\"huggingface_hub\") is None:\n",
    "    common_missing.append(\"huggingface_hub>=0.23.0\")\n",
    "if importlib.util.find_spec(\"soundfile\") is None:\n",
    "    common_missing.append(\"soundfile\")\n",
    "if importlib.util.find_spec(\"opencc\") is None:\n",
    "    common_missing.append(\"opencc-python-reimplemented\")\n",
    "if importlib.util.find_spec(\"jinja2\") is None:\n",
    "    common_missing.append(\"jinja2>=3.1.0\")\n",
    "\n",
    "if common_missing:\n",
    "    if DEBUG_MODE: print(\"→ Installing common missing packages:\", \", \".join(common_missing))\n",
    "    r = pip_install(common_missing)\n",
    "    if r.returncode != 0:\n",
    "        if DEBUG_MODE: print(r.stdout)\n",
    "        raise RuntimeError(\"基礎依賴安裝失敗，請重啟執行階段後重試。\")\n",
    "\n",
    "# Install faster-whisper and yt-dlp separately as they were in the first cell\n",
    "if importlib.util.find_spec(\"faster_whisper\") is None:\n",
    "    if DEBUG_MODE: print(\"→ Installing missing package: faster-whisper yt-dlp\")\n",
    "    r = pip_install([\"faster-whisper\", \"yt-dlp\"])\n",
    "    if r.returncode != 0:\n",
    "        if DEBUG_MODE: print(r.stdout)\n",
    "        raise RuntimeError(\"faster-whisper / yt-dlp 安裝失敗，請重啟執行階段後重試。\")\n",
    "\n",
    "# Import external packages after ensuring installation\n",
    "import soundfile as sf\n",
    "from faster_whisper import WhisperModel\n",
    "from opencc import OpenCC\n",
    "import srt as _srt  # Import srt as _srt to avoid name conflict later with the module itself\n",
    "from huggingface_hub import snapshot_download\n",
    "def suggest_runtime_reset():\n",
    "    print(\"\\n🧹 建議動作（Colab）\")\n",
    "    print(\"1) 依序：『執行階段 Runtime』 → 『刪除執行階段/還原出廠設定 Factory reset runtime』\")\n",
    "    print(\"2) 重新執行本 Notebook（從掛載雲端硬碟那格開始）\\n\", flush=True)\n",
    "\n",
    "def run_cmd(cmd:list, check=True):\n",
    "    if DEBUG_MODE: print(\"  $\", \" \".join(cmd))\n",
    "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
    "    if DEBUG_MODE and p.stdout: sys.stdout.write(p.stdout)\n",
    "    if check and p.returncode != 0:\n",
    "        raise RuntimeError(f\"命令失敗：{' '.join(cmd)}\")\n",
    "    return p\n",
    "\n",
    "def is_youtube_url(s:str)->bool:\n",
    "    return isinstance(s, str) and (\"youtu.be\" in s or \"youtube.com\" in s)\n",
    "def is_http_url(s:str)->bool:\n",
    "    return isinstance(s, str) and s.lower().startswith(\"http\")\n",
    "def to_abs_mydrive(p:str)->Path:\n",
    "    return (Path(p) if p.startswith(\"/\") else (ROOT / p)).resolve()\n",
    "def fmt_ts_srt(t:float)->str:\n",
    "    h = int(t//3600); m = int((t%3600)//60); s = t - h*3600 - m*60\n",
    "    return f\"{h:02d}:{m:02d}:{int(s):02d},{int(round((s-int(s))*1000)):03d}\"\n",
    "def verify_wav_ok(path: Path)->bool:\n",
    "    try:\n",
    "        info = sf.info(str(path))\n",
    "        return info.samplerate > 0 and info.channels in (1, 2)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# OpenCC converter setup\n",
    "def build_opencc_pipeline(choice:str):\n",
    "    if choice.startswith(\"臺灣\"):\n",
    "        return [OpenCC('s2t'), OpenCC('t2tw')]\n",
    "    if choice.startswith(\"香港\"):\n",
    "        return [OpenCC('s2t'), OpenCC('t2hk')]\n",
    "    if choice.startswith(\"大陸\"):\n",
    "        return [OpenCC('t2s')]\n",
    "    return []  # Disable\n",
    "\n",
    "def apply_opencc(text:str, pipeline)->str:\n",
    "    for cc in pipeline:\n",
    "        text = cc.convert(text)\n",
    "    return text\n",
    "\n",
    "def ytdl(yturl:str)->Path:\n",
    "    tmp = Path(\"/tmp/dl\"); tmp.mkdir(parents=True, exist_ok=True)\n",
    "    for x in tmp.glob(\"*\"):\n",
    "        try: x.unlink()\n",
    "        except: shutil.rmtree(x, ignore_errors=True)\n",
    "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    out = tmp / f\"downloaded_{ts}.mp4\"\n",
    "    if DEBUG_MODE: print(\"[Download] Getting YouTube video ...\")\n",
    "    # Use sp.run instead of subprocess.run directly\n",
    "    cmd = [\"yt-dlp\", \"-f\", \"mp4\", \"-o\", str(tmp / \"%(title)s.%(ext)s\")]\n",
    "    if youtube_cookies_txt_path.strip():\n",
    "        cookies_abs = to_abs_mydrive(youtube_cookies_txt_path.strip())\n",
    "        if cookies_abs.exists():\n",
    "            cmd += [\"--cookies\", str(cookies_abs)]\n",
    "        else:\n",
    "            if DEBUG_MODE: print(f\"⚠️ 找不到 cookies 檔：{cookies_abs}（改為不帶 cookies）\")\n",
    "    cmd.append(yturl)\n",
    "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
    "    if DEBUG_MODE and p.stdout: sys.stdout.write(p.stdout)\n",
    "    if p.returncode != 0:\n",
    "        if \"Sign in to confirm\" in (p.stdout or \"\"):\n",
    "            print(\"\\n❗YouTube 要求登入/驗證，請提供 cookies 或先自行下載到雲端硬碟。\")\n",
    "        print(\"🔄 若多次失敗，請刪除執行階段並重啟後重試。\")\n",
    "        suggest_runtime_reset()\n",
    "        raise RuntimeError(\"yt-dlp 下載失敗\")\n",
    "    files = list(tmp.glob(\"*\"))\n",
    "    if not files:\n",
    "        print(\"🔄 下載為空，建議刪除執行階段再重試。\")\n",
    "        suggest_runtime_reset()\n",
    "        raise FileNotFoundError(\"YouTube 下載失敗：/tmp/dl 為空\")\n",
    "    f = files[0]\n",
    "    if save_video_to_google_drive:\n",
    "        shutil.copy2(f, WHISPER_DIR / f.name)\n",
    "    return f\n",
    "\n",
    "def http_dl(url:str)->Path:\n",
    "    tmp = Path(\"/tmp/dl\"); tmp.mkdir(parents=True, exist_ok=True)\n",
    "    for x in tmp.glob(\"*\"):\n",
    "        try: x.unlink()\n",
    "        except: shutil.rmtree(x, ignore_errors=True)\n",
    "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    out = tmp / f\"downloaded_{ts}.mp4\"\n",
    "    if DEBUG_MODE: print(\"[Download] Getting HTTP(S) video ...\")\n",
    "    run_cmd([\"curl\", \"-L\", \"-o\", str(out), url])\n",
    "    if save_video_to_google_drive:\n",
    "        shutil.copy2(out, WHISPER_DIR / out.name)\n",
    "    return out\n",
    "\n",
    "# Extract audio: ffmpeg -> 16k/mono WAV\n",
    "def ffmpeg_extract_wav(in_path:Path, out_wav:Path, sr=16000):\n",
    "    cmd = [\"ffmpeg\",\"-y\",\"-i\",str(in_path),\"-vn\",\"-ac\",\"1\",\"-ar\",str(sr),\"-f\",\"wav\",str(out_wav)]\n",
    "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
    "    if p.returncode != 0:\n",
    "        if DEBUG_MODE: print(p.stdout)\n",
    "        raise RuntimeError(\"ffmpeg 轉 WAV 失敗\")\n",
    "\n",
    "# CPU Denoising: ffmpeg afftdn\n",
    "def ffmpeg_afftdn(in_wav: Path, out_wav: Path, noise_floor_db=DENOISE_NOISE_FLOOR_DB):\n",
    "    cmd = [\"ffmpeg\",\"-y\",\"-i\",str(in_wav),\"-af\",f\"afftdn=nf={noise_floor_db}\",\n",
    "           \"-ac\",\"1\",\"-ar\",\"16000\",\"-f\",\"wav\",str(out_wav)]\n",
    "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
    "    if p.returncode != 0:\n",
    "        if DEBUG_MODE: print(p.stdout)\n",
    "        raise RuntimeError(\"ffmpeg afftdn 失敗\")\n",
    "\n",
    "# Safeguard: Repack WAV header if format is strange\n",
    "def ffmpeg_repack_wav(in_wav: Path, out_wav: Path, sr=16000):\n",
    "    cmd = [\"ffmpeg\",\"-y\",\"-i\",str(in_wav),\"-acodec\",\"pcm_s16le\",\"-ac\",\"1\",\"-ar\",str(sr),str(out_wav)]\n",
    "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
    "    if p.returncode != 0:\n",
    "        if DEBUG_MODE: print(p.stdout)\n",
    "        raise RuntimeError(\"ffmpeg 重包 WAV 失敗\")\n",
    "\n",
    "# [4/8] Parse Source (Transcription) - Uses 'filename' and 'save_video_to_google_drive'\n",
    "if DEBUG_MODE: print(\"[4/8] Parsing input source ...\")\n",
    "try:\n",
    "    if is_youtube_url(filename):\n",
    "        src_path = ytdl(filename); out_base_dir = WHISPER_DIR\n",
    "    elif is_http_url(filename):\n",
    "        src_path = http_dl(filename); out_base_dir = WHISPER_DIR\n",
    "    else:\n",
    "        src_path = to_abs_mydrive(filename)\n",
    "        if not src_path.exists(): raise FileNotFoundError(f\"找不到檔案：{src_path}\")\n",
    "        out_base_dir = src_path.parent\n",
    "except Exception as e:\n",
    "    print(f\"\\n⛔ 來源解析/下載失敗：{e}\")\n",
    "    print(\"🔄 請刪除執行階段並重新啟動後重跑。\"); suggest_runtime_reset(); raise\n",
    "\n",
    "print(f\"→ 來源檔：{src_path}\")\n",
    "print(f\"→ 輸出資料夾：{out_base_dir}\")\n",
    "\n",
    "# [5/8] Extract Audio & CPU Denoising (Transcription) - Uses 'denoise_method' and 'DENOISE_NOISE_FLOOR_DB'\n",
    "AUDIO_16K = Path(\"/tmp/audio_16k.wav\")\n",
    "if DEBUG_MODE: print(\"[5/8] Extracting audio (ffmpeg → 16k/mono WAV) ...\")\n",
    "ffmpeg_extract_wav(src_path, AUDIO_16K, sr=16000)\n",
    "\n",
    "if denoise_method.startswith(\"afftdn\"):\n",
    "    if DEBUG_MODE: print(\"[5.5/8] Denoising (ffmpeg afftdn, CPU) ...\")\n",
    "    DENOISED = Path(\"/tmp/audio_16k_denoised.wav\")\n",
    "    ffmpeg_afftdn(AUDIO_16K, DENOISED, noise_floor_db=DENOISE_NOISE_FLOOR_DB)\n",
    "    denoised_audio = DENOISED if verify_wav_ok(DENOISED) else AUDIO_16K\n",
    "else:\n",
    "    denoised_audio = AUDIO_16K\n",
    "\n",
    "if not verify_wav_ok(denoised_audio):\n",
    "    if DEBUG_MODE: print(\"  - 音訊格式異常；嘗試重包 WAV ...\")\n",
    "    FIXED = Path(\"/tmp/audio_16k_fixed.wav\")\n",
    "    ffmpeg_repack_wav(denoised_audio, FIXED, sr=16000)\n",
    "    denoised_audio = FIXED\n",
    "\n",
    "if DEBUG_MODE: print(f\"→ 最終輸入音訊：{denoised_audio}\")\n",
    "\n",
    "# [6/8] Load faster-whisper (GPU enforced) - Uses 'model_size'\n",
    "if DEBUG_MODE: print(\"[6/8] Loading faster-whisper model (GPU) ...\")\n",
    "device = \"cuda\"  # Enforce GPU\n",
    "model = None; last_err = None\n",
    "for ctype in [\"int8_float16\", \"float16\", \"int8\"]:\n",
    "    try:\n",
    "        if DEBUG_MODE: print(f\"  - Trying compute_type={ctype}\")\n",
    "        model = WhisperModel(model_size, device=device, compute_type=ctype)\n",
    "        if DEBUG_MODE: print(\"  - Model loaded successfully\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "        if DEBUG_MODE: print(f\"  - Load failed: {e}\")\n",
    "if model is None:\n",
    "    print(\"\\n⛔ GPU 模型載入失敗。請確認『變更執行階段類型』選了 GPU（T4/A100），或刪除執行階段後重試。\")\n",
    "    suggest_runtime_reset()\n",
    "    raise RuntimeError(f\"無法載入模型：{last_err}\")\n",
    "\n",
    "gc.collect()  # Clean up before transcription (safety)\n",
    "\n",
    "# [7/8] Transcribe (GPU; real-time progress per segment) - Uses 'language_code', 'TRANSCRIPTION_BEAM_SIZE_PRIMARY', 'TRANSCRIPTION_CHUNK_LENGTH_PRIMARY', 'TRANSCRIPTION_BEAM_SIZE_FALLBACK', 'TRANSCRIPTION_CHUNK_LENGTH_FALLBACK'\n",
    "if DEBUG_MODE: print(f\"[7/8] Starting transcription (GPU: beam={TRANSCRIPTION_BEAM_SIZE_PRIMARY} / chunk={TRANSCRIPTION_CHUNK_LENGTH_PRIMARY}s / no VAD) ...\")\n",
    "\n",
    "def transcribe_gpu(_beam=TRANSCRIPTION_BEAM_SIZE_PRIMARY, _chunk=TRANSCRIPTION_CHUNK_LENGTH_PRIMARY):\n",
    "    return model.transcribe(\n",
    "        str(denoised_audio),\n",
    "        task=\"transcribe\",\n",
    "        language=language_code,\n",
    "        temperature=0.0,\n",
    "        condition_on_previous_text=True,\n",
    "        compression_ratio_threshold=2.4,\n",
    "        log_prob_threshold=-1.0,\n",
    "        no_speech_threshold=0.6,\n",
    "        beam_size=_beam,\n",
    "        chunk_length=_chunk,\n",
    "        vad_filter=False,\n",
    "        word_timestamps=False\n",
    "    )\n",
    "\n",
    "try:\n",
    "    seg_iter, info = transcribe_gpu(_beam=TRANSCRIPTION_BEAM_SIZE_PRIMARY, _chunk=TRANSCRIPTION_CHUNK_LENGTH_PRIMARY)\n",
    "except Exception as e:\n",
    "    if DEBUG_MODE: print(f\"  - First transcription failed: {e}\\n    → Trying more conservative (beam={TRANSCRIPTION_BEAM_SIZE_FALLBACK}, chunk={TRANSCRIPTION_CHUNK_LENGTH_FALLBACK}) ...\")\n",
    "    seg_iter, info = transcribe_gpu(_beam=TRANSCRIPTION_BEAM_SIZE_FALLBACK, _chunk=TRANSCRIPTION_CHUNK_LENGTH_FALLBACK)\n",
    "\n",
    "# Display percentage based on total video duration\n",
    "duration = float(getattr(info, \"duration\", 0.0) or 0.0)\n",
    "if duration <= 0: duration = 1.0\n",
    "\n",
    "segments = []\n",
    "filtered = []\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    print(f\"  - Detected language: {getattr(info,'language','未知')} (p={getattr(info,'language_probability',0):.2f})\")\n",
    "    print(f\"  - Audio length: {duration:.2f}s\")\n",
    "\n",
    "for s in seg_iter:\n",
    "    pct = int(min(100, round((s.end / duration) * 100)))\n",
    "    print(f\"[{pct:3d}%] {fmt_ts_srt(s.start)} → {fmt_ts_srt(s.end)}  {s.text.strip()}\", flush=True)\n",
    "    segments.append(s)\n",
    "\n",
    "    # Low confidence/high no-speech short segment filtering (no blacklist) - Uses FILTER_* parameters\n",
    "    keep = True\n",
    "    seg_dur = float(s.end - s.start)\n",
    "    if seg_dur < FILTER_MIN_DURATION_SHORT and getattr(s, \"avg_logprob\", None) is not None and s.avg_logprob < FILTER_AVG_LOGPROB_THRESHOLD:\n",
    "        keep = False\n",
    "    if seg_dur < FILTER_MIN_DURATION_SPEECH_PROB and getattr(s, \"no_speech_prob\", None) is not None and s.no_speech_prob > FILTER_NO_SPEECH_PROB_THRESHOLD:\n",
    "        keep = False\n",
    "    if keep:\n",
    "        filtered.append(s)\n",
    "\n",
    "if DEBUG_MODE: print(f\"  - Number of segments: Before filtering {len(segments)} → After filtering {len(filtered)}\")\n",
    "\n",
    "# ---- OpenCC Normalization (for output text) ---- - Uses 'text_postprocess'\n",
    "pipeline = build_opencc_pipeline(text_postprocess)\n",
    "def norm(txt: str) -> str:\n",
    "    return apply_opencc(txt, pipeline) if pipeline else txt\n",
    "\n",
    "# [8/8] Output (text after OpenCC) - Uses 'out_base_dir' (derived from 'filename')\n",
    "print(\"[8/8] 輸出 SRT / TXT ...\")\n",
    "# Determine the output directory for transcription based on input type\n",
    "# If input is a network source, output to WHISPER_DIR\n",
    "# If input is a local file, output to the same directory as the input file\n",
    "if is_youtube_url(filename) or is_http_url(filename):\n",
    "    out_base_dir = WHISPER_DIR\n",
    "else:\n",
    "    src_path_abs = to_abs_mydrive(filename)\n",
    "    out_base_dir = src_path_abs.parent\n",
    "\n",
    "# Create the transcription output directory if it doesn't exist\n",
    "out_dir = out_base_dir\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Determine the stem from the original source file path\n",
    "stem = Path(src_path).stem\n",
    "SRT = out_dir / f\"{stem}.srt\"\n",
    "TXT = out_dir / f\"{stem}.txt\"\n",
    "\n",
    "with open(SRT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, s in enumerate(filtered, 1):\n",
    "        text_out = norm(s.text.strip())\n",
    "        f.write(f\"{i}\\n{fmt_ts_srt(s.start)} --> {fmt_ts_srt(s.end)}\\n{text_out}\\n\\n\")\n",
    "\n",
    "with open(TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in filtered:\n",
    "        f.write(norm(s.text.strip()) + \"\\n\")  # Each segment on a new line\n",
    "\n",
    "print(f\"→ 完成！\\n  SRT: {SRT}\\n  TXT: {TXT}\")\n",
    "\n",
    "# Release model (release GPU memory)\n",
    "try: del model\n",
    "except: pass\n",
    "gc.collect()\n",
    "if DEBUG_MODE: print(\"→ Model released; can run again directly if needed.\")\n",
    "\n",
    "\n",
    "# ===== Summarization Logic Starts Here =====\n",
    "# Use SRT from transcription step for summarization\n",
    "summary_srt_path_abs = SRT\n",
    "assert summary_srt_path_abs.exists(), f\"SRT 檔不存在：{summary_srt_path_abs}\"\n",
    "\n",
    "# ===== Summary 1/6) Check GPU and Install Dependencies (llama-cpp-python specific) =====\n",
    "# llama-cpp-python installation logic - Keep this separate as it has specific CUDA requirements\n",
    "# Moved this section to just before reading the SRT for summarization\n",
    "if DEBUG_MODE: print(\"[Summary 1/6] Checking GPU and installing llama-cpp-python ...\")\n",
    "\n",
    "def detect_cuda_tag():\n",
    "    try:\n",
    "        out = sp.check_output([\"nvidia-smi\"], text=True)\n",
    "        m = re.search(r\"CUDA Version:\\s*([\\d.]+)\", out)\n",
    "        if not m:\n",
    "            return \"cu124\"\n",
    "        major, minor = [int(x) for x in m.group(1).split(\".\")[:2]]\n",
    "        if major > 12 or (major == 12 and minor >= 5):\n",
    "            return \"cu125\"\n",
    "        return \"cu124\"\n",
    "    except Exception:\n",
    "        return \"cu124\"\n",
    "\n",
    "cuda_tag = detect_cuda_tag()\n",
    "if DEBUG_MODE: print(f\"GPU 0: Detected CUDA version tag {cuda_tag}\")\n",
    "\n",
    "def try_import_llama():\n",
    "    try:\n",
    "        from llama_cpp import Llama\n",
    "        return Llama\n",
    "    except ModuleNotFoundError:\n",
    "        return None\n",
    "\n",
    "Llama = try_import_llama()\n",
    "if Llama is None:\n",
    "    # Keep your existing installation strategy: extra-index -> fallback to source compilation on failure\n",
    "    candidates = [cuda_tag, \"cu125\", \"cu124\", \"cu122\", \"cu121\"]\n",
    "    ok = False\n",
    "    for tag in candidates:\n",
    "        idx = f\"https://abetlen.github.io/llama-cpp-python/whl/{tag}\"\n",
    "        if DEBUG_MODE: print(f\"→ Attempting to install llama-cpp-python ({tag}) ...\")\n",
    "        r = pip_install([\"llama-cpp-python\"], extra_args=[\"--extra-index-url\", idx])\n",
    "        if r.returncode == 0:\n",
    "            Llama = try_import_llama()\n",
    "            if Llama is not None:\n",
    "                ok = True\n",
    "                break\n",
    "        else:\n",
    "            if DEBUG_MODE: print(\"  ✗ Installation failed (summary):\", \"\\n\".join(r.stdout.splitlines()[-5:]))\n",
    "    if not ok:\n",
    "        if DEBUG_MODE: print(\"→ Pre-compiled wheels not available, switching to 'source compilation (CUDA=ON)' ... (takes longer)\")\n",
    "        try:\n",
    "            import ninja # noqa: F401 # Import ninja to check if installed\n",
    "        except ModuleNotFoundError:\n",
    "            if DEBUG_MODE: print(\"→ Installing missing package: ninja\")\n",
    "            r = pip_install([\"ninja\"])\n",
    "            if r.returncode != 0:\n",
    "                if DEBUG_MODE: print(r.stdout)\n",
    "                raise RuntimeError(\"安裝 ninja 失敗。請重啟後重試。\")\n",
    "        env = os.environ.copy()\n",
    "        env[\"CMAKE_ARGS\"] = \"-DGGML_CUDA=on -DLLAMA_CUBLAS=on\"\n",
    "        env[\"FORCE_CMAKE\"] = \"1\"\n",
    "        r = pip_install([\"llama-cpp-python\"], env=env)\n",
    "        if r.returncode != 0:\n",
    "            if DEBUG_MODE: print(r.stdout)\n",
    "            raise RuntimeError(\"無法安裝 GPU 版 llama-cpp-python。\")\n",
    "        Llama = try_import_llama()\n",
    "\n",
    "\n",
    "if DEBUG_MODE: print(\"[Summary 2/6] Reading SRT ...\")\n",
    "with open(summary_srt_path_abs, \"r\", encoding=\"utf-8\") as f:\n",
    "    srt_text = f.read()\n",
    "subs = list(_srt.parse(srt_text)) # Use _srt as srt module was imported as _srt\n",
    "def td2s(td): return td.total_seconds()\n",
    "segments = []\n",
    "for it in subs:\n",
    "    txt = it.content.strip()\n",
    "    if not txt: continue\n",
    "    segments.append((td2s(it.start), td2s(it.end), txt))\n",
    "total_secs = (segments[-1][1] - segments[0][0]) if segments else 0\n",
    "if DEBUG_MODE: print(f\"→ Number of subtitle segments: {len(segments)}；Video length (est): {total_secs/60:.1f} minutes\")\n",
    "\n",
    "\n",
    "# ===== Summary 3/6) Download and Load GGUF Model (Summary) - Uses summary model parameters (REPO_ID, GGUF_FILE, ctx_window, etc.)\n",
    "# Moved this section to just after installing llama-cpp-python\n",
    "if DEBUG_MODE: print(\"[Summary 3/6] Loading GPT-OSS-20B (GGUF, CUDA) ...\")\n",
    "local_repo = snapshot_download(REPO_ID, allow_patterns=[GGUF_FILE, \"tokenizer_config.json\"])\n",
    "gguf_path = str(Path(local_repo)/GGUF_FILE)\n",
    "\n",
    "tokenizer_config_path = Path(local_repo)/\"tokenizer_config.json\"\n",
    "if tokenizer_config_path.exists():\n",
    "    try:\n",
    "        tokenizer_config_data = json.loads(tokenizer_config_path.read_text())\n",
    "        if DEBUG_MODE: print(\"→ Loaded tokenizer_config.json (Harmony template)\")\n",
    "    except Exception as exc:\n",
    "        if DEBUG_MODE: print(\"  ✗ Failed to parse tokenizer_config.json:\", exc)\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=gguf_path,\n",
    "    n_ctx=ctx_window,\n",
    "    n_gpu_layers=-1,\n",
    "    seed=0,\n",
    "    logits_all=False,\n",
    "    verbose=True          # Display the actual chat format used\n",
    ")\n",
    "if DEBUG_MODE: print(\"→ Model loaded successfully (GPU)\")\n",
    "\n",
    "def ensure_harmony_formatter():\n",
    "    \"\"\"Ensure the Harmony chat formatter is available for GPT-OSS prompts.\"\"\"\n",
    "    global harmony_chat_formatter\n",
    "    if harmony_chat_formatter is not None:\n",
    "        return harmony_chat_formatter\n",
    "    try:\n",
    "        from llama_cpp.llama_chat_format import (\n",
    "            hf_tokenizer_config_to_chat_formatter,\n",
    "            Jinja2ChatFormatter,\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(\"llama_cpp Harmony chat helpers are unavailable\") from exc\n",
    "\n",
    "    formatter = None\n",
    "    if tokenizer_config_data:\n",
    "        try:\n",
    "            formatter = hf_tokenizer_config_to_chat_formatter(\n",
    "                tokenizer_config_data,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "        except Exception as exc:\n",
    "            if DEBUG_MODE:\n",
    "                print(\"  ✗ Failed to initialize Harmony formatter from tokenizer_config.json:\", exc)\n",
    "\n",
    "    if formatter is None:\n",
    "        template = None\n",
    "        if 'llm' in globals() and hasattr(llm, 'metadata'):\n",
    "            template = llm.metadata.get('tokenizer.chat_template')\n",
    "        if template:\n",
    "            try:\n",
    "                bos_token = (\n",
    "                    llm.metadata.get('tokenizer.bos_token')\n",
    "                    or llm.metadata.get('tokenizer.ggml.bos_token')\n",
    "                    or llm.detokenize([llm.token_bos()], special=True).decode('utf-8', errors='ignore')\n",
    "                )\n",
    "                eos_token = (\n",
    "                    llm.metadata.get('tokenizer.eos_token')\n",
    "                    or llm.metadata.get('tokenizer.ggml.eos_token')\n",
    "                    or llm.detokenize([llm.token_eos()], special=True).decode('utf-8', errors='ignore')\n",
    "                )\n",
    "                formatter = Jinja2ChatFormatter(\n",
    "                    template,\n",
    "                    eos_token=eos_token,\n",
    "                    bos_token=bos_token,\n",
    "                    stop_token_ids=[llm.token_eos()],\n",
    "                )\n",
    "            except Exception as exc:\n",
    "                if DEBUG_MODE:\n",
    "                    print(\"  ✗ Failed to build Harmony formatter from GGUF metadata:\", exc)\n",
    "\n",
    "    if formatter is None:\n",
    "        raise RuntimeError(\"Harmony chat formatter could not be prepared\")\n",
    "\n",
    "    harmony_chat_formatter = formatter\n",
    "    return harmony_chat_formatter\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    ensure_harmony_formatter()\n",
    "    if DEBUG_MODE: print(\"→ Harmony formatter prepared\")\n",
    "except Exception as exc:\n",
    "    raise RuntimeError(f\"Failed to prepare Harmony formatter: {exc}\")\n",
    "\n",
    "\n",
    "# ===== Summary 4/6) Token-aware Segmentation (Summary) - Uses ctx_window, map_max_new_tokens, prompt_overhead\n",
    "if DEBUG_MODE: print(\"[Summary 4/6] Generating segments (token-aware; single segment ≤ safety limit) ...\")\n",
    "\n",
    "def count_tokens_text(text: str) -> int:\n",
    "    # Check if llm is initialized before using it\n",
    "    if 'llm' not in globals() or llm is None:\n",
    "         raise RuntimeError(\"LLM model is not loaded. Cannot count tokens.\")\n",
    "    return len(llm.tokenize(text.encode(\"utf-8\")))\n",
    "\n",
    "SYSTEM_INSTR = (\n",
    "  \"你是一個會議總結機器人。根據使用者提供的逐字稿（可能雜訊、重複、錯字），\"\n",
    "  \"請去除雜訊與重複、嚴守事實、不腦補。遇到不明確資訊以「待補充／未明確」標註。\"\n",
    "  \"輸出為 Markdown（繁體中文），不要輸出任何系統／思考標記。\"\n",
    ")\n",
    "\n",
    "# — Segment Summary Prompt: More concise request, avoid verbosity and system language - Uses 'topic_hint'\n",
    "MAP_USER_TMPL = textwrap.dedent(\"\"\"\\\n",
    "主題（可留空）：{topic}\n",
    "\n",
    "以下是逐字稿片段（非完整全文）：\n",
    "{chunk}\n",
    "\n",
    "請就此片段輸出「條列式重點摘要」（500–900 字，繁體中文），注意：\n",
    "- 只寫最終內容，不要寫解題想法、不要出現任何系統提示或中英括號標記。\n",
    "- 聚焦可驗證事實（時間、人物、任務、結論、未決事項、行動）。\n",
    "- 結構：可用小標題＋項目符號，語句務必短、準確、無贅詞。\n",
    "\"\"\")\n",
    "\n",
    "# — Summary Prompt: Maintain your three-section output structure - Uses 'topic_hint'\n",
    "REDUCE_USER_TMPL = textwrap.dedent(\"\"\"\\\n",
    "主題（可留空）：{topic}\n",
    "\n",
    "以下是所有片段的重點摘要彙整（仍可能有重疊）：\n",
    "{maps}\n",
    "\n",
    "請整合為一份會議筆記（Markdown，繁體）：\n",
    "1) **整體提要**（3–6 句，避免冗言）\n",
    "2) **章節要點（含時間脈絡）**：條列呈現，每點一行，可附粗略時間\n",
    "3) **可執行重點**：具體待辦（每條以動詞開頭）\n",
    "請只輸出最終筆記，不要出現系統或思考標記，不要加入未出現的新資訊。\n",
    "\"\"\")\n",
    "\n",
    "# Single segment token budget (reserve space for prompt and generation)\n",
    "prompt_overhead = 700\n",
    "chunk_target    = max(1024, min(3072, ctx_window - prompt_overhead - map_max_new_tokens))\n",
    "\n",
    "chunks: List[Tuple[float,float,str]] = []\n",
    "buf, t0, t1, cur = [], None, None, 0\n",
    "for (s, e, txt) in segments:\n",
    "    t = count_tokens_text(txt)\n",
    "    if not buf:\n",
    "        buf, t0, t1, cur = [txt], s, e, t\n",
    "        continue\n",
    "    if cur + t <= chunk_target:\n",
    "        buf.append(txt); t1 = e; cur += t\n",
    "    else:\n",
    "        chunks.append((t0, t1, \"\\n\".join(buf)))\n",
    "        buf, t0, t1, cur = [txt], s, e, t\n",
    "if buf:\n",
    "    chunks.append((t0, t1, \"\\n\".join(buf)))\n",
    "\n",
    "if DEBUG_MODE: print(f\"→ Generated {len(chunks)} segments (target ~{chunk_target} tokens per segment)\")\n",
    "\n",
    "# ===== Common: Streaming Tools (No regex cleaning; use correct stop sequence) - Uses temperature, top_p, repeat_penalty, map_max_new_tokens, reduce_max_new_tokens\n",
    "\n",
    "\n",
    "def stream_harmony_final_pieces(text_chunks: Iterable[str]) -> Iterator[str]:\n",
    "    \"\"\"Yield Harmony streamed text, preferring the final channel.\n",
    "\n",
    "    Some community models only emit the assistant channel; fall back to it\n",
    "    so we do not drop the actual content.\n",
    "    \"\"\"\n",
    "    buffer = \"\"\n",
    "    current_channel: Optional[str] = None\n",
    "    pending_channel = False\n",
    "    channel_name_buffer = \"\"\n",
    "    in_message = False\n",
    "    assistant_cache: List[str] = []\n",
    "    final_seen = False\n",
    "\n",
    "    def canonical_channel(name: Optional[str]) -> str:\n",
    "        if not name:\n",
    "            return \"\"\n",
    "        lowered = name.strip().lower()\n",
    "        if not lowered:\n",
    "            return \"\"\n",
    "        if \"final\" in lowered:\n",
    "            return \"final\"\n",
    "        if \"assistant\" in lowered:\n",
    "            return \"assistant\"\n",
    "        return lowered\n",
    "\n",
    "    def emit_text(text: str):\n",
    "        nonlocal final_seen, assistant_cache\n",
    "        if not text:\n",
    "            return\n",
    "        channel = canonical_channel(current_channel)\n",
    "        if channel == \"final\":\n",
    "            final_seen = True\n",
    "            if assistant_cache:\n",
    "                cached = assistant_cache[:]\n",
    "                assistant_cache.clear()\n",
    "                for cached_piece in cached:\n",
    "                    if cached_piece:\n",
    "                        yield cached_piece\n",
    "            yield text\n",
    "        elif channel == \"assistant\":\n",
    "            if final_seen:\n",
    "                yield text\n",
    "            else:\n",
    "                assistant_cache.append(text)\n",
    "        elif final_seen:\n",
    "            yield text\n",
    "\n",
    "    for piece in text_chunks:\n",
    "        if not piece:\n",
    "            continue\n",
    "        buffer += piece\n",
    "        while True:\n",
    "            if pending_channel:\n",
    "                idx = buffer.find(\"<|\")\n",
    "                if idx == -1:\n",
    "                    channel_name_buffer += buffer\n",
    "                    buffer = \"\"\n",
    "                    break\n",
    "                channel_name_buffer += buffer[:idx]\n",
    "                buffer = buffer[idx:]\n",
    "                channel = channel_name_buffer.strip()\n",
    "                channel_name_buffer = \"\"\n",
    "                pending_channel = False\n",
    "                current_channel = channel\n",
    "                channel_canonical = canonical_channel(current_channel)\n",
    "                in_message = bool(channel_canonical in {\"assistant\", \"final\"})\n",
    "                continue\n",
    "            tag_start = buffer.find(\"<|\")\n",
    "            if tag_start == -1:\n",
    "                if in_message or canonical_channel(current_channel) in {\"assistant\", \"final\"}:\n",
    "                    for out in emit_text(buffer):\n",
    "                        yield out\n",
    "                    buffer = \"\"\n",
    "                else:\n",
    "                    if len(buffer) > 128:\n",
    "                        buffer = buffer[-128:]\n",
    "                break\n",
    "            if tag_start > 0:\n",
    "                text = buffer[:tag_start]\n",
    "                if in_message or canonical_channel(current_channel) in {\"assistant\", \"final\"}:\n",
    "                    for out in emit_text(text):\n",
    "                        yield out\n",
    "                buffer = buffer[tag_start:]\n",
    "            tag_end = buffer.find(\"|>\")\n",
    "            if tag_end == -1:\n",
    "                break\n",
    "            tag = buffer[2:tag_end].strip().lower()\n",
    "            buffer = buffer[tag_end + 2:]\n",
    "            if tag == \"start\":\n",
    "                current_channel = None\n",
    "                in_message = False\n",
    "            elif tag == \"channel\":\n",
    "                pending_channel = True\n",
    "            elif tag == \"message\":\n",
    "                in_message = True\n",
    "            elif tag == \"end\":\n",
    "                in_message = False\n",
    "                current_channel = None\n",
    "            elif tag == \"return\":\n",
    "                if not final_seen and assistant_cache:\n",
    "                    for cached_piece in assistant_cache:\n",
    "                        if cached_piece:\n",
    "                            yield cached_piece\n",
    "                    assistant_cache.clear()\n",
    "                return\n",
    "            else:\n",
    "                continue\n",
    "    if (in_message or canonical_channel(current_channel) in {\"assistant\", \"final\"}) and buffer:\n",
    "        for out in emit_text(buffer):\n",
    "            yield out\n",
    "    if not final_seen and assistant_cache:\n",
    "        for cached_piece in assistant_cache:\n",
    "            if cached_piece:\n",
    "                yield cached_piece\n",
    "\n",
    "def llm_stream(messages, max_tokens):\n",
    "    \"\"\"Stream Harmony-formatted completions and yield only the final channel.\"\"\"\n",
    "    if 'llm' not in globals() or llm is None:\n",
    "        raise RuntimeError(\"LLM model is not loaded. Cannot stream generation.\")\n",
    "    formatter = ensure_harmony_formatter()\n",
    "    chat_response = formatter(\n",
    "        messages=messages,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    completion_kwargs = dict(\n",
    "        prompt=chat_response.prompt,\n",
    "        temperature=float(temperature),\n",
    "        top_p=float(top_p),\n",
    "        repeat_penalty=float(repeat_penalty),\n",
    "        max_tokens=int(max_tokens),\n",
    "        stream=True,\n",
    "    )\n",
    "    if chat_response.stop:\n",
    "        completion_kwargs[\"stop\"] = chat_response.stop\n",
    "    if chat_response.stopping_criteria is not None:\n",
    "        completion_kwargs[\"stopping_criteria\"] = chat_response.stopping_criteria\n",
    "\n",
    "    gen = llm.create_completion(**completion_kwargs)\n",
    "\n",
    "    def _iter_text_stream(events):\n",
    "        for ev in events:\n",
    "            yield ev.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "\n",
    "    text_stream = _iter_text_stream(gen)\n",
    "\n",
    "    for final_piece in stream_harmony_final_pieces(text_stream):\n",
    "        if final_piece:\n",
    "            yield final_piece\n",
    "\n",
    "# ===== Summary 5/6) Segment Summary (map) - Uses map_max_new_tokens, ctx_window, prompt_overhead, topic_hint\n",
    "if DEBUG_MODE: print(\"[Summary 5/6] Segment summarization (map) ...\")\n",
    "live = display(Markdown(\"\"), display_id=True)\n",
    "maps: List[str] = []\n",
    "\n",
    "\n",
    "def escape_braces(text: str) -> str:\n",
    "    \"\"\"Escape braces so str.format does not treat user content as placeholders.\"\"\"\n",
    "    return text.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "\n",
    "safe_topic_hint = escape_braces(topic_hint or \"（無）\")\n",
    "\n",
    "for i, (s, e, body) in enumerate(chunks, 1):\n",
    "    pct = i / max(len(chunks),1) * 100\n",
    "    sys.stdout.write(f\"  - 處理分段 {i}/{len(chunks)}（~{pct:.1f}%）\\n\"); sys.stdout.flush()\n",
    "\n",
    "    # Shrink to safe budget before sending (prevent prompt+segment from exceeding window and causing model to terminate early)\n",
    "    budget_tokens = max(512, ctx_window - map_max_new_tokens - prompt_overhead)\n",
    "    def shrink_to_budget(text: str, budget_tokens: int) -> str:\n",
    "        cur = text\n",
    "        for _ in range(6):\n",
    "            if count_tokens_text(cur) <= budget_tokens:\n",
    "                return cur\n",
    "            keep = max(800, int(len(cur) * 0.85))\n",
    "            cur = cur[:keep]\n",
    "        return cur\n",
    "    body2 = shrink_to_budget(body, budget_tokens)\n",
    "\n",
    "    safe_body = escape_braces(body2)\n",
    "    user_txt = MAP_USER_TMPL.format(topic=safe_topic_hint, chunk=safe_body)\n",
    "    user_txt = user_txt.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_INSTR},\n",
    "        {\"role\": \"user\",   \"content\": user_txt},\n",
    "    ]\n",
    "\n",
    "    part_buf = [] # Reset part_buf for each segment\n",
    "    for token in llm_stream(messages, map_max_new_tokens):\n",
    "        part_buf.append(token)\n",
    "        # Update live display and terminal character count periodically\n",
    "        if len(part_buf) % 24 == 0:\n",
    "            cur_txt = \"\".join(part_buf)\n",
    "            live.update(Markdown(cur_txt))\n",
    "            sys.stdout.write(f\"    ↳ 分段 {i} 已產生字元：{len(cur_txt)}\\n\"); sys.stdout.flush()\n",
    "    cur_txt = \"\".join(part_buf)\n",
    "    live.update(Markdown(cur_txt))\n",
    "    sys.stdout.write(f\"    ↳ 分段 {i} 已產生字元：{len(cur_txt)}\\n\"); sys.stdout.flush()\n",
    "\n",
    "    # Include the model's final output directly, no regex cleaning\n",
    "    maps.append(cur_txt.strip())\n",
    "\n",
    "if DEBUG_MODE: print(\"→ Segment summarization complete\")\n",
    "\n",
    "if DEBUG_MODE: print(\"[Summary 6/6] Consolidating summary (reduce) ...\")\n",
    "maps_md = \"\\n\\n---\\n\\n\".join(f\"### 片段 {i+1} 要點\\n\\n{m}\" for i, m in enumerate(maps))\n",
    "\n",
    "# If combined text exceeds window, truncate proportionally first (without changing text within segments to avoid breaking meaning)\n",
    "def fit_reduce_payload(md_text: str, max_ctx_tokens: int) -> str:\n",
    "    for _ in range(8):\n",
    "        need = count_tokens_text(md_text)\n",
    "        if need + reduce_max_new_tokens + 400 <= max_ctx_tokens:\n",
    "            return md_text\n",
    "        md_text = md_text[: int(len(md_text) * 0.9)]\n",
    "    return md_text\n",
    "\n",
    "md_cur = fit_reduce_payload(maps_md, ctx_window)\n",
    "\n",
    "safe_md_cur = escape_braces(md_cur)\n",
    "user_txt = REDUCE_USER_TMPL.format(topic=safe_topic_hint, maps=safe_md_cur)\n",
    "user_txt = user_txt.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
    "messages = [{\"role\":\"system\",\"content\":SYSTEM_INSTR},\n",
    "            {\"role\":\"user\",\"content\":user_txt}]\n",
    "\n",
    "live2 = display(Markdown(\"\"), display_id=True)\n",
    "final_buf = []\n",
    "for token in llm_stream(messages, reduce_max_new_tokens):\n",
    "    final_buf.append(token)\n",
    "    if len(final_buf) % 24 == 0:\n",
    "        live2.update(Markdown(\"\".join(final_buf)))\n",
    "        sys.stdout.write(f\"    ↳ 彙整 已產生字元：{len(''.join(final_buf))}\\n\"); sys.stdout.flush()\n",
    "live2.update(Markdown(\"\".join(final_buf)))\n",
    "sys.stdout.write(f\"    ↳ 彙整 已產生字元：{len(''.join(final_buf))}\\n\"); sys.stdout.flush()\n",
    "\n",
    "final_text = \"\".join(final_buf).strip()\n",
    "\n",
    "# Determine and create the summary output directory\n",
    "summary_output_dir_abs = out_base_dir\n",
    "summary_output_dir_abs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Determine the summary output file path using the stem of the input SRT\n",
    "out_md = summary_output_dir_abs / f\"{Path(summary_srt_path_abs).stem}_summary.md\"\n",
    "\n",
    "\n",
    "with open(out_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_text)\n",
    "\n",
    "print(f\"→ 完成 ✅  {out_md}\")\n",
    "try:\n",
    "    del llm\n",
    "except Exception:\n",
    "    pass\n",
    "gc.collect()\n",
    "if DEBUG_MODE: print(\"（顯存已釋放，如需重跑可直接再次執行）\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
