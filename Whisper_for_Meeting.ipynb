{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umdiJ5QkzC9e"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dao-you/Whisper-for-Meeting-on-Colab/blob/main/Whisper_for_Meeting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95849644-392d-48b8-8058-d84b741259a3"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# =========================================================\n",
        "# Whisper Automatic Subtitle Generation: GPU Transcription + CPU Denoising + OpenCC Post-processing (Traditional/Simplified Conversion)\n",
        "# And LLM Summarization (GPT-OSS-20B / llama.cpp / CUDA)\n",
        "# - Transcription: faster-whisper (CUDA, compute: int8_float16â†’float16â†’int8)\n",
        "# - Denoising: ffmpeg afftdn (CPU)\n",
        "# - Progress: Real-time printing of \"current sentence + video total length percentage\"\n",
        "# - Network source download and output: MyDrive/whisper; Files in Drive: Output to the same folder\n",
        "# - LLM Summary: llama.cpp + GPT-OSS-20B GGUF for summarizing transcription\n",
        "# - Prompts \"Delete runtime and restart\" if download is blocked or abnormal\n",
        "# =========================================================\n",
        "\n",
        "# Restrict multithreading (more stable)\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "# [1/8] Mount Google Drive\n",
        "from google.colab import drive\n",
        "print(\"è«‹æŽˆæ¬Šä»¥æŽ›è¼‰ Google Drive\")\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "\n",
        "# Built-in Imports\n",
        "import sys, gc, shutil, datetime, subprocess as sp\n",
        "from pathlib import Path\n",
        "import re, math, time, textwrap\n",
        "from typing import List, Tuple, Optional, Iterable, Iterator\n",
        "from collections import defaultdict\n",
        "from IPython.display import display, Markdown\n",
        "import json\n",
        "\n",
        "\n",
        "ROOT = Path(\"/content/gdrive/MyDrive\")\n",
        "WHISPER_DIR = ROOT / \"whisper\"\n",
        "WHISPER_DIR.mkdir(exist_ok=True, parents=True)\n",
        "os.chdir(ROOT)\n",
        "print(f\"â†’ ç•¶å‰å·¥ä½œç›®éŒ„ï¼š{os.getcwd()}\")\n",
        "\n",
        "# [2/8] User Form Parameters (Unified)\n",
        "#@markdown # Whisper Transcription & LLM Summary Pipeline\n",
        "\n",
        "#@markdown ## Input & Transcription Settings\n",
        "#@markdown **Input Source:** Google Drive file (relative to MyDrive) or video URL (YouTube/HTTP).\n",
        "filename = \"whisper/2022ç¬¬ä¸‰å±†é’æ˜¥ä¾†èªªèª²-ç¬¬1æœƒè­°å®¤å³åœŸåŸŽæ•™æŽˆå›žé¥‹.mp4\"  #@param {type:\"string\"}\n",
        "#@markdown **Download Option:** Check to save network source files to `MyDrive/whisper`.\n",
        "save_video_to_google_drive = True  #@param {type:\"boolean\"}\n",
        "#@markdown **Whisper Model Size:** Choose a model size. `large-v3` requires more GPU VRAM; `medium` is a good alternative if VRAM is limited.\n",
        "model_size = \"medium\"  #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v2\", \"large-v3\"] # Changed model_size to \"medium\"\n",
        "#@markdown **Language:** Select the language for transcription. \"è‡ªå‹•åµæ¸¬\" (Auto-detect) is usually sufficient.\n",
        "language = \"è‡ªå‹•åµæ¸¬\"  #@param [\"è‡ªå‹•åµæ¸¬\", \"ä¸­æ–‡\", \"è‹±æ–‡\"]\n",
        "#@markdown **Denoising:** Apply CPU-based denoising to the audio before transcription. `afftdn` is recommended.\n",
        "denoise_method = \"afftdn (å»ºè­°)\"  #@param [\"afftdn (å»ºè­°)\", \"none\"]\n",
        "#@markdown **Text Post-processing (OpenCC):** Convert the transcribed text (SRT/TXT output) between Simplified and Traditional Chinese variants.\n",
        "text_postprocess = \"è‡ºç£ç¹é«”ä¸­æ–‡ï¼ˆé è¨­ï¼‰\"  #@param [\"è‡ºç£ç¹é«”ä¸­æ–‡ï¼ˆé è¨­ï¼‰\",\"é¦™æ¸¯ç¹é«”ä¸­æ–‡\",\"å¤§é™¸ç°¡é«”ä¸­æ–‡\",\"é—œé–‰\"]\n",
        "#@markdown **YouTube Cookies (Optional):** Path to a Netscape-format cookies file (relative to MyDrive) for accessing age-restricted or member-only YouTube videos (e.g., `cookies/youtube.txt`).\n",
        "youtube_cookies_txt_path = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ## Summarization Settings\n",
        "#@markdown **Topic Hint (Optional):** Provide a brief hint about the topic to guide the summarization process.\n",
        "topic_hint = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "language_code_map = {\"è‡ªå‹•åµæ¸¬\": None, \"ä¸­æ–‡\":\"zh\", \"è‹±æ–‡\":\"en\"}\n",
        "language_code = language_code_map[language]\n",
        "\n",
        "# =========================================================\n",
        "# Developer Options\n",
        "# Advanced users can fine-tune parameters in this section.\n",
        "# Modify only if you understand the impact.\n",
        "# =========================================================\n",
        "DEBUG_MODE = False # Set to True for more detailed logging\n",
        "\n",
        "# --- Transcription Parameters ---\n",
        "TRANSCRIPTION_BEAM_SIZE_PRIMARY = 1\n",
        "TRANSCRIPTION_CHUNK_LENGTH_PRIMARY = 20\n",
        "TRANSCRIPTION_BEAM_SIZE_FALLBACK = 1 # Used if primary fails\n",
        "TRANSCRIPTION_CHUNK_LENGTH_FALLBACK = 15 # Used if primary fails\n",
        "\n",
        "# --- Denoising Parameters ---\n",
        "DENOISE_NOISE_FLOOR_DB = -25\n",
        "\n",
        "# --- Filtering Parameters ---\n",
        "FILTER_MIN_DURATION_SHORT = 1.5 # Minimum duration for short segments\n",
        "FILTER_AVG_LOGPROB_THRESHOLD = -0.5 # Avg log probability threshold for short segments\n",
        "FILTER_MIN_DURATION_SPEECH_PROB = 2.0 # Minimum duration for speech probability filtering\n",
        "FILTER_NO_SPEECH_PROB_THRESHOLD = 0.6 # No speech probability threshold\n",
        "\n",
        "# --- Summary Model Parameters ---\n",
        "REPO_ID   = \"unsloth/gpt-oss-20b-GGUF\"   # GGUF Model Repository\n",
        "GGUF_FILE = \"gpt-oss-20b-Q4_K_M.gguf\"    # Approx. 10.8GiB, T4 can run\n",
        "\n",
        "# --- Summary Inference Parameters (Increase available generation space to avoid truncation) ---\n",
        "CTX_WINDOW_CANDIDATES   = [12288, 16384, 8192]  # T4/Q4_K_M usually handles 12kâ€“16k; fallback to 8192\n",
        "ctx_window              = CTX_WINDOW_CANDIDATES[-1]  # Runtime picks the first successful candidate\n",
        "map_max_new_tokens      = 800   # Segment output upper bound (~550-800 chars)\n",
        "map_repeat_penalty      = 1.10  # Tunable repeat penalty for map stage\n",
        "reduce_repeat_penalty   = 1.10  # Tunable repeat penalty for reduce stage\n",
        "reduce_max_new_tokens   = 1500  # Summary output upper bound (~1k-1.3k chars)\n",
        "temperature             = 0.5\n",
        "top_p                   = 0.9\n",
        "repeat_penalty          = 1.05\n",
        "\n",
        "\n",
        "# --- Summary Segmentation Heuristics ---\n",
        "SEMANTIC_VAD_PRESETS = {\n",
        "    \"Conservative\": 1.2,  # 1.2s: keep more context for safety (ä¿å®ˆ)\n",
        "    \"Aggressive\":   0.8,  # 0.8s: quicker resets to dodge loop traps (ç©æ¥µ)\n",
        "}\n",
        "SELECTED_VAD_SILENCE_PRESET = \"Aggressive\"  # Default preset tuned for repetitive loop mitigation\n",
        "SEMANTIC_PAUSE_THRESHOLD = SEMANTIC_VAD_PRESETS.get(SELECTED_VAD_SILENCE_PRESET, 0.8)\n",
        "SEMANTIC_MIN_TOKENS      = 192   # Minimum tokens before we allow punctuation-based splits (tighter for Chinese)\n",
        "SEMANTIC_MAX_CHARS       = 1800  # Safety valve to avoid overly long segments with no punctuation\n",
        "SLIDING_OVERLAP_TOKENS   = 200   # Tokens preserved between neighbouring summary windows\n",
        "SEMANTIC_FORCE_FLUSH_LINES = 40   # Force flush after 40 lines without punctuation\n",
        "SEMANTIC_FORCE_FLUSH_SECONDS = 120.0  # Force flush if segment spans â‰¥120s\n",
        "\n",
        "\n",
        "tokenizer_config_data = None\n",
        "harmony_chat_formatter = None\n",
        "\n",
        "def ensure_harmony_formatter():\n",
        "    \"\"\"Ensure the Harmony chat formatter is available for GPT-OSS prompts.\"\"\"\n",
        "    global harmony_chat_formatter\n",
        "    if harmony_chat_formatter is not None:\n",
        "        return harmony_chat_formatter\n",
        "    try:\n",
        "        from llama_cpp.llama_chat_format import (\n",
        "            hf_tokenizer_config_to_chat_formatter,\n",
        "            Jinja2ChatFormatter,\n",
        "        )\n",
        "    except Exception as exc:\n",
        "        raise RuntimeError(\"llama_cpp Harmony chat helpers are unavailable\") from exc\n",
        "\n",
        "    formatter = None\n",
        "    if tokenizer_config_data:\n",
        "        try:\n",
        "            formatter = hf_tokenizer_config_to_chat_formatter(\n",
        "                tokenizer_config_data,\n",
        "                add_generation_prompt=True,\n",
        "            )\n",
        "        except Exception as exc:\n",
        "            if DEBUG_MODE:\n",
        "                print(\"  âœ— Failed to initialize Harmony formatter from tokenizer_config.json:\", exc)\n",
        "\n",
        "    if formatter is None:\n",
        "        template = None\n",
        "        if 'llm' in globals() and hasattr(llm, 'metadata'):\n",
        "            template = llm.metadata.get('tokenizer.chat_template')\n",
        "        if template:\n",
        "            try:\n",
        "                bos_token = (\n",
        "                    llm.metadata.get('tokenizer.bos_token')\n",
        "                    or llm.metadata.get('tokenizer.ggml.bos_token')\n",
        "                    or llm.detokenize([llm.token_bos()], special=True).decode('utf-8', errors='ignore')\n",
        "                )\n",
        "                eos_token = (\n",
        "                    llm.metadata.get('tokenizer.eos_token')\n",
        "                    or llm.metadata.get('tokenizer.ggml.eos_token')\n",
        "                    or llm.detokenize([llm.token_eos()], special=True).decode('utf-8', errors='ignore')\n",
        "                )\n",
        "                formatter = Jinja2ChatFormatter(\n",
        "                    template,\n",
        "                    eos_token=eos_token,\n",
        "                    bos_token=bos_token,\n",
        "                    stop_token_ids=[llm.token_eos()],\n",
        "                )\n",
        "            except Exception as exc:\n",
        "                if DEBUG_MODE:\n",
        "                    print(\"  âœ— Failed to build Harmony formatter from GGUF metadata:\", exc)\n",
        "\n",
        "    if formatter is None:\n",
        "        raise RuntimeError(\"Harmony chat formatter could not be prepared\")\n",
        "\n",
        "    harmony_chat_formatter = formatter\n",
        "    return harmony_chat_formatter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# End of Developer Options\n",
        "# =========================================================\n",
        "\n",
        "\n",
        "# [3/8] Install Dependencies\n",
        "# Combine installation steps from both original cells\n",
        "if DEBUG_MODE: print(\"[Install] faster-whisper / yt-dlp / soundfile / opencc / srt / huggingface_hub / llama-cpp-python ...\")\n",
        "\n",
        "def pip_install(pkgs, extra_args=None, env=None):\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\"]\n",
        "    if extra_args:\n",
        "        cmd += extra_args\n",
        "    cmd += pkgs\n",
        "    return sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True, env=env)\n",
        "\n",
        "import importlib\n",
        "import importlib.util\n",
        "# Install common dependencies first\n",
        "common_missing = []\n",
        "if importlib.util.find_spec(\"srt\") is None:\n",
        "    common_missing.append(\"srt>=3.5.3\")\n",
        "if importlib.util.find_spec(\"huggingface_hub\") is None:\n",
        "    common_missing.append(\"huggingface_hub>=0.23.0\")\n",
        "if importlib.util.find_spec(\"soundfile\") is None:\n",
        "    common_missing.append(\"soundfile\")\n",
        "if importlib.util.find_spec(\"opencc\") is None:\n",
        "    common_missing.append(\"opencc-python-reimplemented\")\n",
        "if importlib.util.find_spec(\"jinja2\") is None:\n",
        "    common_missing.append(\"jinja2>=3.1.0\")\n",
        "\n",
        "if common_missing:\n",
        "    if DEBUG_MODE: print(\"â†’ Installing common missing packages:\", \", \".join(common_missing))\n",
        "    r = pip_install(common_missing)\n",
        "    if r.returncode != 0:\n",
        "        if DEBUG_MODE: print(r.stdout)\n",
        "        raise RuntimeError(\"åŸºç¤Žä¾è³´å®‰è£å¤±æ•—ï¼Œè«‹é‡å•ŸåŸ·è¡ŒéšŽæ®µå¾Œé‡è©¦ã€‚\")\n",
        "\n",
        "# Install faster-whisper and yt-dlp separately as they were in the first cell\n",
        "if importlib.util.find_spec(\"faster_whisper\") is None:\n",
        "    if DEBUG_MODE: print(\"â†’ Installing missing package: faster-whisper yt-dlp\")\n",
        "    r = pip_install([\"faster-whisper\", \"yt-dlp\"])\n",
        "    if r.returncode != 0:\n",
        "        if DEBUG_MODE: print(r.stdout)\n",
        "        raise RuntimeError(\"faster-whisper / yt-dlp å®‰è£å¤±æ•—ï¼Œè«‹é‡å•ŸåŸ·è¡ŒéšŽæ®µå¾Œé‡è©¦ã€‚\")\n",
        "\n",
        "# Import external packages after ensuring installation\n",
        "import soundfile as sf\n",
        "from faster_whisper import WhisperModel\n",
        "from opencc import OpenCC\n",
        "import srt as _srt  # Import srt as _srt to avoid name conflict later with the module itself\n",
        "from huggingface_hub import snapshot_download\n",
        "def suggest_runtime_reset():\n",
        "    print(\"\\nðŸ§¹ å»ºè­°å‹•ä½œï¼ˆColabï¼‰\")\n",
        "    print(\"1) ä¾åºï¼šã€ŽåŸ·è¡ŒéšŽæ®µ Runtimeã€ â†’ ã€Žåˆªé™¤åŸ·è¡ŒéšŽæ®µ/é‚„åŽŸå‡ºå» è¨­å®š Factory reset runtimeã€\\n2) é‡æ–°åŸ·è¡Œæœ¬ Notebookï¼ˆå¾žæŽ›è¼‰é›²ç«¯ç¡¬ç¢Ÿé‚£æ ¼é–‹å§‹ï¼‰\", flush=True)\n",
        "\n",
        "def run_cmd(cmd:list, check=True):\n",
        "    if DEBUG_MODE: print(\"  $\", \" \".join(cmd))\n",
        "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
        "    if DEBUG_MODE and p.stdout: sys.stdout.write(p.stdout)\n",
        "    if check and p.returncode != 0:\n",
        "        raise RuntimeError(f\"å‘½ä»¤å¤±æ•—ï¼š{' '.join(cmd)}\")\n",
        "    return p\n",
        "\n",
        "def is_youtube_url(s:str)->bool:\n",
        "    return isinstance(s, str) and (\"youtu.be\" in s or \"youtube.com\" in s)\n",
        "def is_http_url(s:str)->bool:\n",
        "    return isinstance(s, str) and s.lower().startswith(\"http\")\n",
        "def to_abs_mydrive(p:str)->Path:\n",
        "    return (Path(p) if p.startswith(\"/\") else (ROOT / p)).resolve()\n",
        "def fmt_ts_srt(t:float)->str:\n",
        "    h = int(t//3600); m = int((t%3600)//60); s = t - h*3600 - m*60\n",
        "    return f\"{h:02d}:{m:02d}:{int(s):02d},{int(round((s-int(s))*1000)):03d}\"\n",
        "def verify_wav_ok(path: Path)->bool:\n",
        "    try:\n",
        "        info = sf.info(str(path))\n",
        "        return info.samplerate > 0 and info.channels in (1, 2)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# OpenCC converter setup\n",
        "def build_opencc_pipeline(choice:str):\n",
        "    if choice.startswith(\"è‡ºç£\"):\n",
        "        return [OpenCC('s2t'), OpenCC('t2tw')]\n",
        "    if choice.startswith(\"é¦™æ¸¯\"):\n",
        "        return [OpenCC('s2t'), OpenCC('t2hk')]\n",
        "    if choice.startswith(\"å¤§é™¸\"):\n",
        "        return [OpenCC('t2s')]\n",
        "    return []  # Disable\n",
        "\n",
        "def apply_opencc(text:str, pipeline)->str:\n",
        "    for cc in pipeline:\n",
        "        text = cc.convert(text)\n",
        "    return text\n",
        "\n",
        "def ytdl(yturl:str)->Path:\n",
        "    tmp = Path(\"/tmp/dl\"); tmp.mkdir(parents=True, exist_ok=True)\n",
        "    for x in tmp.glob(\"*\"):\n",
        "        try: x.unlink()\n",
        "        except: shutil.rmtree(x, ignore_errors=True)\n",
        "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    out = tmp / f\"downloaded_{ts}.mp4\"\n",
        "    if DEBUG_MODE: print(\"[Download] Getting YouTube video ...\")\n",
        "    # Use sp.run instead of subprocess.run directly\n",
        "    cmd = [\"yt-dlp\", \"-f\", \"mp4\", \"-o\", str(tmp / \"%(title)s.%(ext)s\")]\n",
        "    if youtube_cookies_txt_path.strip():\n",
        "        cookies_abs = to_abs_mydrive(youtube_cookies_txt_path.strip())\n",
        "        if cookies_abs.exists():\n",
        "            cmd += [\"--cookies\", str(cookies_abs)]\n",
        "        else:\n",
        "            if DEBUG_MODE: print(f\"âš ï¸ æ‰¾ä¸åˆ° cookies æª”ï¼š{cookies_abs}ï¼ˆæ”¹ç‚ºä¸å¸¶ cookiesï¼‰\")\n",
        "    cmd.append(yturl)\n",
        "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
        "    if DEBUG_MODE and p.stdout: sys.stdout.write(p.stdout)\n",
        "    if p.returncode != 0:\n",
        "        if \"Sign in to confirm\" in (p.stdout or \"\"):\n",
        "            print()\n",
        "            print(\"â—YouTube è¦æ±‚ç™»å…¥/é©—è­‰ï¼Œè«‹æä¾› cookies æˆ–å…ˆè‡ªè¡Œä¸‹è¼‰åˆ°é›²ç«¯ç¡¬ç¢Ÿã€‚\")\n",
        "        print(\"ðŸ”„ è‹¥å¤šæ¬¡å¤±æ•—ï¼Œè«‹åˆªé™¤åŸ·è¡ŒéšŽæ®µä¸¦é‡å•Ÿå¾Œé‡è©¦ã€‚\")\n",
        "        suggest_runtime_reset()\n",
        "        raise RuntimeError(\"yt-dlp ä¸‹è¼‰å¤±æ•—\")\n",
        "    files = list(tmp.glob(\"*\"))\n",
        "    if not files:\n",
        "        print(\"ðŸ”„ ä¸‹è¼‰ç‚ºç©ºï¼Œå»ºè­°åˆªé™¤åŸ·è¡ŒéšŽæ®µå†é‡è©¦ã€‚\")\n",
        "        suggest_runtime_reset()\n",
        "        raise FileNotFoundError(\"YouTube ä¸‹è¼‰å¤±æ•—ï¼š/tmp/dl ç‚ºç©º\")\n",
        "    f = files[0]\n",
        "    if save_video_to_google_drive:\n",
        "        shutil.copy2(f, WHISPER_DIR / f.name)\n",
        "    return f\n",
        "\n",
        "def http_dl(url:str)->Path:\n",
        "    tmp = Path(\"/tmp/dl\"); tmp.mkdir(parents=True, exist_ok=True)\n",
        "    for x in tmp.glob(\"*\"):\n",
        "        try: x.unlink()\n",
        "        except: shutil.rmtree(x, ignore_errors=True)\n",
        "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    out = tmp / f\"downloaded_{ts}.mp4\"\n",
        "    if DEBUG_MODE: print(\"[Download] Getting HTTP(S) video ...\")\n",
        "    run_cmd([\"curl\", \"-L\", \"-o\", str(out), url])\n",
        "    if save_video_to_google_drive:\n",
        "        shutil.copy2(out, WHISPER_DIR / out.name)\n",
        "    return out\n",
        "\n",
        "# Extract audio: ffmpeg -> 16k/mono WAV\n",
        "def ffmpeg_extract_wav(in_path:Path, out_wav:Path, sr=16000):\n",
        "    cmd = [\"ffmpeg\",\"-y\",\"-i\",str(in_path),\"-vn\",\"-ac\",\"1\",\"-ar\",str(sr),\"-f\",\"wav\",str(out_wav)]\n",
        "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
        "    if p.returncode != 0:\n",
        "        if DEBUG_MODE: print(p.stdout)\n",
        "        raise RuntimeError(\"ffmpeg è½‰ WAV å¤±æ•—\")\n",
        "\n",
        "# CPU Denoising: ffmpeg afftdn\n",
        "def ffmpeg_afftdn(in_wav: Path, out_wav: Path, noise_floor_db=DENOISE_NOISE_FLOOR_DB):\n",
        "    cmd = [\"ffmpeg\",\"-y\",\"-i\",str(in_wav),\"-af\",f\"afftdn=nf={noise_floor_db}\",\n",
        "           \"-ac\",\"1\",\"-ar\",\"16000\",\"-f\",\"wav\",str(out_wav)]\n",
        "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
        "    if p.returncode != 0:\n",
        "        if DEBUG_MODE: print(p.stdout)\n",
        "        raise RuntimeError(\"ffmpeg afftdn å¤±æ•—\")\n",
        "\n",
        "# Safeguard: Repack WAV header if format is strange\n",
        "def ffmpeg_repack_wav(in_wav: Path, out_wav: Path, sr=16000):\n",
        "    cmd = [\"ffmpeg\",\"-y\",\"-i\",str(in_wav),\"-acodec\",\"pcm_s16le\",\"-ac\",\"1\",\"-ar\",str(sr),str(out_wav)]\n",
        "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
        "    if p.returncode != 0:\n",
        "        if DEBUG_MODE: print(p.stdout)\n",
        "        raise RuntimeError(\"ffmpeg é‡åŒ… WAV å¤±æ•—\")\n",
        "\n",
        "# [4/8] Parse Source (Transcription) - Uses 'filename' and 'save_video_to_google_drive'\n",
        "if DEBUG_MODE: print(\"[4/8] Parsing input source ...\")\n",
        "try:\n",
        "    if is_youtube_url(filename):\n",
        "        src_path = ytdl(filename); out_base_dir = WHISPER_DIR\n",
        "    elif is_http_url(filename):\n",
        "        src_path = http_dl(filename); out_base_dir = WHISPER_DIR\n",
        "    else:\n",
        "        src_path = to_abs_mydrive(filename)\n",
        "        if not src_path.exists(): raise FileNotFoundError(f\"æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{src_path}\")\n",
        "        out_base_dir = src_path.parent\n",
        "except Exception as e:\n",
        "    print(f\"\\nâ›” ä¾†æºè§£æž/ä¸‹è¼‰å¤±æ•—ï¼š{e}\")\n",
        "    print(\"ðŸ”„ è«‹åˆªé™¤åŸ·è¡ŒéšŽæ®µä¸¦é‡æ–°å•Ÿå‹•å¾Œé‡è·‘ã€‚\"); suggest_runtime_reset(); raise\n",
        "\n",
        "print(f\"â†’ ä¾†æºæª”ï¼š{src_path}\")\n",
        "print(f\"â†’ è¼¸å‡ºè³‡æ–™å¤¾ï¼š{out_base_dir}\")\n",
        "\n",
        "# [5/8] Extract Audio & CPU Denoising (Transcription) - Uses 'denoise_method' and 'DENOISE_NOISE_FLOOR_DB'\n",
        "AUDIO_16K = Path(\"/tmp/audio_16k.wav\")\n",
        "if DEBUG_MODE: print(\"[5/8] Extracting audio (ffmpeg â†’ 16k/mono WAV) ...\")\n",
        "ffmpeg_extract_wav(src_path, AUDIO_16K, sr=16000)\n",
        "\n",
        "if denoise_method.startswith(\"afftdn\"):\n",
        "    if DEBUG_MODE: print(\"[5.5/8] Denoising (ffmpeg afftdn, CPU) ...\")\n",
        "    DENOISED = Path(\"/tmp/audio_16k_denoised.wav\")\n",
        "    ffmpeg_afftdn(AUDIO_16K, DENOISED, noise_floor_db=DENOISE_NOISE_FLOOR_DB)\n",
        "    denoised_audio = DENOISED if verify_wav_ok(DENOISED) else AUDIO_16K\n",
        "else:\n",
        "    denoised_audio = AUDIO_16K\n",
        "\n",
        "if not verify_wav_ok(denoised_audio):\n",
        "    if DEBUG_MODE: print(\"  - éŸ³è¨Šæ ¼å¼ç•°å¸¸ï¼›å˜—è©¦é‡åŒ… WAV ...\")\n",
        "    FIXED = Path(\"/tmp/audio_16k_fixed.wav\")\n",
        "    ffmpeg_repack_wav(denoised_audio, FIXED, sr=16000)\n",
        "    denoised_audio = FIXED\n",
        "\n",
        "if DEBUG_MODE: print(f\"â†’ æœ€çµ‚è¼¸å…¥éŸ³è¨Šï¼š{denoised_audio}\")\n",
        "\n",
        "# [6/8] Load faster-whisper (GPU enforced) - Uses 'model_size'\n",
        "if DEBUG_MODE: print(\"[6/8] Loading faster-whisper model (GPU) ...\")\n",
        "device = \"cuda\"  # Enforce GPU\n",
        "model = None; last_err = None\n",
        "compute_type_candidates = [\"float16\", \"int8_float16\", \"int8\"]\n",
        "chosen_compute_type = None\n",
        "for ctype in compute_type_candidates:\n",
        "    try:\n",
        "        if DEBUG_MODE: print(f\"  - Trying compute_type={ctype}\")\n",
        "        model = WhisperModel(model_size, device=device, compute_type=ctype)\n",
        "        chosen_compute_type = ctype\n",
        "        if DEBUG_MODE: print(\"  - Model loaded successfully\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        last_err = e\n",
        "        if DEBUG_MODE: print(f\"  - Load failed: {e}\")\n",
        "if model is None:\n",
        "    print()\n",
        "    print(\"â›” GPU æ¨¡åž‹è¼‰å…¥å¤±æ•—ã€‚è«‹ç¢ºèªã€Žè®Šæ›´åŸ·è¡ŒéšŽæ®µé¡žåž‹ã€é¸äº† GPUï¼ˆT4/A100ï¼‰ï¼Œæˆ–åˆªé™¤åŸ·è¡ŒéšŽæ®µå¾Œé‡è©¦ã€‚\")\n",
        "    suggest_runtime_reset()\n",
        "    raise RuntimeError(f\"ç„¡æ³•è¼‰å…¥æ¨¡åž‹ï¼š{last_err}\")\n",
        "print(f\"â†’ faster-whisper compute_type: {chosen_compute_type}\")\n",
        "\n",
        "gc.collect()  # Clean up before transcription (safety)\n",
        "\n",
        "# [7/8] Transcribe (GPU; real-time progress per segment) - Uses 'language_code', 'TRANSCRIPTION_BEAM_SIZE_PRIMARY', 'TRANSCRIPTION_CHUNK_LENGTH_PRIMARY', 'TRANSCRIPTION_BEAM_SIZE_FALLBACK', 'TRANSCRIPTION_CHUNK_LENGTH_FALLBACK'\n",
        "if DEBUG_MODE: print(f\"[7/8] Starting transcription (GPU: beam={TRANSCRIPTION_BEAM_SIZE_PRIMARY} / chunk={TRANSCRIPTION_CHUNK_LENGTH_PRIMARY}s / VAD+no-repeat) ...\")\n",
        "\n",
        "SELECTED_VAD_SILENCE_MS = int(max(0.2, SEMANTIC_PAUSE_THRESHOLD) * 1000)\n",
        "LOOP_SENTINEL_MIN_REPEAT = 5          # Trigger guard if the same line repeats >=5 times\n",
        "LOOP_SENTINEL_TOLERANCE = 0.35        # Allow Â±350ms drift when checking 1s increments\n",
        "LOOP_ESCALATE_REPEAT_THRESHOLD = 8    # Require â‰¥8 repeats before escalating guardrails\n",
        "LOOP_ESCALATE_DURATION_SECONDS = 10.0 # Require loops to span â‰¥10s when escalating by duration\n",
        "LOOP_ESCALATE_EVENTS_PER_MIN = 2      # Escalate when â‰¥2 loop events occur within the same minute\n",
        "\n",
        "\n",
        "def transcribe_gpu(\n",
        "    _beam: int = TRANSCRIPTION_BEAM_SIZE_PRIMARY,\n",
        "    _chunk: int = TRANSCRIPTION_CHUNK_LENGTH_PRIMARY,\n",
        "    *,\n",
        "    cond_prev: bool = True,\n",
        "    compression_threshold: float = 1.35,\n",
        "    no_repeat_ngram: int = 2,\n",
        "):\n",
        "    return model.transcribe(\n",
        "        str(denoised_audio),\n",
        "        task=\"transcribe\",\n",
        "        language=language_code,\n",
        "        temperature=0.0,\n",
        "        condition_on_previous_text=cond_prev,\n",
        "        compression_ratio_threshold=compression_threshold,\n",
        "        log_prob_threshold=-0.5,\n",
        "        no_speech_threshold=0.6,\n",
        "        beam_size=_beam,\n",
        "        chunk_length=_chunk,\n",
        "        vad_filter=True,\n",
        "        vad_parameters={\"min_silence_duration_ms\": SELECTED_VAD_SILENCE_MS},\n",
        "        no_repeat_ngram_size=no_repeat_ngram,\n",
        "        word_timestamps=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def run_transcription_attempt(\n",
        "    label: str,\n",
        "    *,\n",
        "    cond_prev: bool,\n",
        "    compression_threshold: float,\n",
        "    no_repeat_ngram: int,\n",
        "):\n",
        "    try:\n",
        "        seg_iter, info = transcribe_gpu(\n",
        "            cond_prev=cond_prev,\n",
        "            compression_threshold=compression_threshold,\n",
        "            no_repeat_ngram=no_repeat_ngram,\n",
        "        )\n",
        "    except Exception as exc:\n",
        "        if DEBUG_MODE:\n",
        "            print(\n",
        "                f\"  - Transcription attempt \\'{label}\\' failed ({exc})\\n\"\n",
        "                f\"    â†’ Retrying with fallback beam/chunk (beam={TRANSCRIPTION_BEAM_SIZE_FALLBACK}, \"\n",
        "                f\"chunk={TRANSCRIPTION_CHUNK_LENGTH_FALLBACK})\"\n",
        "            )\n",
        "        seg_iter, info = transcribe_gpu(\n",
        "            _beam=TRANSCRIPTION_BEAM_SIZE_FALLBACK,\n",
        "            _chunk=TRANSCRIPTION_CHUNK_LENGTH_FALLBACK,\n",
        "            cond_prev=cond_prev,\n",
        "            compression_threshold=compression_threshold,\n",
        "            no_repeat_ngram=no_repeat_ngram,\n",
        "        )\n",
        "\n",
        "    duration = float(getattr(info, \"duration\", 0.0) or 0.0)\n",
        "    if duration <= 0:\n",
        "        duration = 1.0\n",
        "\n",
        "    segments: List[Tuple[float, float, str]] = []\n",
        "    filtered: List[Tuple[float, float, str]] = []\n",
        "    last_text: Optional[str] = None\n",
        "    last_start: Optional[float] = None\n",
        "    last_end: Optional[float] = None\n",
        "    repeat_streak = 1\n",
        "    max_repeat_streak = 1\n",
        "    loop_anchor_start: Optional[float] = None\n",
        "    loop_events: List[dict] = []\n",
        "    raw_speech_seconds = 0.0\n",
        "    filtered_speech_seconds = 0.0\n",
        "\n",
        "    stats = {\n",
        "        \"duration\": duration,\n",
        "        \"speech_coverage\": 0.0,\n",
        "        \"raw_segment_seconds\": 0.0,\n",
        "        \"vad_removed_seconds\": 0.0,\n",
        "        \"post_filter_removed_seconds\": 0.0,\n",
        "        \"compression_hits\": 0,\n",
        "        \"max_repeat_streak\": 1,\n",
        "        \"loop_events\": [],\n",
        "        \"loop_escalate_reasons\": [],\n",
        "        \"loop_events_per_minute\": {},\n",
        "        \"language\": getattr(info, \"language\", \"æœªçŸ¥\"),\n",
        "        \"language_probability\": float(getattr(info, \"language_probability\", 0.0) or 0.0),\n",
        "        \"condition_on_previous_text\": cond_prev,\n",
        "        \"compression_threshold\": compression_threshold,\n",
        "        \"no_repeat_ngram_size\": no_repeat_ngram,\n",
        "    }\n",
        "\n",
        "    for seg in seg_iter:\n",
        "        text = seg.text.strip()\n",
        "        pct = int(min(100, round((float(seg.end) / duration) * 100)))\n",
        "        print(f\"[{pct:3d}%] {fmt_ts_srt(seg.start)} â†’ {fmt_ts_srt(seg.end)}  {text}\", flush=True)\n",
        "\n",
        "        seg_tuple = (float(seg.start), float(seg.end), text)\n",
        "        segments.append(seg_tuple)\n",
        "\n",
        "        keep = True\n",
        "        seg_dur = max(0.0, float(seg_tuple[1] - seg_tuple[0]))\n",
        "        raw_speech_seconds += seg_dur\n",
        "        if (\n",
        "            seg_dur < FILTER_MIN_DURATION_SHORT\n",
        "            and getattr(seg, \"avg_logprob\", None) is not None\n",
        "            and seg.avg_logprob < FILTER_AVG_LOGPROB_THRESHOLD\n",
        "        ):\n",
        "            keep = False\n",
        "        if (\n",
        "            seg_dur < FILTER_MIN_DURATION_SPEECH_PROB\n",
        "            and getattr(seg, \"no_speech_prob\", None) is not None\n",
        "            and seg.no_speech_prob > FILTER_NO_SPEECH_PROB_THRESHOLD\n",
        "        ):\n",
        "            keep = False\n",
        "        if keep:\n",
        "            filtered.append(seg_tuple)\n",
        "            filtered_speech_seconds += seg_dur\n",
        "\n",
        "        compression_ratio = getattr(seg, \"compression_ratio\", None)\n",
        "        if compression_ratio is not None and compression_ratio >= compression_threshold:\n",
        "            stats[\"compression_hits\"] += 1\n",
        "\n",
        "        if last_text is not None and text == last_text:\n",
        "            gap = float(seg_tuple[0] - (last_start or seg_tuple[0]))\n",
        "            if abs(gap - 1.0) <= LOOP_SENTINEL_TOLERANCE:\n",
        "                repeat_streak += 1\n",
        "                if loop_anchor_start is None:\n",
        "                    loop_anchor_start = last_start\n",
        "                if repeat_streak == LOOP_SENTINEL_MIN_REPEAT:\n",
        "                    event_start = (\n",
        "                        loop_anchor_start\n",
        "                        if loop_anchor_start is not None\n",
        "                        else (last_start if last_start is not None else seg_tuple[0])\n",
        "                    )\n",
        "                    event = {\n",
        "                        \"label\": label,\n",
        "                        \"text\": text,\n",
        "                        \"start\": event_start,\n",
        "                        \"end\": seg_tuple[1],\n",
        "                        \"count\": repeat_streak,\n",
        "                    }\n",
        "                    loop_events.append(event)\n",
        "                    print(\n",
        "                        f\"âš ï¸ De-loop sentinel triggered ({label}): {fmt_ts_srt(event_start)} â†’ {fmt_ts_srt(seg_tuple[1])} | repeats={repeat_streak}\"\n",
        "                    )\n",
        "                elif loop_events:\n",
        "                    loop_events[-1][\"end\"] = seg_tuple[1]\n",
        "                    loop_events[-1][\"count\"] = repeat_streak\n",
        "            else:\n",
        "                if repeat_streak >= LOOP_SENTINEL_MIN_REPEAT and loop_events:\n",
        "                    loop_events[-1][\"end\"] = last_end if last_end is not None else seg_tuple[1]\n",
        "                    loop_events[-1][\"count\"] = repeat_streak\n",
        "                repeat_streak = 1\n",
        "                loop_anchor_start = None\n",
        "        else:\n",
        "            if repeat_streak >= LOOP_SENTINEL_MIN_REPEAT and loop_events:\n",
        "                loop_events[-1][\"end\"] = last_end if last_end is not None else seg_tuple[1]\n",
        "                loop_events[-1][\"count\"] = repeat_streak\n",
        "            repeat_streak = 1\n",
        "            loop_anchor_start = None\n",
        "\n",
        "        max_repeat_streak = max(max_repeat_streak, repeat_streak)\n",
        "        last_text = text\n",
        "        last_start = seg_tuple[0]\n",
        "        last_end = seg_tuple[1]\n",
        "\n",
        "    if repeat_streak >= LOOP_SENTINEL_MIN_REPEAT and loop_events:\n",
        "        loop_events[-1][\"end\"] = last_end if last_end is not None else loop_events[-1][\"end\"]\n",
        "        loop_events[-1][\"count\"] = repeat_streak\n",
        "\n",
        "    stats[\"raw_segment_seconds\"] = max(0.0, raw_speech_seconds)\n",
        "    stats[\"speech_coverage\"] = max(0.0, filtered_speech_seconds)\n",
        "    stats[\"vad_removed_seconds\"] = max(0.0, duration - raw_speech_seconds)\n",
        "    stats[\"post_filter_removed_seconds\"] = max(0.0, raw_speech_seconds - filtered_speech_seconds)\n",
        "    stats[\"max_repeat_streak\"] = max(max_repeat_streak, repeat_streak)\n",
        "\n",
        "    minute_counts = defaultdict(int)\n",
        "    escalate_reasons = []\n",
        "    for event in loop_events:\n",
        "        start_val = float(event.get(\"start\", 0.0) or 0.0)\n",
        "        end_val = float(event.get(\"end\", start_val) or start_val)\n",
        "        duration_val = max(0.0, end_val - start_val)\n",
        "        event[\"duration\"] = duration_val\n",
        "        bucket = int(max(0.0, start_val) // 60)\n",
        "        minute_counts[bucket] += 1\n",
        "        if (\n",
        "            event.get(\"count\", 0) >= LOOP_ESCALATE_REPEAT_THRESHOLD\n",
        "            and duration_val >= LOOP_ESCALATE_DURATION_SECONDS\n",
        "        ):\n",
        "            escalate_reasons.append(\n",
        "                f\"repeat_streak={event.get('count', 0)} lasting {duration_val:.1f}s near {fmt_ts_srt(start_val)}\"\n",
        "            )\n",
        "\n",
        "    for minute, count in sorted(minute_counts.items()):\n",
        "        if count >= LOOP_ESCALATE_EVENTS_PER_MIN:\n",
        "            window_start = minute * 60\n",
        "            window_end = window_start + 60\n",
        "            escalate_reasons.append(\n",
        "                f\"{count} loop events within {fmt_ts_srt(window_start)}â€“{fmt_ts_srt(window_end)}\"\n",
        "            )\n",
        "\n",
        "    stats[\"loop_events\"] = loop_events\n",
        "    stats[\"loop_escalate_reasons\"] = escalate_reasons\n",
        "    stats[\"loop_events_per_minute\"] = dict(minute_counts)\n",
        "\n",
        "    return {\n",
        "        \"segments\": segments,\n",
        "        \"filtered\": filtered,\n",
        "        \"info\": info,\n",
        "        \"stats\": stats,\n",
        "    }\n",
        "\n",
        "\n",
        "transcription_attempts = [\n",
        "\n",
        "    (\"Baseline\", dict(cond_prev=True,  compression_threshold=1.35, no_repeat_ngram=2)),\n",
        "    (\"Reset\",    dict(cond_prev=False, compression_threshold=1.35, no_repeat_ngram=2)),\n",
        "    (\"Reinforce\",dict(cond_prev=False, compression_threshold=1.30, no_repeat_ngram=3)),\n",
        "]\n",
        "\n",
        "segments: List[Tuple[float, float, str]] = []\n",
        "filtered: List[Tuple[float, float, str]] = []\n",
        "info = None\n",
        "transcription_stats = {}\n",
        "\n",
        "for idx, (label, params) in enumerate(transcription_attempts, 1):\n",
        "    if idx > 1:\n",
        "        print(\n",
        "            f\"â†’ De-loop retry {idx}/{len(transcription_attempts)}: {label} \"\n",
        "            f\"(condition_on_previous_text={params['cond_prev']}, no_repeat_ngram_size={params['no_repeat_ngram']}, \"\n",
        "            f\"compression_ratio_threshold={params['compression_threshold']})\"\n",
        "        )\n",
        "    attempt = run_transcription_attempt(label, **params)\n",
        "    segments = attempt[\"segments\"]\n",
        "    filtered = attempt[\"filtered\"]\n",
        "    info = attempt[\"info\"]\n",
        "    transcription_stats = attempt[\"stats\"]\n",
        "    loop_events = transcription_stats.get(\"loop_events\", [])\n",
        "    escalate_reasons = transcription_stats.get(\"loop_escalate_reasons\", [])\n",
        "    if not loop_events:\n",
        "        break\n",
        "    if not escalate_reasons:\n",
        "        print(\"  â†³ De-loop sentinel noted repeats but below escalation thresholds; keeping current parameters.\")\n",
        "        break\n",
        "    if idx < len(transcription_attempts):\n",
        "        print(\"  â†³ De-loop sentinel escalation triggered due to:\")\n",
        "        for reason in escalate_reasons:\n",
        "            print(f\"     â€¢ {reason}\")\n",
        "        print(\"    â†’ Retrying with tighter decoding guardrails ...\")\n",
        "    else:\n",
        "        print(\"  â†³ Warning: loop persisted despite all guardrails.\")\n",
        "\n",
        "if DEBUG_MODE:\n",
        "    print(\n",
        "        f\"  - Detected language: {transcription_stats.get('language', 'æœªçŸ¥')} \"\n",
        "        f\"(p={transcription_stats.get('language_probability', 0.0):.2f})\"\n",
        "    )\n",
        "    print(f\"  - Audio length: {transcription_stats.get('duration', 0.0):.2f}s\")\n",
        "\n",
        "\n",
        "# ---- OpenCC Normalization (for output text) ---- - Uses 'text_postprocess'\n",
        "pipeline = build_opencc_pipeline(text_postprocess)\n",
        "def norm(txt: str) -> str:\n",
        "    return apply_opencc(txt, pipeline) if pipeline else txt\n",
        "\n",
        "# [8/8] Output (text after OpenCC) - Uses 'out_base_dir' (derived from 'filename')\n",
        "print(\"[8/8] è¼¸å‡º SRT / TXT ...\")\n",
        "# Determine the output directory for transcription based on input type\n",
        "# If input is a network source, output to WHISPER_DIR\n",
        "# If input is a local file, output to the same directory as the input file\n",
        "if is_youtube_url(filename) or is_http_url(filename):\n",
        "    out_base_dir = WHISPER_DIR\n",
        "else:\n",
        "    src_path_abs = to_abs_mydrive(filename)\n",
        "    out_base_dir = src_path_abs.parent\n",
        "\n",
        "# Create the transcription output directory if it doesn't exist\n",
        "out_dir = out_base_dir\n",
        "out_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Determine the stem from the original source file path\n",
        "stem = Path(src_path).stem\n",
        "SRT = out_dir / f\"{stem}.srt\"\n",
        "TXT = out_dir / f\"{stem}.txt\"\n",
        "\n",
        "with open(SRT, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, (seg_start, seg_end, seg_text) in enumerate(filtered, 1):\n",
        "        text_out = norm(seg_text.strip())\n",
        "        f.write(f\"{i}\\n{fmt_ts_srt(seg_start)} --> {fmt_ts_srt(seg_end)}\\n{text_out}\\n\\n\")\n",
        "\n",
        "with open(TXT, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, _, seg_text in filtered:\n",
        "        f.write(norm(seg_text.strip()) + \"\\n\")\n",
        "\n",
        "print(f\"â†’ å®Œæˆï¼\\n  SRT: {SRT}\\n  TXT: {TXT}\")\n",
        "print(\"[Transcription Metrics]\")\n",
        "print(f\"â†’ faster-whisper compute_type: {chosen_compute_type}\")\n",
        "print(f\"â†’ VAD preset: {SELECTED_VAD_SILENCE_PRESET} (silence â‰¥ {SEMANTIC_PAUSE_THRESHOLD:.2f}s â†’ {SELECTED_VAD_SILENCE_MS} ms)\")\n",
        "print(\n",
        "    f\"â†’ Language: {transcription_stats.get('language', 'æœªçŸ¥')} \"\n",
        "    f\"(p={transcription_stats.get('language_probability', 0.0):.2f})\"\n",
        ")\n",
        "print(\n",
        "    f\"â†’ Audio length: {transcription_stats.get('duration', 0.0):.2f}sï¼›\"\n",
        "    f\"Raw speech: {transcription_stats.get('raw_segment_seconds', 0.0):.2f}sï¼›\"\n",
        "    f\"VAD removed: {transcription_stats.get('vad_removed_seconds', 0.0):.2f}sï¼›\"\n",
        "    f\"Post-filter removed: {transcription_stats.get('post_filter_removed_seconds', 0.0):.2f}sï¼›\"\n",
        "    f\"Speech kept: {transcription_stats.get('speech_coverage', 0.0):.2f}s\"\n",
        ")\n",
        "print(f\"â†’ compression_ratio_threshold hits: {transcription_stats.get('compression_hits', 0)} segments\")\n",
        "print(f\"â†’ Max consecutive repeat streak: {transcription_stats.get('max_repeat_streak', 1)}\")\n",
        "if transcription_stats.get('loop_events'):\n",
        "    for idx_evt, evt in enumerate(transcription_stats.get('loop_events', []), 1):\n",
        "        start_ts = fmt_ts_srt(evt.get('start', 0.0))\n",
        "        end_ts = fmt_ts_srt(evt.get('end', evt.get('start', 0.0)))\n",
        "        duration_val = evt.get('duration', max(0.0, (evt.get('end', 0.0) or 0.0) - (evt.get('start', 0.0) or 0.0)))\n",
        "        print(\n",
        "            f\"   âš ï¸ Event {idx_evt}: {start_ts} â†’ {end_ts} \"\n",
        "            f\"(durationâ‰ˆ{duration_val:.1f}s, repeats={evt.get('count', LOOP_SENTINEL_MIN_REPEAT)}, tag={evt.get('label')})\"\n",
        "        )\n",
        "    if transcription_stats.get('loop_escalate_reasons'):\n",
        "        for reason in transcription_stats.get('loop_escalate_reasons', []):\n",
        "            print(f\"   â†³ escalation note: {reason}\")\n",
        "else:\n",
        "    print(\"â†’ No de-loop sentinel events detected.\")\n",
        "\n",
        "# Release model (release GPU memory)\n",
        "try: del model\n",
        "except: pass\n",
        "gc.collect()\n",
        "if DEBUG_MODE: print(\"â†’ Model released; can run again directly if needed.\")\n",
        "\n",
        "\n",
        "# ===== Summarization Logic Starts Here =====\n",
        "# Use SRT from transcription step for summarization\n",
        "summary_srt_path_abs = SRT\n",
        "assert summary_srt_path_abs.exists(), f\"SRT æª”ä¸å­˜åœ¨ï¼š{summary_srt_path_abs}\"\n",
        "\n",
        "# ===== Summary 1/6) Check GPU and Install Dependencies (llama-cpp-python specific) =====\n",
        "# llama-cpp-python installation logic - Keep this separate as it has specific CUDA requirements\n",
        "# Moved this section to just before reading the SRT for summarization\n",
        "if DEBUG_MODE: print(\"[Summary 1/6] Checking GPU and installing llama-cpp-python ...\")\n",
        "\n",
        "def detect_cuda_tag():\n",
        "    try:\n",
        "        out = sp.check_output([\"nvidia-smi\"], text=True)\n",
        "        version_token = None\n",
        "        for line in out.splitlines():\n",
        "            if \"CUDA Version\" in line:\n",
        "                _, _, tail = line.partition(\"CUDA Version:\")\n",
        "                cleaned = ''.join(ch for ch in tail if ch.isdigit() or ch == '.')\n",
        "                if cleaned:\n",
        "                    version_token = cleaned\n",
        "                    break\n",
        "        if not version_token:\n",
        "            return \"cu124\"\n",
        "        parts = version_token.split(\".\")\n",
        "        major = int(parts[0]) if parts and parts[0].isdigit() else 0\n",
        "        minor = int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else 0\n",
        "        if major > 12 or (major == 12 and minor >= 5):\n",
        "            return \"cu125\"\n",
        "        if major == 12 and minor >= 0:\n",
        "            return \"cu124\"\n",
        "        if major == 11 and minor >= 8:\n",
        "            return \"cu118\"\n",
        "        return \"cu117\"\n",
        "    except Exception:\n",
        "        return \"cu124\"\n",
        "\n",
        "cuda_tag = detect_cuda_tag()\n",
        "if DEBUG_MODE: print(f\"GPU 0: Detected CUDA version tag {cuda_tag}\")\n",
        "\n",
        "def try_import_llama():\n",
        "    try:\n",
        "        from llama_cpp import Llama\n",
        "        return Llama\n",
        "    except ModuleNotFoundError:\n",
        "        return None\n",
        "\n",
        "Llama = try_import_llama()\n",
        "if Llama is None:\n",
        "    # Keep your existing installation strategy: extra-index -> fallback to source compilation on failure\n",
        "    candidates = [cuda_tag, \"cu125\", \"cu124\", \"cu122\", \"cu121\"]\n",
        "    ok = False\n",
        "    for tag in candidates:\n",
        "        idx = f\"https://abetlen.github.io/llama-cpp-python/whl/{tag}\"\n",
        "        if DEBUG_MODE: print(f\"â†’ Attempting to install llama-cpp-python ({tag}) ...\")\n",
        "        r = pip_install([\"llama-cpp-python\"], extra_args=[\"--extra-index-url\", idx])\n",
        "        if r.returncode == 0:\n",
        "            Llama = try_import_llama()\n",
        "            if Llama is not None:\n",
        "                ok = True\n",
        "                break\n",
        "        else:\n",
        "            if DEBUG_MODE: print(\"  âœ— Installation failed (summary):\", \"\\n\".join(r.stdout.splitlines()[-5:]))\n",
        "    if not ok:\n",
        "        if DEBUG_MODE: print(\"â†’ Pre-compiled wheels not available, switching to 'source compilation (CUDA=ON)' ... (takes longer)\")\n",
        "        try:\n",
        "            import ninja # noqa: F401 # Import ninja to check if installed\n",
        "        except ModuleNotFoundError:\n",
        "            if DEBUG_MODE: print(\"â†’ Installing missing package: ninja\")\n",
        "            r = pip_install([\"ninja\"])\n",
        "            if r.returncode != 0:\n",
        "                if DEBUG_MODE: print(r.stdout)\n",
        "                raise RuntimeError(\"å®‰è£ ninja å¤±æ•—ã€‚è«‹é‡å•Ÿå¾Œé‡è©¦ã€‚\")\n",
        "        env = os.environ.copy()\n",
        "        env[\"CMAKE_ARGS\"] = \"-DGGML_CUDA=on -DLLAMA_CUBLAS=on\"\n",
        "        env[\"FORCE_CMAKE\"] = \"1\"\n",
        "        r = pip_install([\"llama-cpp-python\"], env=env)\n",
        "        if r.returncode != 0:\n",
        "            if DEBUG_MODE: print(r.stdout)\n",
        "            raise RuntimeError(\"ç„¡æ³•å®‰è£ GPU ç‰ˆ llama-cpp-pythonã€‚\")\n",
        "        Llama = try_import_llama()\n",
        "\n",
        "\n",
        "if DEBUG_MODE: print(\"[Summary 2/6] Reading SRT ...\")\n",
        "with open(summary_srt_path_abs, \"r\", encoding=\"utf-8\") as f:\n",
        "    srt_text = f.read()\n",
        "subs = list(_srt.parse(srt_text)) # Use _srt as srt module was imported as _srt\n",
        "def td2s(td): return td.total_seconds()\n",
        "segments = []\n",
        "for it in subs:\n",
        "    txt = it.content.strip()\n",
        "    if not txt:\n",
        "        continue\n",
        "    segments.append((td2s(it.start), td2s(it.end), txt))\n",
        "\n",
        "def compress_repetitive_segments(\n",
        "    segs: List[Tuple[float, float, str]],\n",
        "    *,\n",
        "    short_len: int = 2,\n",
        "    long_duration: float = 60.0,\n",
        "    long_count: int = 30,\n",
        ") -> Tuple[List[Tuple[float, float, str]], List[dict]]:\n",
        "    \"\"\"Collapse repetitive floods in subtitles while keeping context samples.\"\"\"\n",
        "    if not segs:\n",
        "        return [], []\n",
        "    compressed: List[Tuple[float, float, str]] = []\n",
        "    reports: List[dict] = []\n",
        "    i = 0\n",
        "    while i < len(segs):\n",
        "        s, e, text = segs[i]\n",
        "        stripped = text.strip()\n",
        "        run_start = i\n",
        "        run_end = i + 1\n",
        "        total_duration = max(0.0, e - s)\n",
        "        while run_end < len(segs) and segs[run_end][2].strip() == stripped:\n",
        "            total_duration += max(0.0, segs[run_end][1] - segs[run_end][0])\n",
        "            run_end += 1\n",
        "        run_len = run_end - run_start\n",
        "        if run_len > 1 and len(stripped) <= short_len:\n",
        "            marker = f\"[é‡è¤‡ x{run_len}: {stripped}]\"\n",
        "            start_ts = segs[run_start][0]\n",
        "            end_ts = segs[run_end - 1][1]\n",
        "            compressed.append((start_ts, end_ts, marker))\n",
        "            report = {\n",
        "                \"text\": stripped,\n",
        "                \"count\": run_len,\n",
        "                \"duration\": total_duration,\n",
        "                \"start\": start_ts,\n",
        "                \"end\": end_ts,\n",
        "                \"samples\": [],\n",
        "            }\n",
        "            if total_duration >= long_duration and run_len >= long_count:\n",
        "                stride = max(5, min(10, max(1, run_len // 10)))\n",
        "                sample_indices = list(range(run_start, run_end, stride))\n",
        "                # Always keep the final segment for context\n",
        "                if sample_indices[-1] != run_end - 1:\n",
        "                    sample_indices.append(run_end - 1)\n",
        "                for idx in sample_indices[:10]:\n",
        "                    sample_seg = segs[idx]\n",
        "                    sample_txt = sample_seg[2].strip()\n",
        "                    compressed.append((sample_seg[0], sample_seg[1], f\"[é‡è¤‡æ¨£æœ¬] {sample_txt}\"))\n",
        "                    report[\"samples\"].append({\n",
        "                        \"index\": idx,\n",
        "                        \"start\": sample_seg[0],\n",
        "                        \"end\": sample_seg[1],\n",
        "                        \"text\": sample_txt,\n",
        "                    })\n",
        "            reports.append(report)\n",
        "            i = run_end\n",
        "            continue\n",
        "        compressed.append((s, e, text))\n",
        "        i += 1\n",
        "    return compressed, reports\n",
        "\n",
        "segments, repetition_reports = compress_repetitive_segments(segments)\n",
        "if repetition_reports:\n",
        "    print(f\"â†’ åµæ¸¬åˆ° {len(repetition_reports)} çµ„é‡è¤‡æ´ªæ°´ï¼Œå·²å£“ç¸®ä¸¦ä¿ç•™æ¨£æœ¬\")\n",
        "\n",
        "total_secs = (segments[-1][1] - segments[0][0]) if segments else 0\n",
        "if DEBUG_MODE: print(f\"â†’ Number of subtitle segments: {len(segments)}ï¼›Video length (est): {total_secs/60:.1f} minutes\")\n",
        "\n",
        "\n",
        "# ===== Summary 3/6) Download and Load GGUF Model (Summary) - Uses summary model parameters (REPO_ID, GGUF_FILE, ctx_window, etc.)\n",
        "# Moved this section to just after installing llama-cpp-python\n",
        "if DEBUG_MODE: print(\"[Summary 3/6] Loading GPT-OSS-20B (GGUF, CUDA) ...\")\n",
        "local_repo = snapshot_download(REPO_ID, allow_patterns=[GGUF_FILE, \"tokenizer_config.json\"])\n",
        "gguf_path = str(Path(local_repo)/GGUF_FILE)\n",
        "\n",
        "tokenizer_config_path = Path(local_repo)/\"tokenizer_config.json\"\n",
        "if tokenizer_config_path.exists():\n",
        "    try:\n",
        "        tokenizer_config_data = json.loads(tokenizer_config_path.read_text())\n",
        "        if DEBUG_MODE: print(\"â†’ Loaded tokenizer_config.json (Harmony template)\")\n",
        "    except Exception as exc:\n",
        "        if DEBUG_MODE: print(\"  âœ— Failed to parse tokenizer_config.json:\", exc)\n",
        "\n",
        "llm = None\n",
        "selected_ctx_window = None\n",
        "ctx_errors = []\n",
        "for ctx_candidate in CTX_WINDOW_CANDIDATES:\n",
        "    try:\n",
        "        if DEBUG_MODE:\n",
        "            print(f\"  - Trying ctx_window={ctx_candidate}\")\n",
        "        llm = Llama(\n",
        "            model_path=gguf_path,\n",
        "            n_ctx=ctx_candidate,\n",
        "            n_gpu_layers=-1,\n",
        "            seed=0,\n",
        "            logits_all=False,\n",
        "            verbose=True  # Display the actual chat format used\n",
        "        )\n",
        "        selected_ctx_window = ctx_candidate\n",
        "        break\n",
        "    except Exception as exc:\n",
        "        ctx_errors.append((ctx_candidate, exc))\n",
        "        if DEBUG_MODE:\n",
        "            print(f\"  âœ— Failed ctx_window={ctx_candidate}: {exc}\")\n",
        "        gc.collect()\n",
        "\n",
        "if llm is None:\n",
        "    msgs = \", \".join(f\"{cand}â†’{err}\" for cand, err in ctx_errors[-3:])\n",
        "    raise RuntimeError(f\"ç„¡æ³•è¼‰å…¥ GGUFï¼ˆctx candidates={CTX_WINDOW_CANDIDATES}ï¼‰ï¼š{msgs}\")\n",
        "\n",
        "ctx_window = selected_ctx_window or ctx_window\n",
        "print(f\"â†’ Selected ctx_window: {ctx_window}\")\n",
        "if DEBUG_MODE:\n",
        "    print(\"â†’ Model loaded successfully (GPU)\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    ensure_harmony_formatter()\n",
        "    if DEBUG_MODE: print(\"â†’ Harmony formatter prepared\")\n",
        "except Exception as exc:\n",
        "    raise RuntimeError(f\"Failed to prepare Harmony formatter: {exc}\")\n",
        "\n",
        "\n",
        "# ===== Summary 4/6) Token-aware Segmentation (Summary) - Uses ctx_window, map_max_new_tokens, prompt_overhead\n",
        "if DEBUG_MODE: print(\"[Summary 4/6] Generating segments (token-aware; single segment â‰¤ safety limit) ...\")\n",
        "\n",
        "def count_tokens_text(text: str) -> int:\n",
        "    # Check if llm is initialized before using it\n",
        "    if 'llm' not in globals() or llm is None:\n",
        "         raise RuntimeError(\"LLM model is not loaded. Cannot count tokens.\")\n",
        "    return len(llm.tokenize(text.encode(\"utf-8\")))\n",
        "\n",
        "SYSTEM_INSTR = (\n",
        "  \"ä½ æ˜¯ä¸€å€‹æœƒè­°ç¸½çµæ©Ÿå™¨äººã€‚æ ¹æ“šä½¿ç”¨è€…æä¾›çš„é€å­—ç¨¿ï¼ˆå¯èƒ½é›œè¨Šã€é‡è¤‡ã€éŒ¯å­—ï¼‰ï¼Œ\"\n",
        "  \"è«‹åŽ»é™¤é›œè¨Šèˆ‡é‡è¤‡ã€åš´å®ˆäº‹å¯¦ã€ä¸è…¦è£œã€‚é‡åˆ°ä¸æ˜Žç¢ºè³‡è¨Šä»¥ã€Œå¾…è£œå……ï¼æœªæ˜Žç¢ºã€æ¨™è¨»ã€‚\"\n",
        "  \"è¼¸å‡ºç‚º Markdownï¼ˆç¹é«”ä¸­æ–‡ï¼‰ï¼Œä¸è¦è¼¸å‡ºä»»ä½•ç³»çµ±ï¼æ€è€ƒæ¨™è¨˜ã€‚\"\n",
        ")\n",
        "\n",
        "# â€” Segment Summary Prompt: More concise request, avoid verbosity and system language - Uses 'topic_hint'\n",
        "MAP_USER_TMPL = textwrap.dedent(\"\"\"\\\n",
        "ä¸»é¡Œï¼ˆå¯ç•™ç©ºï¼‰ï¼š{topic}\n",
        "\n",
        "ä»¥ä¸‹æ˜¯é€å­—ç¨¿ç‰‡æ®µï¼ˆéžå®Œæ•´å…¨æ–‡ï¼‰ï¼š\n",
        "{chunk}\n",
        "\n",
        "è«‹å°±æ­¤ç‰‡æ®µè¼¸å‡ºã€Œæ¢åˆ—å¼é‡é»žæ‘˜è¦ã€ï¼ˆ500â€“900 å­—ï¼Œç¹é«”ä¸­æ–‡ï¼‰ï¼Œæ³¨æ„ï¼š\n",
        "- åªå¯«æœ€çµ‚å…§å®¹ï¼Œä¸è¦å¯«è§£é¡Œæƒ³æ³•ã€ä¸è¦å‡ºç¾ä»»ä½•ç³»çµ±æç¤ºæˆ–ä¸­è‹±æ‹¬è™Ÿæ¨™è¨˜ã€‚\n",
        "- èšç„¦å¯é©—è­‰äº‹å¯¦ï¼ˆæ™‚é–“ã€äººç‰©ã€ä»»å‹™ã€çµè«–ã€æœªæ±ºäº‹é …ã€è¡Œå‹•ï¼‰ã€‚\n",
        "- çµæ§‹ï¼šå¯ç”¨å°æ¨™é¡Œï¼‹é …ç›®ç¬¦è™Ÿï¼Œèªžå¥å‹™å¿…çŸ­ã€æº–ç¢ºã€ç„¡è´…è©žã€‚\n",
        "\"\"\")\n",
        "\n",
        "# â€” Summary Prompt: Maintain your three-section output structure - Uses 'topic_hint'\n",
        "REDUCE_USER_TMPL = textwrap.dedent(\"\"\"\\\n",
        "ä¸»é¡Œï¼ˆå¯ç•™ç©ºï¼‰ï¼š{topic}\n",
        "\n",
        "ä»¥ä¸‹æ˜¯æ‰€æœ‰ç‰‡æ®µçš„é‡é»žæ‘˜è¦å½™æ•´ï¼ˆä»å¯èƒ½æœ‰é‡ç–Šï¼‰ï¼š\n",
        "{maps}\n",
        "\n",
        "è«‹æ•´åˆç‚ºä¸€ä»½æœƒè­°ç­†è¨˜ï¼ˆMarkdownï¼Œç¹é«”ï¼‰ï¼š\n",
        "1) **æ•´é«”æè¦**ï¼ˆ3â€“6 å¥ï¼Œé¿å…å†—è¨€ï¼‰\n",
        "2) **ç« ç¯€è¦é»žï¼ˆå«æ™‚é–“è„ˆçµ¡ï¼‰**ï¼šæ¢åˆ—å‘ˆç¾ï¼Œæ¯é»žä¸€è¡Œï¼Œå¯é™„ç²—ç•¥æ™‚é–“ï¼Œ**ä¸å¾—å¿½ç•¥ä»»ä½•ç‰‡æ®µ**ï¼Œæ¯æ¢å°¾ç«¯æ¨™è¨»å°æ‡‰ç‰‡æ®µç·¨è™Ÿ (ç‰‡æ®µ i) æˆ–æ™‚é–“ã€‚\n",
        "3) **å¯åŸ·è¡Œé‡é»ž**ï¼šå…·é«”å¾…è¾¦ï¼ˆæ¯æ¢ä»¥å‹•è©žé–‹é ­ï¼‰\n",
        "è«‹ç¢ºä¿æ‰€æœ‰ç‰‡æ®µè‡³å°‘ç´å…¥ 1â€“2 å€‹é‡é»žï¼Œè‹¥è³‡è¨Šä¸è¶³è«‹è¨»æ˜Žã€Œå¾…è£œå……ã€ã€‚\n",
        "è«‹åªè¼¸å‡ºæœ€çµ‚ç­†è¨˜ï¼Œä¸è¦å‡ºç¾ç³»çµ±æˆ–æ€è€ƒæ¨™è¨˜ï¼Œä¸è¦åŠ å…¥æœªå‡ºç¾çš„æ–°è³‡è¨Šã€‚\n",
        "\"\"\")\n",
        "\n",
        "# Single segment token budget (reserve space for prompt and generation)\n",
        "prompt_overhead = 700\n",
        "chunk_target    = max(1024, min(2048, ctx_window - prompt_overhead - map_max_new_tokens))\n",
        "SENTENCE_END_RE = re.compile(r\"[ã€‚ï¼ï¼Ÿ?!â€¦]+[\\\"â€ã€ã€‘ï¼‰]*$\")\n",
        "\n",
        "def build_semantic_segments(raw_segments: List[Tuple[float, float, str]]) -> List[Tuple[float, float, str, int]]:\n",
        "    semantic_segments: List[Tuple[float, float, str, int]] = []\n",
        "    buffer: List[str] = []\n",
        "    buffer_tokens = 0\n",
        "    buffer_chars = 0\n",
        "    buffer_line_count = 0\n",
        "    lines_since_punct = 0\n",
        "    start_time: Optional[float] = None\n",
        "    buffer_end: Optional[float] = None\n",
        "    last_end: Optional[float] = None\n",
        "\n",
        "    def flush_buffer():\n",
        "        nonlocal buffer, buffer_tokens, buffer_chars, buffer_line_count, lines_since_punct, start_time, buffer_end\n",
        "        if not buffer:\n",
        "            return\n",
        "        text = \"\\n\".join(buffer).strip()\n",
        "        if text:\n",
        "            semantic_segments.append((start_time or 0.0, buffer_end or start_time or 0.0, text, buffer_tokens))\n",
        "        buffer = []\n",
        "        buffer_tokens = 0\n",
        "        buffer_chars = 0\n",
        "        buffer_line_count = 0\n",
        "        lines_since_punct = 0\n",
        "        start_time = None\n",
        "        buffer_end = None\n",
        "\n",
        "    for s, e, txt in raw_segments:\n",
        "        txt = txt.strip()\n",
        "        if not txt:\n",
        "            last_end = e\n",
        "            continue\n",
        "\n",
        "        if buffer and last_end is not None:\n",
        "            gap = max(0.0, s - last_end)\n",
        "            if gap >= SEMANTIC_PAUSE_THRESHOLD:\n",
        "                flush_buffer()\n",
        "\n",
        "        if not buffer:\n",
        "            start_time = s\n",
        "\n",
        "        buffer.append(txt)\n",
        "        piece_tokens = count_tokens_text(txt)\n",
        "        buffer_tokens += piece_tokens\n",
        "        buffer_chars += len(txt)\n",
        "        buffer_line_count += 1\n",
        "        if SENTENCE_END_RE.search(txt):\n",
        "            lines_since_punct = 0\n",
        "        else:\n",
        "            lines_since_punct += 1\n",
        "        buffer_end = e\n",
        "        last_end = e\n",
        "\n",
        "        buffer_duration = 0.0\n",
        "        if start_time is not None and buffer_end is not None:\n",
        "            buffer_duration = max(0.0, buffer_end - start_time)\n",
        "\n",
        "        should_flush = False\n",
        "        if buffer_tokens >= chunk_target:\n",
        "            should_flush = True\n",
        "        elif buffer_tokens >= SEMANTIC_MIN_TOKENS and SENTENCE_END_RE.search(txt):\n",
        "            should_flush = True\n",
        "        elif buffer_chars >= SEMANTIC_MAX_CHARS and SENTENCE_END_RE.search(txt):\n",
        "            should_flush = True\n",
        "        elif lines_since_punct >= SEMANTIC_FORCE_FLUSH_LINES:\n",
        "            should_flush = True\n",
        "        elif buffer_duration >= SEMANTIC_FORCE_FLUSH_SECONDS:\n",
        "            should_flush = True\n",
        "\n",
        "        if should_flush:\n",
        "            flush_buffer()\n",
        "\n",
        "    flush_buffer()\n",
        "    return semantic_segments\n",
        "\n",
        "\n",
        "semantic_segments = build_semantic_segments(segments)\n",
        "\n",
        "chunks: List[Tuple[float, float, str]] = []\n",
        "i = 0\n",
        "while i < len(semantic_segments):\n",
        "    window: List[Tuple[float, float, str, int]] = []\n",
        "    total_tokens = 0\n",
        "    start_ts = semantic_segments[i][0]\n",
        "    end_ts = semantic_segments[i][1]\n",
        "    j = i\n",
        "    while j < len(semantic_segments):\n",
        "        seg = semantic_segments[j]\n",
        "        seg_tokens = seg[3]\n",
        "        # Always include at least one semantic block per chunk\n",
        "        if window and total_tokens + seg_tokens > chunk_target and SENTENCE_END_RE.search(window[-1][2]):\n",
        "            break\n",
        "        window.append(seg)\n",
        "        total_tokens += seg_tokens\n",
        "        end_ts = seg[1]\n",
        "        j += 1\n",
        "        if total_tokens >= chunk_target or (total_tokens >= SEMANTIC_MIN_TOKENS and SENTENCE_END_RE.search(seg[2])):\n",
        "            break\n",
        "\n",
        "    chunk_text = \"\".join(seg[2] for seg in window).strip()\n",
        "    if chunk_text:\n",
        "        chunks.append((start_ts, end_ts, chunk_text))\n",
        "\n",
        "    if j >= len(semantic_segments):\n",
        "        break\n",
        "\n",
        "    overlap_segments = 0\n",
        "    if len(window) > 1:\n",
        "        trailing_tokens = 0\n",
        "        for idx in range(len(window) - 1, -1, -1):\n",
        "            trailing_tokens += window[idx][3]\n",
        "            if trailing_tokens >= SLIDING_OVERLAP_TOKENS:\n",
        "                break\n",
        "            overlap_segments += 1\n",
        "        overlap_segments = min(overlap_segments, len(window) - 1)\n",
        "\n",
        "    next_i = j if overlap_segments == 0 else max(j - overlap_segments, i + 1)\n",
        "    i = max(next_i, i + 1)\n",
        "\n",
        "if DEBUG_MODE:\n",
        "    avg_tokens = (sum(seg[3] for seg in semantic_segments) / max(len(semantic_segments), 1)) if semantic_segments else 0\n",
        "    print(f\"â†’ Semantic segments: {len(semantic_segments)} (avg tokens â‰ˆ {avg_tokens:.0f}); summary windows: {len(chunks)} (target ~{chunk_target} tokens)\")\n",
        "\n",
        "# ===== Common: Streaming Tools (No regex cleaning; use correct stop sequence) - Uses temperature, top_p, repeat_penalty, map_max_new_tokens, reduce_max_new_tokens\n",
        "\n",
        "\n",
        "STREAM_FALLBACK_USED = False\n",
        "ARTIFACT_TAG_RE = re.compile(r\"<\\|[^|]*\\|>\")\n",
        "def clean_harmony_artifacts(s: str) -> str:\n",
        "    return ARTIFACT_TAG_RE.sub(\"\", s or \"\")\n",
        "\n",
        "def stream_harmony_final_pieces(text_chunks: Iterable[str]) -> Iterator[str]:\n",
        "    \"\"\"Yield Harmony streamed text, preferring the final channel.\n",
        "\n",
        "    Some community models only emit the assistant channel; fall back to it\n",
        "    so we do not drop the actual content.\n",
        "    \"\"\"\n",
        "    global STREAM_FALLBACK_USED\n",
        "    buffer = \"\"\n",
        "    current_channel: Optional[str] = None\n",
        "    pending_channel = False\n",
        "    channel_name_buffer = \"\"\n",
        "    in_message = False\n",
        "    assistant_cache: List[str] = []\n",
        "    final_seen = False\n",
        "    any_text_emitted = False\n",
        "    raw_plain_chunks: List[str] = []\n",
        "\n",
        "    def canonical_channel(name: Optional[str]) -> str:\n",
        "        if not name:\n",
        "            return \"\"\n",
        "        lowered = name.strip().lower()\n",
        "        if not lowered:\n",
        "            return \"\"\n",
        "        if \"final\" in lowered:\n",
        "            return \"final\"\n",
        "        if \"assistant\" in lowered:\n",
        "            return \"assistant\"\n",
        "        return lowered\n",
        "\n",
        "    def emit_text(text: str):\n",
        "        nonlocal final_seen, assistant_cache, any_text_emitted\n",
        "        if not text:\n",
        "            return\n",
        "        channel = canonical_channel(current_channel)\n",
        "        if channel == \"final\":\n",
        "            final_seen = True\n",
        "            if assistant_cache:\n",
        "                cached = assistant_cache[:]\n",
        "                assistant_cache.clear()\n",
        "                for cached_piece in cached:\n",
        "                    if cached_piece:\n",
        "                        any_text_emitted = True\n",
        "                        yield cached_piece\n",
        "            any_text_emitted = True\n",
        "            yield text\n",
        "        elif channel == \"assistant\":\n",
        "            if final_seen:\n",
        "                any_text_emitted = True\n",
        "                yield text\n",
        "            else:\n",
        "                assistant_cache.append(text)\n",
        "        elif final_seen:\n",
        "            any_text_emitted = True\n",
        "            yield text\n",
        "\n",
        "    for piece in text_chunks:\n",
        "        if not piece:\n",
        "            continue\n",
        "        raw_plain_chunks.append(piece)\n",
        "        buffer += piece\n",
        "        while True:\n",
        "            if pending_channel:\n",
        "                idx = buffer.find(\"<|\")\n",
        "                if idx == -1:\n",
        "                    channel_name_buffer += buffer\n",
        "                    buffer = \"\"\n",
        "                    break\n",
        "                channel_name_buffer += buffer[:idx]\n",
        "                buffer = buffer[idx:]\n",
        "                channel = channel_name_buffer.strip()\n",
        "                channel_name_buffer = \"\"\n",
        "                pending_channel = False\n",
        "                current_channel = channel\n",
        "                channel_canonical = canonical_channel(current_channel)\n",
        "                in_message = bool(channel_canonical in {\"assistant\", \"final\"})\n",
        "                continue\n",
        "            tag_start = buffer.find(\"<|\")\n",
        "            if tag_start == -1:\n",
        "                if in_message or canonical_channel(current_channel) in {\"assistant\", \"final\"}:\n",
        "                    for out in emit_text(buffer):\n",
        "                        yield out\n",
        "                buffer = \"\"\n",
        "                break\n",
        "            if tag_start > 0:\n",
        "                text = buffer[:tag_start]\n",
        "                if in_message or canonical_channel(current_channel) in {\"assistant\", \"final\"}:\n",
        "                    for out in emit_text(text):\n",
        "                        yield out\n",
        "                buffer = buffer[tag_start:]\n",
        "            tag_end = buffer.find(\"|>\")\n",
        "            if tag_end == -1:\n",
        "                break\n",
        "            tag = buffer[2:tag_end].strip().lower()\n",
        "            buffer = buffer[tag_end + 2:]\n",
        "            if tag == \"start\":\n",
        "                current_channel = None\n",
        "                in_message = False\n",
        "            elif tag == \"channel\":\n",
        "                pending_channel = True\n",
        "            elif tag == \"message\":\n",
        "                in_message = True\n",
        "            elif tag == \"end\":\n",
        "                in_message = False\n",
        "                current_channel = None\n",
        "            elif tag == \"return\":\n",
        "                if not final_seen and assistant_cache:\n",
        "                    for cached_piece in assistant_cache:\n",
        "                        if cached_piece:\n",
        "                            any_text_emitted = True\n",
        "                            yield cached_piece\n",
        "                    assistant_cache.clear()\n",
        "                return\n",
        "            else:\n",
        "                continue\n",
        "    if (in_message or canonical_channel(current_channel) in {\"assistant\", \"final\"}) and buffer:\n",
        "        for out in emit_text(buffer):\n",
        "            yield out\n",
        "    if not final_seen and assistant_cache:\n",
        "        for cached_piece in assistant_cache:\n",
        "            if cached_piece:\n",
        "                any_text_emitted = True\n",
        "                yield cached_piece\n",
        "    if not any_text_emitted and raw_plain_chunks:\n",
        "        fallback_text = \"\".join(raw_plain_chunks).strip()\n",
        "        if fallback_text:\n",
        "            STREAM_FALLBACK_USED = True\n",
        "            fallback_text = clean_harmony_artifacts(fallback_text)\n",
        "            print(\"stream-flush fallback used\")\n",
        "            yield fallback_text\n",
        "\n",
        "\n",
        "def llm_stream(messages, max_tokens, *, repeat_penalty_override=None):\n",
        "    \"\"\"Stream Harmony-formatted completions and yield only the final channel.\"\"\"\n",
        "    if 'llm' not in globals() or llm is None:\n",
        "        raise RuntimeError(\"LLM model is not loaded. Cannot stream generation.\")\n",
        "    formatter = ensure_harmony_formatter()\n",
        "    chat_response = formatter(\n",
        "        messages=messages,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    completion_kwargs = dict(\n",
        "        prompt=chat_response.prompt,\n",
        "        temperature=float(temperature),\n",
        "        top_p=float(top_p),\n",
        "        repeat_penalty=float(repeat_penalty_override if repeat_penalty_override is not None else repeat_penalty),\n",
        "        max_tokens=int(max_tokens),\n",
        "        stream=True,\n",
        "    )\n",
        "    if chat_response.stop:\n",
        "        completion_kwargs[\"stop\"] = chat_response.stop\n",
        "    if chat_response.stopping_criteria is not None:\n",
        "        completion_kwargs[\"stopping_criteria\"] = chat_response.stopping_criteria\n",
        "\n",
        "    gen = llm.create_completion(**completion_kwargs)\n",
        "\n",
        "    def _iter_text_stream(events):\n",
        "        for ev in events:\n",
        "            yield ev.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
        "\n",
        "    text_stream = _iter_text_stream(gen)\n",
        "\n",
        "    for final_piece in stream_harmony_final_pieces(text_stream):\n",
        "        if final_piece:\n",
        "            yield final_piece\n",
        "\n",
        "# ===== Summary 5/6) Segment Summary (map) - Uses map_max_new_tokens, ctx_window, prompt_overhead, topic_hint\n",
        "if DEBUG_MODE: print(\"[Summary 5/6] Segment summarization (map) ...\")\n",
        "maps: List[str] = []\n",
        "map_stats: List[dict] = []\n",
        "map_debug_payload: List[dict] = []\n",
        "\n",
        "\n",
        "def escape_braces(text: str) -> str:\n",
        "    \"\"\"Escape braces so str.format does not treat user content as placeholders.\"\"\"\n",
        "    return text.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "\n",
        "\n",
        "def format_timestamp(seconds: float) -> str:\n",
        "    seconds = max(0, int(seconds))\n",
        "    h, rem = divmod(seconds, 3600)\n",
        "    m, s = divmod(rem, 60)\n",
        "    if h:\n",
        "        return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
        "    return f\"{m:02d}:{s:02d}\"\n",
        "\n",
        "\n",
        "safe_topic_hint = escape_braces(topic_hint or \"ï¼ˆç„¡ï¼‰\")\n",
        "map_generation_limit = map_max_new_tokens\n",
        "if len(chunks) > 6:\n",
        "    map_generation_limit = min(map_generation_limit, 600)\n",
        "    if DEBUG_MODE:\n",
        "        print(f\"â†’ å‹•æ…‹èª¿æ•´ map_max_new_tokens â†’ {map_generation_limit}\")\n",
        "\n",
        "for i, (s, e, body) in enumerate(chunks, 1):\n",
        "    pct = i / max(len(chunks), 1) * 100\n",
        "    print(f\"  - è™•ç†åˆ†æ®µ {i}/{len(chunks)}ï¼ˆ~{pct:.1f}%ï¼‰\")\n",
        "\n",
        "    budget_tokens = max(512, ctx_window - map_generation_limit - prompt_overhead)\n",
        "    def shrink_to_budget(text: str, budget_tokens: int) -> str:\n",
        "        cur = text\n",
        "        for _ in range(6):\n",
        "            if count_tokens_text(cur) <= budget_tokens:\n",
        "                return cur\n",
        "            keep = max(800, int(len(cur) * 0.85))\n",
        "            cur = cur[:keep]\n",
        "        return cur\n",
        "    body2 = shrink_to_budget(body, budget_tokens)\n",
        "    input_tokens = count_tokens_text(body2)\n",
        "    input_chars = len(body2)\n",
        "\n",
        "    safe_body = escape_braces(body2)\n",
        "    user_txt = MAP_USER_TMPL.format(topic=safe_topic_hint, chunk=safe_body)\n",
        "    user_txt = user_txt.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_INSTR},\n",
        "        {\"role\": \"user\",   \"content\": user_txt},\n",
        "    ]\n",
        "\n",
        "    live = display(Markdown(\"\"), display_id=True)\n",
        "    part_buf: List[str] = []\n",
        "    for token in llm_stream(messages, map_generation_limit, repeat_penalty_override=map_repeat_penalty):\n",
        "        part_buf.append(token)\n",
        "        if len(part_buf) % 24 == 0:\n",
        "            cur_txt = \"\".join(part_buf)\n",
        "            live.update(Markdown(cur_txt))\n",
        "            print(f\"    â†³ åˆ†æ®µ {i} å·²ç”¢ç”Ÿå­—å…ƒï¼š{len(cur_txt)}\")\n",
        "    cur_txt = \"\".join(part_buf)\n",
        "    if STREAM_FALLBACK_USED:\n",
        "        cur_txt = clean_harmony_artifacts(cur_txt)\n",
        "    live.update(Markdown(cur_txt))\n",
        "    print(f\"    â†³ åˆ†æ®µ {i} æœ€çµ‚å­—å…ƒï¼š{len(cur_txt)}\")\n",
        "\n",
        "    cleaned_txt = cur_txt.strip()\n",
        "    map_empty = not cleaned_txt\n",
        "    if map_empty:\n",
        "        cleaned_txt = f\"[ç‰‡æ®µ {i} ç„¡è¼¸å‡ºï¼›æ™‚é–“ {format_timestamp(s)}â€“{format_timestamp(e)}]\"\n",
        "        print(f\"    âš ï¸ ç‰‡æ®µ {i} ç„¡è¼¸å‡ºï¼Œå·²å¯«å…¥å ä½è¨Šæ¯\")\n",
        "\n",
        "    output_tokens = count_tokens_text(cleaned_txt)\n",
        "    try:\n",
        "        token_ids = llm.tokenize(cleaned_txt.encode(\"utf-8\"))\n",
        "    except Exception:\n",
        "        token_ids = []\n",
        "    unique_ratio = len(set(token_ids)) / max(1, len(token_ids))\n",
        "    map_entry = {\n",
        "        \"index\": i,\n",
        "        \"input_tokens\": input_tokens,\n",
        "        \"input_chars\": input_chars,\n",
        "        \"output_tokens\": output_tokens,\n",
        "        \"output_chars\": len(cleaned_txt),\n",
        "        \"hit_limit\": output_tokens >= map_generation_limit,\n",
        "        \"start\": s,\n",
        "        \"end\": e,\n",
        "        \"empty\": map_empty,\n",
        "        \"unique_token_ratio\": unique_ratio,\n",
        "        \"max_new_tokens\": map_generation_limit,\n",
        "    }\n",
        "    map_stats.append(map_entry)\n",
        "    map_debug_payload.append({\n",
        "        \"meta\": map_entry,\n",
        "        \"text\": cleaned_txt,\n",
        "    })\n",
        "    maps.append(cleaned_txt)\n",
        "\n",
        "print(f\"[Map Summary] chunks={len(chunks)} / maps={len(maps)}\")\n",
        "if repetition_reports:\n",
        "    for report in repetition_reports:\n",
        "        duration = report[\"duration\"]\n",
        "        start_ts = format_timestamp(report[\"start\"])\n",
        "        end_ts = format_timestamp(report[\"end\"])\n",
        "        print(\n",
        "            f\"  â†³ é‡è¤‡å£“ç¸®ï¼š'{report['text']}' x{report['count']} | æ™‚é–“ {start_ts}â€“{end_ts} | æŒçºŒ {duration:.1f}s\"\n",
        "        )\n",
        "\n",
        "for stat in map_stats:\n",
        "    print(\n",
        "        f\"  - Map {stat['index']:02d}: in={stat['input_tokens']} tok/{stat['input_chars']} chars, \"\n",
        "        f\"out={stat['output_tokens']} tok/{stat['output_chars']} chars, \"\n",
        "        f\"unique_ratio={stat['unique_token_ratio']:.2f}, hit_limit={stat['hit_limit']}, empty={stat['empty']}\"\n",
        "    )\n",
        "\n",
        "if DEBUG_MODE: print(\"â†’ Segment summarization complete\")\n",
        "\n",
        "summary_stem = Path(summary_srt_path_abs).stem\n",
        "debug_output_dir = out_base_dir\n",
        "debug_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "map_file_paths = []\n",
        "\n",
        "maps_md_parts = []\n",
        "for payload in map_debug_payload:\n",
        "    meta = payload[\"meta\"]\n",
        "    text_body = payload[\"text\"]\n",
        "    seg_header = (\n",
        "        f\"[SEG {meta['index']} | tâ‰ˆ{format_timestamp(meta['start'])}â€“{format_timestamp(meta['end'])} | \"\n",
        "        f\"lenâ‰ˆ{meta['output_tokens']} tok]\"\n",
        "    )\n",
        "    maps_md_parts.append(f\"{seg_header}\\n### ç‰‡æ®µ {meta['index']} è¦é»ž\\n\\n{text_body}\")\n",
        "    map_path = debug_output_dir / f\"{summary_stem}_map_{meta['index']:02d}.md\"\n",
        "    header_lines = [\n",
        "        f\"# ç‰‡æ®µ {meta['index']:02d} | æ™‚é–“ {format_timestamp(meta['start'])}â€“{format_timestamp(meta['end'])}\",\n",
        "        f\"> in={meta['input_tokens']} tok/{meta['input_chars']} chars | out={meta['output_tokens']} tok/{meta['output_chars']} chars | unique_ratio={meta['unique_token_ratio']:.2f}\",\n",
        "        \"\",\n",
        "    ]\n",
        "    map_path.write_text(\"\\n\".join(header_lines) + text_body + \"\\n\", encoding=\"utf-8\")\n",
        "    map_file_paths.append(map_path)\n",
        "\n",
        "maps_md_with_headers = \"\\n\\n---\\n\\n\".join(maps_md_parts)\n",
        "reduce_input_path = debug_output_dir / f\"{summary_stem}_reduce_input.md\"\n",
        "reduce_input_content = maps_md_with_headers\n",
        "if repetition_reports:\n",
        "    diag_lines = [\"\", \"<!-- é‡è¤‡æ´ªæ°´å£“ç¸®åŽŸå§‹ç‰‡æ®µ -->\"]\n",
        "    for report in repetition_reports:\n",
        "        diag_lines.append(\n",
        "            f\"<!-- {format_timestamp(report['start'])}â€“{format_timestamp(report['end'])} | '{report['text']}' x{report['count']} | {report['duration']:.1f}s -->\"\n",
        "        )\n",
        "        for sample in report.get('samples', []):\n",
        "            diag_lines.append(\n",
        "                f\"<!--   sample {format_timestamp(sample['start'])}â€“{format_timestamp(sample['end'])}: {sample['text']} -->\"\n",
        "            )\n",
        "    reduce_input_content += \"\\n\".join(diag_lines)\n",
        "reduce_input_path.write_text(reduce_input_content + \"\\n\", encoding=\"utf-8\")\n",
        "print(f\"â†’ Map è¼¸å‡ºå·²å¯«å…¥ {len(map_file_paths)} æ®µï¼›reduce_input: {reduce_input_path}\")\n",
        "\n",
        "if DEBUG_MODE: print(\"[Summary 6/6] Consolidating summary (reduce) ...\")\n",
        "\n",
        "# If combined text exceeds window, truncate proportionally first (without changing text within segments to avoid breaking meaning)\n",
        "def fit_reduce_payload(md_text: str, max_ctx_tokens: int) -> str:\n",
        "    for _ in range(8):\n",
        "        need = count_tokens_text(md_text)\n",
        "        if need + reduce_max_new_tokens + 400 <= max_ctx_tokens:\n",
        "            return md_text\n",
        "        md_text = md_text[: int(len(md_text) * 0.9)]\n",
        "    return md_text\n",
        "\n",
        "md_cur = fit_reduce_payload(maps_md_with_headers, ctx_window)\n",
        "\n",
        "safe_md_cur = escape_braces(md_cur)\n",
        "user_txt = REDUCE_USER_TMPL.format(topic=safe_topic_hint, maps=safe_md_cur)\n",
        "user_txt = user_txt.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
        "messages = [{\"role\":\"system\",\"content\":SYSTEM_INSTR},\n",
        "            {\"role\":\"user\",\"content\":user_txt}]\n",
        "\n",
        "live2 = display(Markdown(\"\"), display_id=True)\n",
        "final_buf = []\n",
        "for token in llm_stream(messages, reduce_max_new_tokens, repeat_penalty_override=reduce_repeat_penalty):\n",
        "    final_buf.append(token)\n",
        "    if len(final_buf) % 24 == 0:\n",
        "        current_text = \"\".join(final_buf)\n",
        "        live2.update(Markdown(current_text))\n",
        "        print(f\"    â†³ å½™æ•´ å·²ç”¢ç”Ÿå­—å…ƒï¼š{len(current_text)}\")\n",
        "live2.update(Markdown(\"\".join(final_buf)))\n",
        "print(f\"    â†³ å½™æ•´ æœ€çµ‚å­—å…ƒï¼š{len(\"\".join(final_buf))}\")\n",
        "\n",
        "final_text = \"\".join(final_buf).strip()\n",
        "if STREAM_FALLBACK_USED:\n",
        "    final_text = clean_harmony_artifacts(final_text)\n",
        "reduce_input_tokens = count_tokens_text(md_cur)\n",
        "reduce_output_tokens = count_tokens_text(final_text)\n",
        "reduce_hit_limit = reduce_output_tokens >= reduce_max_new_tokens\n",
        "\n",
        "missing_segments = []\n",
        "for payload in map_debug_payload:\n",
        "    meta = payload[\"meta\"]\n",
        "    idx = meta[\"index\"]\n",
        "    patterns = [\n",
        "        rf\"ç‰‡æ®µ\\s*{idx}\\b\",\n",
        "        re.escape(format_timestamp(meta[\"start\"])),\n",
        "        re.escape(format_timestamp(meta[\"end\"])),\n",
        "    ]\n",
        "    if not any(re.search(pattern, final_text) for pattern in patterns):\n",
        "        missing_segments.append(idx)\n",
        "if missing_segments:\n",
        "    print(f\"âš ï¸ ç« ç¯€è¦†è“‹æª¢æŸ¥ï¼šç¼ºå°‘ç‰‡æ®µ {', '.join(str(i) for i in missing_segments)}\")\n",
        "else:\n",
        "    print(\"â†’ ç« ç¯€è¦†è“‹æª¢æŸ¥é€šéŽï¼ˆæ‰€æœ‰ç‰‡æ®µçš†è¢«æåŠï¼‰\")\n",
        "\n",
        "# Determine and create the summary output directory\n",
        "summary_output_dir_abs = out_base_dir\n",
        "summary_output_dir_abs.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Determine the summary output file path using the stem of the input SRT\n",
        "out_md = summary_output_dir_abs / f\"{Path(summary_srt_path_abs).stem}_summary.md\"\n",
        "\n",
        "\n",
        "with open(out_md, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(final_text)\n",
        "\n",
        "print(f\"â†’ å®Œæˆ âœ…  {out_md}\")\n",
        "print(\"[Summary Metrics]\")\n",
        "print(f\"â†’ ctx_window candidates: {CTX_WINDOW_CANDIDATES} | selected: {ctx_window}\")\n",
        "print(f\"â†’ Segments: {len(chunks)} | Maps: {len(maps)}\")\n",
        "for stat in map_stats:\n",
        "    ratio = stat[\"output_tokens\"] / max(1, stat.get(\"max_new_tokens\", map_max_new_tokens))\n",
        "    print(\n",
        "        f\"  - Map {stat['index']:02d}: in={stat['input_tokens']} tok/{stat['input_chars']} chars, \"\n",
        "        f\"out={stat['output_tokens']} tok/{stat['output_chars']} chars, \"\n",
        "        f\"unique_ratio={stat['unique_token_ratio']:.2f}, hit_limit={stat['hit_limit']}, empty={stat['empty']} (ratio={ratio:.2f})\"\n",
        "    )\n",
        "print(\n",
        "    f\"  â†³ Reduce: in={reduce_input_tokens} tok, out={reduce_output_tokens} tok, \"\n",
        "    f\"hit_limit={reduce_hit_limit} (ratio={reduce_output_tokens / max(1, reduce_max_new_tokens):.2f})\"\n",
        ")\n",
        "if missing_segments:\n",
        "    print(f\"  â†³ Reduce coverage warning: æœªæåŠç‰‡æ®µ {missing_segments}\")\n",
        "else:\n",
        "    print(\"  â†³ Reduce coverage OK (all segments referenced)\")\n",
        "if STREAM_FALLBACK_USED:\n",
        "    print(\"  â†³ stream-flush fallback used\")\n",
        "try:\n",
        "    del llm\n",
        "except Exception:\n",
        "    pass\n",
        "gc.collect()\n",
        "if DEBUG_MODE: print(\"ï¼ˆé¡¯å­˜å·²é‡‹æ”¾ï¼Œå¦‚éœ€é‡è·‘å¯ç›´æŽ¥å†æ¬¡åŸ·è¡Œï¼‰\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
