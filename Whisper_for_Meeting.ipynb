{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umdiJ5QkzC9e"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dao-you/Whisper-for-Meeting-on-Colab/blob/main/Whisper_for_Meeting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95849644-392d-48b8-8058-d84b741259a3"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# =========================================================\n",
        "# Whisper Automatic Subtitle Generation: GPU Transcription + CPU Denoising + OpenCC Post-processing (Traditional/Simplified Conversion)\n",
        "# And LLM Summarization (GPT-OSS-20B / llama.cpp / CUDA)\n",
        "# - Transcription: faster-whisper (CUDA, compute: int8_float16→float16→int8)\n",
        "# - Denoising: ffmpeg afftdn (CPU)\n",
        "# - Progress: Real-time printing of \"current sentence + video total length percentage\"\n",
        "# - Network source download and output: MyDrive/whisper; Files in Drive: Output to the same folder\n",
        "# - LLM Summary: llama.cpp + GPT-OSS-20B GGUF for summarizing transcription\n",
        "# - Prompts \"Delete runtime and restart\" if download is blocked or abnormal\n",
        "# =========================================================\n",
        "\n",
        "# Restrict multithreading (more stable)\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "# [1/8] Mount Google Drive\n",
        "from google.colab import drive\n",
        "print(\"請授權以掛載 Google Drive\")\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "\n",
        "# Built-in Imports\n",
        "import sys, gc, shutil, datetime, subprocess as sp\n",
        "from pathlib import Path\n",
        "import re, math, time, textwrap\n",
        "from typing import List, Tuple, Optional, Iterable, Iterator\n",
        "from collections import defaultdict\n",
        "from IPython.display import display, Markdown\n",
        "import json\n",
        "\n",
        "\n",
        "ROOT = Path(\"/content/gdrive/MyDrive\")\n",
        "WHISPER_DIR = ROOT / \"whisper\"\n",
        "WHISPER_DIR.mkdir(exist_ok=True, parents=True)\n",
        "os.chdir(ROOT)\n",
        "print(f\"→ 當前工作目錄：{os.getcwd()}\")\n",
        "\n",
        "# [2/8] User Form Parameters (Unified)\n",
        "#@markdown # Whisper Transcription & LLM Summary Pipeline\n",
        "\n",
        "#@markdown ## Input & Transcription Settings\n",
        "#@markdown **Input Source:** Google Drive file (relative to MyDrive) or video URL (YouTube/HTTP).\n",
        "filename = \"whisper/2022第三屆青春來說課-第1會議室吳土城教授回饋.mp4\"  #@param {type:\"string\"}\n",
        "#@markdown **Download Option:** Check to save network source files to `MyDrive/whisper`.\n",
        "save_video_to_google_drive = True  #@param {type:\"boolean\"}\n",
        "#@markdown **Whisper Model Size:** Choose a model size. `large-v3` requires more GPU VRAM; `medium` is a good alternative if VRAM is limited.\n",
        "model_size = \"medium\"  #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v2\", \"large-v3\"] # Changed model_size to \"medium\"\n",
        "#@markdown **Language:** Select the language for transcription. \"自動偵測\" (Auto-detect) is usually sufficient.\n",
        "language = \"自動偵測\"  #@param [\"自動偵測\", \"中文\", \"英文\"]\n",
        "#@markdown **Denoising:** Apply CPU-based denoising to the audio before transcription. `afftdn` is recommended.\n",
        "denoise_method = \"afftdn (建議)\"  #@param [\"afftdn (建議)\", \"none\"]\n",
        "#@markdown **Text Post-processing (OpenCC):** Convert the transcribed text (SRT/TXT output) between Simplified and Traditional Chinese variants.\n",
        "text_postprocess = \"臺灣繁體中文（預設）\"  #@param [\"臺灣繁體中文（預設）\",\"香港繁體中文\",\"大陸簡體中文\",\"關閉\"]\n",
        "#@markdown **YouTube Cookies (Optional):** Path to a Netscape-format cookies file (relative to MyDrive) for accessing age-restricted or member-only YouTube videos (e.g., `cookies/youtube.txt`).\n",
        "youtube_cookies_txt_path = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ## Summarization Settings\n",
        "#@markdown **Topic Hint (Optional):** Provide a brief hint about the topic to guide the summarization process.\n",
        "topic_hint = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "language_code_map = {\"自動偵測\": None, \"中文\":\"zh\", \"英文\":\"en\"}\n",
        "language_code = language_code_map[language]\n",
        "\n",
        "# =========================================================\n",
        "# Developer Options\n",
        "# Advanced users can fine-tune parameters in this section.\n",
        "# Modify only if you understand the impact.\n",
        "# =========================================================\n",
        "DEBUG_MODE = False # Set to True for more detailed logging\n",
        "\n",
        "# --- Transcription Parameters ---\n",
        "TRANSCRIPTION_BEAM_SIZE_PRIMARY = 1\n",
        "TRANSCRIPTION_CHUNK_LENGTH_PRIMARY = 20\n",
        "TRANSCRIPTION_BEAM_SIZE_FALLBACK = 1 # Used if primary fails\n",
        "TRANSCRIPTION_CHUNK_LENGTH_FALLBACK = 15 # Used if primary fails\n",
        "\n",
        "# --- Denoising Parameters ---\n",
        "DENOISE_NOISE_FLOOR_DB = -25\n",
        "\n",
        "# --- Filtering Parameters ---\n",
        "FILTER_MIN_DURATION_SHORT = 1.5 # Minimum duration for short segments\n",
        "FILTER_AVG_LOGPROB_THRESHOLD = -0.5 # Avg log probability threshold for short segments\n",
        "FILTER_MIN_DURATION_SPEECH_PROB = 2.0 # Minimum duration for speech probability filtering\n",
        "FILTER_NO_SPEECH_PROB_THRESHOLD = 0.6 # No speech probability threshold\n",
        "\n",
        "# --- Summary Model Parameters ---\n",
        "REPO_ID   = \"unsloth/gpt-oss-20b-GGUF\"   # GGUF Model Repository\n",
        "GGUF_FILE = \"gpt-oss-20b-Q4_K_M.gguf\"    # Approx. 10.8GiB, T4 can run\n",
        "\n",
        "# --- Summary Inference Parameters (Increase available generation space to avoid truncation) ---\n",
        "CTX_WINDOW_CANDIDATES   = [12288, 16384, 8192]  # T4/Q4_K_M usually handles 12k–16k; fallback to 8192\n",
        "ctx_window              = CTX_WINDOW_CANDIDATES[-1]  # Runtime picks the first successful candidate\n",
        "map_max_new_tokens      = 800   # Segment output upper bound (~550-800 chars)\n",
        "map_repeat_penalty      = 1.10  # Tunable repeat penalty for map stage\n",
        "reduce_repeat_penalty   = 1.10  # Tunable repeat penalty for reduce stage\n",
        "reduce_max_new_tokens   = 1500  # Summary output upper bound (~1k-1.3k chars)\n",
        "temperature             = 0.5\n",
        "top_p                   = 0.9\n",
        "repeat_penalty          = 1.05\n",
        "\n",
        "\n",
        "# --- Summary Segmentation Heuristics ---\n",
        "SEMANTIC_VAD_PRESETS = {\n",
        "    \"Conservative\": 1.2,  # 1.2s: keep more context for safety (保守)\n",
        "    \"Aggressive\":   0.8,  # 0.8s: quicker resets to dodge loop traps (積極)\n",
        "}\n",
        "SELECTED_VAD_SILENCE_PRESET = \"Aggressive\"  # Default preset tuned for repetitive loop mitigation\n",
        "SEMANTIC_PAUSE_THRESHOLD = SEMANTIC_VAD_PRESETS.get(SELECTED_VAD_SILENCE_PRESET, 0.8)\n",
        "SEMANTIC_MIN_TOKENS      = 192   # Minimum tokens before we allow punctuation-based splits (tighter for Chinese)\n",
        "SEMANTIC_MAX_CHARS       = 1800  # Safety valve to avoid overly long segments with no punctuation\n",
        "SLIDING_OVERLAP_TOKENS   = 200   # Tokens preserved between neighbouring summary windows\n",
        "SEMANTIC_FORCE_FLUSH_LINES = 40   # Force flush after 40 lines without punctuation\n",
        "SEMANTIC_FORCE_FLUSH_SECONDS = 120.0  # Force flush if segment spans ≥120s\n",
        "\n",
        "\n",
        "tokenizer_config_data = None\n",
        "harmony_chat_formatter = None\n",
        "\n",
        "def ensure_harmony_formatter():\n",
        "    \"\"\"Ensure the Harmony chat formatter is available for GPT-OSS prompts.\"\"\"\n",
        "    global harmony_chat_formatter\n",
        "    if harmony_chat_formatter is not None:\n",
        "        return harmony_chat_formatter\n",
        "    try:\n",
        "        from llama_cpp.llama_chat_format import (\n",
        "            hf_tokenizer_config_to_chat_formatter,\n",
        "            Jinja2ChatFormatter,\n",
        "        )\n",
        "    except Exception as exc:\n",
        "        raise RuntimeError(\"llama_cpp Harmony chat helpers are unavailable\") from exc\n",
        "\n",
        "    formatter = None\n",
        "    if tokenizer_config_data:\n",
        "        try:\n",
        "            formatter = hf_tokenizer_config_to_chat_formatter(\n",
        "                tokenizer_config_data,\n",
        "                add_generation_prompt=True,\n",
        "            )\n",
        "        except Exception as exc:\n",
        "            if DEBUG_MODE:\n",
        "                print(\"  ✗ Failed to initialize Harmony formatter from tokenizer_config.json:\", exc)\n",
        "\n",
        "    if formatter is None:\n",
        "        template = None\n",
        "        if 'llm' in globals() and hasattr(llm, 'metadata'):\n",
        "            template = llm.metadata.get('tokenizer.chat_template')\n",
        "        if template:\n",
        "            try:\n",
        "                bos_token = (\n",
        "                    llm.metadata.get('tokenizer.bos_token')\n",
        "                    or llm.metadata.get('tokenizer.ggml.bos_token')\n",
        "                    or llm.detokenize([llm.token_bos()], special=True).decode('utf-8', errors='ignore')\n",
        "                )\n",
        "                eos_token = (\n",
        "                    llm.metadata.get('tokenizer.eos_token')\n",
        "                    or llm.metadata.get('tokenizer.ggml.eos_token')\n",
        "                    or llm.detokenize([llm.token_eos()], special=True).decode('utf-8', errors='ignore')\n",
        "                )\n",
        "                formatter = Jinja2ChatFormatter(\n",
        "                    template,\n",
        "                    eos_token=eos_token,\n",
        "                    bos_token=bos_token,\n",
        "                    stop_token_ids=[llm.token_eos()],\n",
        "                )\n",
        "            except Exception as exc:\n",
        "                if DEBUG_MODE:\n",
        "                    print(\"  ✗ Failed to build Harmony formatter from GGUF metadata:\", exc)\n",
        "\n",
        "    if formatter is None:\n",
        "        raise RuntimeError(\"Harmony chat formatter could not be prepared\")\n",
        "\n",
        "    harmony_chat_formatter = formatter\n",
        "    return harmony_chat_formatter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# End of Developer Options\n",
        "# =========================================================\n",
        "\n",
        "\n",
        "# [3/8] Install Dependencies\n",
        "# Combine installation steps from both original cells\n",
        "if DEBUG_MODE: print(\"[Install] faster-whisper / yt-dlp / soundfile / opencc / srt / huggingface_hub / llama-cpp-python ...\")\n",
        "\n",
        "def pip_install(pkgs, extra_args=None, env=None):\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\"]\n",
        "    if extra_args:\n",
        "        cmd += extra_args\n",
        "    cmd += pkgs\n",
        "    return sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True, env=env)\n",
        "\n",
        "import importlib\n",
        "import importlib.util\n",
        "# Install common dependencies first\n",
        "common_missing = []\n",
        "if importlib.util.find_spec(\"srt\") is None:\n",
        "    common_missing.append(\"srt>=3.5.3\")\n",
        "if importlib.util.find_spec(\"huggingface_hub\") is None:\n",
        "    common_missing.append(\"huggingface_hub>=0.23.0\")\n",
        "if importlib.util.find_spec(\"soundfile\") is None:\n",
        "    common_missing.append(\"soundfile\")\n",
        "if importlib.util.find_spec(\"opencc\") is None:\n",
        "    common_missing.append(\"opencc-python-reimplemented\")\n",
        "if importlib.util.find_spec(\"jinja2\") is None:\n",
        "    common_missing.append(\"jinja2>=3.1.0\")\n",
        "\n",
        "if common_missing:\n",
        "    if DEBUG_MODE: print(\"→ Installing common missing packages:\", \", \".join(common_missing))\n",
        "    r = pip_install(common_missing)\n",
        "    if r.returncode != 0:\n",
        "        if DEBUG_MODE: print(r.stdout)\n",
        "        raise RuntimeError(\"基礎依賴安裝失敗，請重啟執行階段後重試。\")\n",
        "\n",
        "# Install faster-whisper and yt-dlp separately as they were in the first cell\n",
        "if importlib.util.find_spec(\"faster_whisper\") is None:\n",
        "    if DEBUG_MODE: print(\"→ Installing missing package: faster-whisper yt-dlp\")\n",
        "    r = pip_install([\"faster-whisper\", \"yt-dlp\"])\n",
        "    if r.returncode != 0:\n",
        "        if DEBUG_MODE: print(r.stdout)\n",
        "        raise RuntimeError(\"faster-whisper / yt-dlp 安裝失敗，請重啟執行階段後重試。\")\n",
        "\n",
        "# Import external packages after ensuring installation\n",
        "import soundfile as sf\n",
        "from faster_whisper import WhisperModel\n",
        "from opencc import OpenCC\n",
        "import srt as _srt  # Import srt as _srt to avoid name conflict later with the module itself\n",
        "from huggingface_hub import snapshot_download\n",
        "def suggest_runtime_reset():\n",
        "    print(\"\\n🧹 建議動作（Colab）\")\n",
        "    print(\"1) 依序：『執行階段 Runtime』 → 『刪除執行階段/還原出廠設定 Factory reset runtime』\\n2) 重新執行本 Notebook（從掛載雲端硬碟那格開始）\", flush=True)\n",
        "\n",
        "def run_cmd(cmd:list, check=True):\n",
        "    if DEBUG_MODE: print(\"  $\", \" \".join(cmd))\n",
        "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
        "    if DEBUG_MODE and p.stdout: sys.stdout.write(p.stdout)\n",
        "    if check and p.returncode != 0:\n",
        "        raise RuntimeError(f\"命令失敗：{' '.join(cmd)}\")\n",
        "    return p\n",
        "\n",
        "def is_youtube_url(s:str)->bool:\n",
        "    return isinstance(s, str) and (\"youtu.be\" in s or \"youtube.com\" in s)\n",
        "def is_http_url(s:str)->bool:\n",
        "    return isinstance(s, str) and s.lower().startswith(\"http\")\n",
        "def to_abs_mydrive(p:str)->Path:\n",
        "    return (Path(p) if p.startswith(\"/\") else (ROOT / p)).resolve()\n",
        "def fmt_ts_srt(t:float)->str:\n",
        "    h = int(t//3600); m = int((t%3600)//60); s = t - h*3600 - m*60\n",
        "    return f\"{h:02d}:{m:02d}:{int(s):02d},{int(round((s-int(s))*1000)):03d}\"\n",
        "def verify_wav_ok(path: Path)->bool:\n",
        "    try:\n",
        "        info = sf.info(str(path))\n",
        "        return info.samplerate > 0 and info.channels in (1, 2)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# OpenCC converter setup\n",
        "def build_opencc_pipeline(choice:str):\n",
        "    if choice.startswith(\"臺灣\"):\n",
        "        return [OpenCC('s2t'), OpenCC('t2tw')]\n",
        "    if choice.startswith(\"香港\"):\n",
        "        return [OpenCC('s2t'), OpenCC('t2hk')]\n",
        "    if choice.startswith(\"大陸\"):\n",
        "        return [OpenCC('t2s')]\n",
        "    return []  # Disable\n",
        "\n",
        "def apply_opencc(text:str, pipeline)->str:\n",
        "    for cc in pipeline:\n",
        "        text = cc.convert(text)\n",
        "    return text\n",
        "\n",
        "def ytdl(yturl:str)->Path:\n",
        "    tmp = Path(\"/tmp/dl\"); tmp.mkdir(parents=True, exist_ok=True)\n",
        "    for x in tmp.glob(\"*\"):\n",
        "        try: x.unlink()\n",
        "        except: shutil.rmtree(x, ignore_errors=True)\n",
        "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    out = tmp / f\"downloaded_{ts}.mp4\"\n",
        "    if DEBUG_MODE: print(\"[Download] Getting YouTube video ...\")\n",
        "    # Use sp.run instead of subprocess.run directly\n",
        "    cmd = [\"yt-dlp\", \"-f\", \"mp4\", \"-o\", str(tmp / \"%(title)s.%(ext)s\")]\n",
        "    if youtube_cookies_txt_path.strip():\n",
        "        cookies_abs = to_abs_mydrive(youtube_cookies_txt_path.strip())\n",
        "        if cookies_abs.exists():\n",
        "            cmd += [\"--cookies\", str(cookies_abs)]\n",
        "        else:\n",
        "            if DEBUG_MODE: print(f\"⚠️ 找不到 cookies 檔：{cookies_abs}（改為不帶 cookies）\")\n",
        "    cmd.append(yturl)\n",
        "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
        "    if DEBUG_MODE and p.stdout: sys.stdout.write(p.stdout)\n",
        "    if p.returncode != 0:\n",
        "        if \"Sign in to confirm\" in (p.stdout or \"\"):\n",
        "            print()\n",
        "            print(\"❗YouTube 要求登入/驗證，請提供 cookies 或先自行下載到雲端硬碟。\")\n",
        "        print(\"🔄 若多次失敗，請刪除執行階段並重啟後重試。\")\n",
        "        suggest_runtime_reset()\n",
        "        raise RuntimeError(\"yt-dlp 下載失敗\")\n",
        "    files = list(tmp.glob(\"*\"))\n",
        "    if not files:\n",
        "        print(\"🔄 下載為空，建議刪除執行階段再重試。\")\n",
        "        suggest_runtime_reset()\n",
        "        raise FileNotFoundError(\"YouTube 下載失敗：/tmp/dl 為空\")\n",
        "    f = files[0]\n",
        "    if save_video_to_google_drive:\n",
        "        shutil.copy2(f, WHISPER_DIR / f.name)\n",
        "    return f\n",
        "\n",
        "def http_dl(url:str)->Path:\n",
        "    tmp = Path(\"/tmp/dl\"); tmp.mkdir(parents=True, exist_ok=True)\n",
        "    for x in tmp.glob(\"*\"):\n",
        "        try: x.unlink()\n",
        "        except: shutil.rmtree(x, ignore_errors=True)\n",
        "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    out = tmp / f\"downloaded_{ts}.mp4\"\n",
        "    if DEBUG_MODE: print(\"[Download] Getting HTTP(S) video ...\")\n",
        "    run_cmd([\"curl\", \"-L\", \"-o\", str(out), url])\n",
        "    if save_video_to_google_drive:\n",
        "        shutil.copy2(out, WHISPER_DIR / out.name)\n",
        "    return out\n",
        "\n",
        "# Extract audio: ffmpeg -> 16k/mono WAV\n",
        "def ffmpeg_extract_wav(in_path:Path, out_wav:Path, sr=16000):\n",
        "    cmd = [\"ffmpeg\",\"-y\",\"-i\",str(in_path),\"-vn\",\"-ac\",\"1\",\"-ar\",str(sr),\"-f\",\"wav\",str(out_wav)]\n",
        "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
        "    if p.returncode != 0:\n",
        "        if DEBUG_MODE: print(p.stdout)\n",
        "        raise RuntimeError(\"ffmpeg 轉 WAV 失敗\")\n",
        "\n",
        "# CPU Denoising: ffmpeg afftdn\n",
        "def ffmpeg_afftdn(in_wav: Path, out_wav: Path, noise_floor_db=DENOISE_NOISE_FLOOR_DB):\n",
        "    cmd = [\"ffmpeg\",\"-y\",\"-i\",str(in_wav),\"-af\",f\"afftdn=nf={noise_floor_db}\",\n",
        "           \"-ac\",\"1\",\"-ar\",\"16000\",\"-f\",\"wav\",str(out_wav)]\n",
        "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
        "    if p.returncode != 0:\n",
        "        if DEBUG_MODE: print(p.stdout)\n",
        "        raise RuntimeError(\"ffmpeg afftdn 失敗\")\n",
        "\n",
        "# Safeguard: Repack WAV header if format is strange\n",
        "def ffmpeg_repack_wav(in_wav: Path, out_wav: Path, sr=16000):\n",
        "    cmd = [\"ffmpeg\",\"-y\",\"-i\",str(in_wav),\"-acodec\",\"pcm_s16le\",\"-ac\",\"1\",\"-ar\",str(sr),str(out_wav)]\n",
        "    p = sp.run(cmd, stdout=sp.PIPE, stderr=sp.STDOUT, text=True)\n",
        "    if p.returncode != 0:\n",
        "        if DEBUG_MODE: print(p.stdout)\n",
        "        raise RuntimeError(\"ffmpeg 重包 WAV 失敗\")\n",
        "\n",
        "# [4/8] Parse Source (Transcription) - Uses 'filename' and 'save_video_to_google_drive'\n",
        "if DEBUG_MODE: print(\"[4/8] Parsing input source ...\")\n",
        "try:\n",
        "    if is_youtube_url(filename):\n",
        "        src_path = ytdl(filename); out_base_dir = WHISPER_DIR\n",
        "    elif is_http_url(filename):\n",
        "        src_path = http_dl(filename); out_base_dir = WHISPER_DIR\n",
        "    else:\n",
        "        src_path = to_abs_mydrive(filename)\n",
        "        if not src_path.exists(): raise FileNotFoundError(f\"找不到檔案：{src_path}\")\n",
        "        out_base_dir = src_path.parent\n",
        "except Exception as e:\n",
        "    print(f\"\\n⛔ 來源解析/下載失敗：{e}\")\n",
        "    print(\"🔄 請刪除執行階段並重新啟動後重跑。\"); suggest_runtime_reset(); raise\n",
        "\n",
        "print(f\"→ 來源檔：{src_path}\")\n",
        "print(f\"→ 輸出資料夾：{out_base_dir}\")\n",
        "\n",
        "# [5/8] Extract Audio & CPU Denoising (Transcription) - Uses 'denoise_method' and 'DENOISE_NOISE_FLOOR_DB'\n",
        "AUDIO_16K = Path(\"/tmp/audio_16k.wav\")\n",
        "if DEBUG_MODE: print(\"[5/8] Extracting audio (ffmpeg → 16k/mono WAV) ...\")\n",
        "ffmpeg_extract_wav(src_path, AUDIO_16K, sr=16000)\n",
        "\n",
        "if denoise_method.startswith(\"afftdn\"):\n",
        "    if DEBUG_MODE: print(\"[5.5/8] Denoising (ffmpeg afftdn, CPU) ...\")\n",
        "    DENOISED = Path(\"/tmp/audio_16k_denoised.wav\")\n",
        "    ffmpeg_afftdn(AUDIO_16K, DENOISED, noise_floor_db=DENOISE_NOISE_FLOOR_DB)\n",
        "    denoised_audio = DENOISED if verify_wav_ok(DENOISED) else AUDIO_16K\n",
        "else:\n",
        "    denoised_audio = AUDIO_16K\n",
        "\n",
        "if not verify_wav_ok(denoised_audio):\n",
        "    if DEBUG_MODE: print(\"  - 音訊格式異常；嘗試重包 WAV ...\")\n",
        "    FIXED = Path(\"/tmp/audio_16k_fixed.wav\")\n",
        "    ffmpeg_repack_wav(denoised_audio, FIXED, sr=16000)\n",
        "    denoised_audio = FIXED\n",
        "\n",
        "if DEBUG_MODE: print(f\"→ 最終輸入音訊：{denoised_audio}\")\n",
        "\n",
        "# [6/8] Load faster-whisper (GPU enforced) - Uses 'model_size'\n",
        "if DEBUG_MODE: print(\"[6/8] Loading faster-whisper model (GPU) ...\")\n",
        "device = \"cuda\"  # Enforce GPU\n",
        "model = None; last_err = None\n",
        "compute_type_candidates = [\"float16\", \"int8_float16\", \"int8\"]\n",
        "chosen_compute_type = None\n",
        "for ctype in compute_type_candidates:\n",
        "    try:\n",
        "        if DEBUG_MODE: print(f\"  - Trying compute_type={ctype}\")\n",
        "        model = WhisperModel(model_size, device=device, compute_type=ctype)\n",
        "        chosen_compute_type = ctype\n",
        "        if DEBUG_MODE: print(\"  - Model loaded successfully\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        last_err = e\n",
        "        if DEBUG_MODE: print(f\"  - Load failed: {e}\")\n",
        "if model is None:\n",
        "    print()\n",
        "    print(\"⛔ GPU 模型載入失敗。請確認『變更執行階段類型』選了 GPU（T4/A100），或刪除執行階段後重試。\")\n",
        "    suggest_runtime_reset()\n",
        "    raise RuntimeError(f\"無法載入模型：{last_err}\")\n",
        "print(f\"→ faster-whisper compute_type: {chosen_compute_type}\")\n",
        "\n",
        "gc.collect()  # Clean up before transcription (safety)\n",
        "\n",
        "# [7/8] Transcribe (GPU; real-time progress per segment) - Uses 'language_code', 'TRANSCRIPTION_BEAM_SIZE_PRIMARY', 'TRANSCRIPTION_CHUNK_LENGTH_PRIMARY', 'TRANSCRIPTION_BEAM_SIZE_FALLBACK', 'TRANSCRIPTION_CHUNK_LENGTH_FALLBACK'\n",
        "if DEBUG_MODE: print(f\"[7/8] Starting transcription (GPU: beam={TRANSCRIPTION_BEAM_SIZE_PRIMARY} / chunk={TRANSCRIPTION_CHUNK_LENGTH_PRIMARY}s / VAD+no-repeat) ...\")\n",
        "\n",
        "SELECTED_VAD_SILENCE_MS = int(max(0.2, SEMANTIC_PAUSE_THRESHOLD) * 1000)\n",
        "LOOP_SENTINEL_MIN_REPEAT = 5          # Trigger guard if the same line repeats >=5 times\n",
        "LOOP_SENTINEL_TOLERANCE = 0.35        # Allow ±350ms drift when checking 1s increments\n",
        "LOOP_ESCALATE_REPEAT_THRESHOLD = 8    # Require ≥8 repeats before escalating guardrails\n",
        "LOOP_ESCALATE_DURATION_SECONDS = 10.0 # Require loops to span ≥10s when escalating by duration\n",
        "LOOP_ESCALATE_EVENTS_PER_MIN = 2      # Escalate when ≥2 loop events occur within the same minute\n",
        "\n",
        "\n",
        "def transcribe_gpu(\n",
        "    _beam: int = TRANSCRIPTION_BEAM_SIZE_PRIMARY,\n",
        "    _chunk: int = TRANSCRIPTION_CHUNK_LENGTH_PRIMARY,\n",
        "    *,\n",
        "    cond_prev: bool = True,\n",
        "    compression_threshold: float = 1.35,\n",
        "    no_repeat_ngram: int = 2,\n",
        "):\n",
        "    return model.transcribe(\n",
        "        str(denoised_audio),\n",
        "        task=\"transcribe\",\n",
        "        language=language_code,\n",
        "        temperature=0.0,\n",
        "        condition_on_previous_text=cond_prev,\n",
        "        compression_ratio_threshold=compression_threshold,\n",
        "        log_prob_threshold=-0.5,\n",
        "        no_speech_threshold=0.6,\n",
        "        beam_size=_beam,\n",
        "        chunk_length=_chunk,\n",
        "        vad_filter=True,\n",
        "        vad_parameters={\"min_silence_duration_ms\": SELECTED_VAD_SILENCE_MS},\n",
        "        no_repeat_ngram_size=no_repeat_ngram,\n",
        "        word_timestamps=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def run_transcription_attempt(\n",
        "    label: str,\n",
        "    *,\n",
        "    cond_prev: bool,\n",
        "    compression_threshold: float,\n",
        "    no_repeat_ngram: int,\n",
        "):\n",
        "    try:\n",
        "        seg_iter, info = transcribe_gpu(\n",
        "            cond_prev=cond_prev,\n",
        "            compression_threshold=compression_threshold,\n",
        "            no_repeat_ngram=no_repeat_ngram,\n",
        "        )\n",
        "    except Exception as exc:\n",
        "        if DEBUG_MODE:\n",
        "            print(\n",
        "                f\"  - Transcription attempt \\'{label}\\' failed ({exc})\\n\"\n",
        "                f\"    → Retrying with fallback beam/chunk (beam={TRANSCRIPTION_BEAM_SIZE_FALLBACK}, \"\n",
        "                f\"chunk={TRANSCRIPTION_CHUNK_LENGTH_FALLBACK})\"\n",
        "            )\n",
        "        seg_iter, info = transcribe_gpu(\n",
        "            _beam=TRANSCRIPTION_BEAM_SIZE_FALLBACK,\n",
        "            _chunk=TRANSCRIPTION_CHUNK_LENGTH_FALLBACK,\n",
        "            cond_prev=cond_prev,\n",
        "            compression_threshold=compression_threshold,\n",
        "            no_repeat_ngram=no_repeat_ngram,\n",
        "        )\n",
        "\n",
        "    duration = float(getattr(info, \"duration\", 0.0) or 0.0)\n",
        "    if duration <= 0:\n",
        "        duration = 1.0\n",
        "\n",
        "    segments: List[Tuple[float, float, str]] = []\n",
        "    filtered: List[Tuple[float, float, str]] = []\n",
        "    last_text: Optional[str] = None\n",
        "    last_start: Optional[float] = None\n",
        "    last_end: Optional[float] = None\n",
        "    repeat_streak = 1\n",
        "    max_repeat_streak = 1\n",
        "    loop_anchor_start: Optional[float] = None\n",
        "    loop_events: List[dict] = []\n",
        "    raw_speech_seconds = 0.0\n",
        "    filtered_speech_seconds = 0.0\n",
        "\n",
        "    stats = {\n",
        "        \"duration\": duration,\n",
        "        \"speech_coverage\": 0.0,\n",
        "        \"raw_segment_seconds\": 0.0,\n",
        "        \"vad_removed_seconds\": 0.0,\n",
        "        \"post_filter_removed_seconds\": 0.0,\n",
        "        \"compression_hits\": 0,\n",
        "        \"max_repeat_streak\": 1,\n",
        "        \"loop_events\": [],\n",
        "        \"loop_escalate_reasons\": [],\n",
        "        \"loop_events_per_minute\": {},\n",
        "        \"language\": getattr(info, \"language\", \"未知\"),\n",
        "        \"language_probability\": float(getattr(info, \"language_probability\", 0.0) or 0.0),\n",
        "        \"condition_on_previous_text\": cond_prev,\n",
        "        \"compression_threshold\": compression_threshold,\n",
        "        \"no_repeat_ngram_size\": no_repeat_ngram,\n",
        "    }\n",
        "\n",
        "    for seg in seg_iter:\n",
        "        text = seg.text.strip()\n",
        "        pct = int(min(100, round((float(seg.end) / duration) * 100)))\n",
        "        print(f\"[{pct:3d}%] {fmt_ts_srt(seg.start)} → {fmt_ts_srt(seg.end)}  {text}\", flush=True)\n",
        "\n",
        "        seg_tuple = (float(seg.start), float(seg.end), text)\n",
        "        segments.append(seg_tuple)\n",
        "\n",
        "        keep = True\n",
        "        seg_dur = max(0.0, float(seg_tuple[1] - seg_tuple[0]))\n",
        "        raw_speech_seconds += seg_dur\n",
        "        if (\n",
        "            seg_dur < FILTER_MIN_DURATION_SHORT\n",
        "            and getattr(seg, \"avg_logprob\", None) is not None\n",
        "            and seg.avg_logprob < FILTER_AVG_LOGPROB_THRESHOLD\n",
        "        ):\n",
        "            keep = False\n",
        "        if (\n",
        "            seg_dur < FILTER_MIN_DURATION_SPEECH_PROB\n",
        "            and getattr(seg, \"no_speech_prob\", None) is not None\n",
        "            and seg.no_speech_prob > FILTER_NO_SPEECH_PROB_THRESHOLD\n",
        "        ):\n",
        "            keep = False\n",
        "        if keep:\n",
        "            filtered.append(seg_tuple)\n",
        "            filtered_speech_seconds += seg_dur\n",
        "\n",
        "        compression_ratio = getattr(seg, \"compression_ratio\", None)\n",
        "        if compression_ratio is not None and compression_ratio >= compression_threshold:\n",
        "            stats[\"compression_hits\"] += 1\n",
        "\n",
        "        if last_text is not None and text == last_text:\n",
        "            gap = float(seg_tuple[0] - (last_start or seg_tuple[0]))\n",
        "            if abs(gap - 1.0) <= LOOP_SENTINEL_TOLERANCE:\n",
        "                repeat_streak += 1\n",
        "                if loop_anchor_start is None:\n",
        "                    loop_anchor_start = last_start\n",
        "                if repeat_streak == LOOP_SENTINEL_MIN_REPEAT:\n",
        "                    event_start = (\n",
        "                        loop_anchor_start\n",
        "                        if loop_anchor_start is not None\n",
        "                        else (last_start if last_start is not None else seg_tuple[0])\n",
        "                    )\n",
        "                    event = {\n",
        "                        \"label\": label,\n",
        "                        \"text\": text,\n",
        "                        \"start\": event_start,\n",
        "                        \"end\": seg_tuple[1],\n",
        "                        \"count\": repeat_streak,\n",
        "                    }\n",
        "                    loop_events.append(event)\n",
        "                    print(\n",
        "                        f\"⚠️ De-loop sentinel triggered ({label}): {fmt_ts_srt(event_start)} → {fmt_ts_srt(seg_tuple[1])} | repeats={repeat_streak}\"\n",
        "                    )\n",
        "                elif loop_events:\n",
        "                    loop_events[-1][\"end\"] = seg_tuple[1]\n",
        "                    loop_events[-1][\"count\"] = repeat_streak\n",
        "            else:\n",
        "                if repeat_streak >= LOOP_SENTINEL_MIN_REPEAT and loop_events:\n",
        "                    loop_events[-1][\"end\"] = last_end if last_end is not None else seg_tuple[1]\n",
        "                    loop_events[-1][\"count\"] = repeat_streak\n",
        "                repeat_streak = 1\n",
        "                loop_anchor_start = None\n",
        "        else:\n",
        "            if repeat_streak >= LOOP_SENTINEL_MIN_REPEAT and loop_events:\n",
        "                loop_events[-1][\"end\"] = last_end if last_end is not None else seg_tuple[1]\n",
        "                loop_events[-1][\"count\"] = repeat_streak\n",
        "            repeat_streak = 1\n",
        "            loop_anchor_start = None\n",
        "\n",
        "        max_repeat_streak = max(max_repeat_streak, repeat_streak)\n",
        "        last_text = text\n",
        "        last_start = seg_tuple[0]\n",
        "        last_end = seg_tuple[1]\n",
        "\n",
        "    if repeat_streak >= LOOP_SENTINEL_MIN_REPEAT and loop_events:\n",
        "        loop_events[-1][\"end\"] = last_end if last_end is not None else loop_events[-1][\"end\"]\n",
        "        loop_events[-1][\"count\"] = repeat_streak\n",
        "\n",
        "    stats[\"raw_segment_seconds\"] = max(0.0, raw_speech_seconds)\n",
        "    stats[\"speech_coverage\"] = max(0.0, filtered_speech_seconds)\n",
        "    stats[\"vad_removed_seconds\"] = max(0.0, duration - raw_speech_seconds)\n",
        "    stats[\"post_filter_removed_seconds\"] = max(0.0, raw_speech_seconds - filtered_speech_seconds)\n",
        "    stats[\"max_repeat_streak\"] = max(max_repeat_streak, repeat_streak)\n",
        "\n",
        "    minute_counts = defaultdict(int)\n",
        "    escalate_reasons = []\n",
        "    for event in loop_events:\n",
        "        start_val = float(event.get(\"start\", 0.0) or 0.0)\n",
        "        end_val = float(event.get(\"end\", start_val) or start_val)\n",
        "        duration_val = max(0.0, end_val - start_val)\n",
        "        event[\"duration\"] = duration_val\n",
        "        bucket = int(max(0.0, start_val) // 60)\n",
        "        minute_counts[bucket] += 1\n",
        "        if (\n",
        "            event.get(\"count\", 0) >= LOOP_ESCALATE_REPEAT_THRESHOLD\n",
        "            and duration_val >= LOOP_ESCALATE_DURATION_SECONDS\n",
        "        ):\n",
        "            escalate_reasons.append(\n",
        "                f\"repeat_streak={event.get('count', 0)} lasting {duration_val:.1f}s near {fmt_ts_srt(start_val)}\"\n",
        "            )\n",
        "\n",
        "    for minute, count in sorted(minute_counts.items()):\n",
        "        if count >= LOOP_ESCALATE_EVENTS_PER_MIN:\n",
        "            window_start = minute * 60\n",
        "            window_end = window_start + 60\n",
        "            escalate_reasons.append(\n",
        "                f\"{count} loop events within {fmt_ts_srt(window_start)}–{fmt_ts_srt(window_end)}\"\n",
        "            )\n",
        "\n",
        "    stats[\"loop_events\"] = loop_events\n",
        "    stats[\"loop_escalate_reasons\"] = escalate_reasons\n",
        "    stats[\"loop_events_per_minute\"] = dict(minute_counts)\n",
        "\n",
        "    return {\n",
        "        \"segments\": segments,\n",
        "        \"filtered\": filtered,\n",
        "        \"info\": info,\n",
        "        \"stats\": stats,\n",
        "    }\n",
        "\n",
        "\n",
        "transcription_attempts = [\n",
        "\n",
        "    (\"Baseline\", dict(cond_prev=True,  compression_threshold=1.35, no_repeat_ngram=2)),\n",
        "    (\"Reset\",    dict(cond_prev=False, compression_threshold=1.35, no_repeat_ngram=2)),\n",
        "    (\"Reinforce\",dict(cond_prev=False, compression_threshold=1.30, no_repeat_ngram=3)),\n",
        "]\n",
        "\n",
        "segments: List[Tuple[float, float, str]] = []\n",
        "filtered: List[Tuple[float, float, str]] = []\n",
        "info = None\n",
        "transcription_stats = {}\n",
        "\n",
        "for idx, (label, params) in enumerate(transcription_attempts, 1):\n",
        "    if idx > 1:\n",
        "        print(\n",
        "            f\"→ De-loop retry {idx}/{len(transcription_attempts)}: {label} \"\n",
        "            f\"(condition_on_previous_text={params['cond_prev']}, no_repeat_ngram_size={params['no_repeat_ngram']}, \"\n",
        "            f\"compression_ratio_threshold={params['compression_threshold']})\"\n",
        "        )\n",
        "    attempt = run_transcription_attempt(label, **params)\n",
        "    segments = attempt[\"segments\"]\n",
        "    filtered = attempt[\"filtered\"]\n",
        "    info = attempt[\"info\"]\n",
        "    transcription_stats = attempt[\"stats\"]\n",
        "    loop_events = transcription_stats.get(\"loop_events\", [])\n",
        "    escalate_reasons = transcription_stats.get(\"loop_escalate_reasons\", [])\n",
        "    if not loop_events:\n",
        "        break\n",
        "    if not escalate_reasons:\n",
        "        print(\"  ↳ De-loop sentinel noted repeats but below escalation thresholds; keeping current parameters.\")\n",
        "        break\n",
        "    if idx < len(transcription_attempts):\n",
        "        print(\"  ↳ De-loop sentinel escalation triggered due to:\")\n",
        "        for reason in escalate_reasons:\n",
        "            print(f\"     • {reason}\")\n",
        "        print(\"    → Retrying with tighter decoding guardrails ...\")\n",
        "    else:\n",
        "        print(\"  ↳ Warning: loop persisted despite all guardrails.\")\n",
        "\n",
        "if DEBUG_MODE:\n",
        "    print(\n",
        "        f\"  - Detected language: {transcription_stats.get('language', '未知')} \"\n",
        "        f\"(p={transcription_stats.get('language_probability', 0.0):.2f})\"\n",
        "    )\n",
        "    print(f\"  - Audio length: {transcription_stats.get('duration', 0.0):.2f}s\")\n",
        "\n",
        "\n",
        "# ---- OpenCC Normalization (for output text) ---- - Uses 'text_postprocess'\n",
        "pipeline = build_opencc_pipeline(text_postprocess)\n",
        "def norm(txt: str) -> str:\n",
        "    return apply_opencc(txt, pipeline) if pipeline else txt\n",
        "\n",
        "# [8/8] Output (text after OpenCC) - Uses 'out_base_dir' (derived from 'filename')\n",
        "print(\"[8/8] 輸出 SRT / TXT ...\")\n",
        "# Determine the output directory for transcription based on input type\n",
        "# If input is a network source, output to WHISPER_DIR\n",
        "# If input is a local file, output to the same directory as the input file\n",
        "if is_youtube_url(filename) or is_http_url(filename):\n",
        "    out_base_dir = WHISPER_DIR\n",
        "else:\n",
        "    src_path_abs = to_abs_mydrive(filename)\n",
        "    out_base_dir = src_path_abs.parent\n",
        "\n",
        "# Create the transcription output directory if it doesn't exist\n",
        "out_dir = out_base_dir\n",
        "out_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Determine the stem from the original source file path\n",
        "stem = Path(src_path).stem\n",
        "SRT = out_dir / f\"{stem}.srt\"\n",
        "TXT = out_dir / f\"{stem}.txt\"\n",
        "\n",
        "with open(SRT, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, (seg_start, seg_end, seg_text) in enumerate(filtered, 1):\n",
        "        text_out = norm(seg_text.strip())\n",
        "        f.write(f\"{i}\\n{fmt_ts_srt(seg_start)} --> {fmt_ts_srt(seg_end)}\\n{text_out}\\n\\n\")\n",
        "\n",
        "with open(TXT, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, _, seg_text in filtered:\n",
        "        f.write(norm(seg_text.strip()) + \"\\n\")\n",
        "\n",
        "print(f\"→ 完成！\\n  SRT: {SRT}\\n  TXT: {TXT}\")\n",
        "print(\"[Transcription Metrics]\")\n",
        "print(f\"→ faster-whisper compute_type: {chosen_compute_type}\")\n",
        "print(f\"→ VAD preset: {SELECTED_VAD_SILENCE_PRESET} (silence ≥ {SEMANTIC_PAUSE_THRESHOLD:.2f}s → {SELECTED_VAD_SILENCE_MS} ms)\")\n",
        "print(\n",
        "    f\"→ Language: {transcription_stats.get('language', '未知')} \"\n",
        "    f\"(p={transcription_stats.get('language_probability', 0.0):.2f})\"\n",
        ")\n",
        "print(\n",
        "    f\"→ Audio length: {transcription_stats.get('duration', 0.0):.2f}s；\"\n",
        "    f\"Raw speech: {transcription_stats.get('raw_segment_seconds', 0.0):.2f}s；\"\n",
        "    f\"VAD removed: {transcription_stats.get('vad_removed_seconds', 0.0):.2f}s；\"\n",
        "    f\"Post-filter removed: {transcription_stats.get('post_filter_removed_seconds', 0.0):.2f}s；\"\n",
        "    f\"Speech kept: {transcription_stats.get('speech_coverage', 0.0):.2f}s\"\n",
        ")\n",
        "print(f\"→ compression_ratio_threshold hits: {transcription_stats.get('compression_hits', 0)} segments\")\n",
        "print(f\"→ Max consecutive repeat streak: {transcription_stats.get('max_repeat_streak', 1)}\")\n",
        "if transcription_stats.get('loop_events'):\n",
        "    for idx_evt, evt in enumerate(transcription_stats.get('loop_events', []), 1):\n",
        "        start_ts = fmt_ts_srt(evt.get('start', 0.0))\n",
        "        end_ts = fmt_ts_srt(evt.get('end', evt.get('start', 0.0)))\n",
        "        duration_val = evt.get('duration', max(0.0, (evt.get('end', 0.0) or 0.0) - (evt.get('start', 0.0) or 0.0)))\n",
        "        print(\n",
        "            f\"   ⚠️ Event {idx_evt}: {start_ts} → {end_ts} \"\n",
        "            f\"(duration≈{duration_val:.1f}s, repeats={evt.get('count', LOOP_SENTINEL_MIN_REPEAT)}, tag={evt.get('label')})\"\n",
        "        )\n",
        "    if transcription_stats.get('loop_escalate_reasons'):\n",
        "        for reason in transcription_stats.get('loop_escalate_reasons', []):\n",
        "            print(f\"   ↳ escalation note: {reason}\")\n",
        "else:\n",
        "    print(\"→ No de-loop sentinel events detected.\")\n",
        "\n",
        "# Release model (release GPU memory)\n",
        "try: del model\n",
        "except: pass\n",
        "gc.collect()\n",
        "if DEBUG_MODE: print(\"→ Model released; can run again directly if needed.\")\n",
        "\n",
        "\n",
        "# ===== Summarization Logic Starts Here =====\n",
        "# Use SRT from transcription step for summarization\n",
        "summary_srt_path_abs = SRT\n",
        "assert summary_srt_path_abs.exists(), f\"SRT 檔不存在：{summary_srt_path_abs}\"\n",
        "\n",
        "# ===== Summary 1/6) Check GPU and Install Dependencies (llama-cpp-python specific) =====\n",
        "# llama-cpp-python installation logic - Keep this separate as it has specific CUDA requirements\n",
        "# Moved this section to just before reading the SRT for summarization\n",
        "if DEBUG_MODE: print(\"[Summary 1/6] Checking GPU and installing llama-cpp-python ...\")\n",
        "\n",
        "def detect_cuda_tag():\n",
        "    try:\n",
        "        out = sp.check_output([\"nvidia-smi\"], text=True)\n",
        "        version_token = None\n",
        "        for line in out.splitlines():\n",
        "            if \"CUDA Version\" in line:\n",
        "                _, _, tail = line.partition(\"CUDA Version:\")\n",
        "                cleaned = ''.join(ch for ch in tail if ch.isdigit() or ch == '.')\n",
        "                if cleaned:\n",
        "                    version_token = cleaned\n",
        "                    break\n",
        "        if not version_token:\n",
        "            return \"cu124\"\n",
        "        parts = version_token.split(\".\")\n",
        "        major = int(parts[0]) if parts and parts[0].isdigit() else 0\n",
        "        minor = int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else 0\n",
        "        if major > 12 or (major == 12 and minor >= 5):\n",
        "            return \"cu125\"\n",
        "        if major == 12 and minor >= 0:\n",
        "            return \"cu124\"\n",
        "        if major == 11 and minor >= 8:\n",
        "            return \"cu118\"\n",
        "        return \"cu117\"\n",
        "    except Exception:\n",
        "        return \"cu124\"\n",
        "\n",
        "cuda_tag = detect_cuda_tag()\n",
        "if DEBUG_MODE: print(f\"GPU 0: Detected CUDA version tag {cuda_tag}\")\n",
        "\n",
        "def try_import_llama():\n",
        "    try:\n",
        "        from llama_cpp import Llama\n",
        "        return Llama\n",
        "    except ModuleNotFoundError:\n",
        "        return None\n",
        "\n",
        "Llama = try_import_llama()\n",
        "if Llama is None:\n",
        "    # Keep your existing installation strategy: extra-index -> fallback to source compilation on failure\n",
        "    candidates = [cuda_tag, \"cu125\", \"cu124\", \"cu122\", \"cu121\"]\n",
        "    ok = False\n",
        "    for tag in candidates:\n",
        "        idx = f\"https://abetlen.github.io/llama-cpp-python/whl/{tag}\"\n",
        "        if DEBUG_MODE: print(f\"→ Attempting to install llama-cpp-python ({tag}) ...\")\n",
        "        r = pip_install([\"llama-cpp-python\"], extra_args=[\"--extra-index-url\", idx])\n",
        "        if r.returncode == 0:\n",
        "            Llama = try_import_llama()\n",
        "            if Llama is not None:\n",
        "                ok = True\n",
        "                break\n",
        "        else:\n",
        "            if DEBUG_MODE: print(\"  ✗ Installation failed (summary):\", \"\\n\".join(r.stdout.splitlines()[-5:]))\n",
        "    if not ok:\n",
        "        if DEBUG_MODE: print(\"→ Pre-compiled wheels not available, switching to 'source compilation (CUDA=ON)' ... (takes longer)\")\n",
        "        try:\n",
        "            import ninja # noqa: F401 # Import ninja to check if installed\n",
        "        except ModuleNotFoundError:\n",
        "            if DEBUG_MODE: print(\"→ Installing missing package: ninja\")\n",
        "            r = pip_install([\"ninja\"])\n",
        "            if r.returncode != 0:\n",
        "                if DEBUG_MODE: print(r.stdout)\n",
        "                raise RuntimeError(\"安裝 ninja 失敗。請重啟後重試。\")\n",
        "        env = os.environ.copy()\n",
        "        env[\"CMAKE_ARGS\"] = \"-DGGML_CUDA=on -DLLAMA_CUBLAS=on\"\n",
        "        env[\"FORCE_CMAKE\"] = \"1\"\n",
        "        r = pip_install([\"llama-cpp-python\"], env=env)\n",
        "        if r.returncode != 0:\n",
        "            if DEBUG_MODE: print(r.stdout)\n",
        "            raise RuntimeError(\"無法安裝 GPU 版 llama-cpp-python。\")\n",
        "        Llama = try_import_llama()\n",
        "\n",
        "\n",
        "if DEBUG_MODE: print(\"[Summary 2/6] Reading SRT ...\")\n",
        "with open(summary_srt_path_abs, \"r\", encoding=\"utf-8\") as f:\n",
        "    srt_text = f.read()\n",
        "subs = list(_srt.parse(srt_text)) # Use _srt as srt module was imported as _srt\n",
        "def td2s(td): return td.total_seconds()\n",
        "segments = []\n",
        "for it in subs:\n",
        "    txt = it.content.strip()\n",
        "    if not txt:\n",
        "        continue\n",
        "    segments.append((td2s(it.start), td2s(it.end), txt))\n",
        "\n",
        "def compress_repetitive_segments(\n",
        "    segs: List[Tuple[float, float, str]],\n",
        "    *,\n",
        "    short_len: int = 2,\n",
        "    long_duration: float = 60.0,\n",
        "    long_count: int = 30,\n",
        ") -> Tuple[List[Tuple[float, float, str]], List[dict]]:\n",
        "    \"\"\"Collapse repetitive floods in subtitles while keeping context samples.\"\"\"\n",
        "    if not segs:\n",
        "        return [], []\n",
        "    compressed: List[Tuple[float, float, str]] = []\n",
        "    reports: List[dict] = []\n",
        "    i = 0\n",
        "    while i < len(segs):\n",
        "        s, e, text = segs[i]\n",
        "        stripped = text.strip()\n",
        "        run_start = i\n",
        "        run_end = i + 1\n",
        "        total_duration = max(0.0, e - s)\n",
        "        while run_end < len(segs) and segs[run_end][2].strip() == stripped:\n",
        "            total_duration += max(0.0, segs[run_end][1] - segs[run_end][0])\n",
        "            run_end += 1\n",
        "        run_len = run_end - run_start\n",
        "        if run_len > 1 and len(stripped) <= short_len:\n",
        "            marker = f\"[重複 x{run_len}: {stripped}]\"\n",
        "            start_ts = segs[run_start][0]\n",
        "            end_ts = segs[run_end - 1][1]\n",
        "            compressed.append((start_ts, end_ts, marker))\n",
        "            report = {\n",
        "                \"text\": stripped,\n",
        "                \"count\": run_len,\n",
        "                \"duration\": total_duration,\n",
        "                \"start\": start_ts,\n",
        "                \"end\": end_ts,\n",
        "                \"samples\": [],\n",
        "            }\n",
        "            if total_duration >= long_duration and run_len >= long_count:\n",
        "                stride = max(5, min(10, max(1, run_len // 10)))\n",
        "                sample_indices = list(range(run_start, run_end, stride))\n",
        "                # Always keep the final segment for context\n",
        "                if sample_indices[-1] != run_end - 1:\n",
        "                    sample_indices.append(run_end - 1)\n",
        "                for idx in sample_indices[:10]:\n",
        "                    sample_seg = segs[idx]\n",
        "                    sample_txt = sample_seg[2].strip()\n",
        "                    compressed.append((sample_seg[0], sample_seg[1], f\"[重複樣本] {sample_txt}\"))\n",
        "                    report[\"samples\"].append({\n",
        "                        \"index\": idx,\n",
        "                        \"start\": sample_seg[0],\n",
        "                        \"end\": sample_seg[1],\n",
        "                        \"text\": sample_txt,\n",
        "                    })\n",
        "            reports.append(report)\n",
        "            i = run_end\n",
        "            continue\n",
        "        compressed.append((s, e, text))\n",
        "        i += 1\n",
        "    return compressed, reports\n",
        "\n",
        "segments, repetition_reports = compress_repetitive_segments(segments)\n",
        "if repetition_reports:\n",
        "    print(f\"→ 偵測到 {len(repetition_reports)} 組重複洪水，已壓縮並保留樣本\")\n",
        "\n",
        "total_secs = (segments[-1][1] - segments[0][0]) if segments else 0\n",
        "if DEBUG_MODE: print(f\"→ Number of subtitle segments: {len(segments)}；Video length (est): {total_secs/60:.1f} minutes\")\n",
        "\n",
        "\n",
        "# ===== Summary 3/6) Download and Load GGUF Model (Summary) - Uses summary model parameters (REPO_ID, GGUF_FILE, ctx_window, etc.)\n",
        "# Moved this section to just after installing llama-cpp-python\n",
        "if DEBUG_MODE: print(\"[Summary 3/6] Loading GPT-OSS-20B (GGUF, CUDA) ...\")\n",
        "local_repo = snapshot_download(REPO_ID, allow_patterns=[GGUF_FILE, \"tokenizer_config.json\"])\n",
        "gguf_path = str(Path(local_repo)/GGUF_FILE)\n",
        "\n",
        "tokenizer_config_path = Path(local_repo)/\"tokenizer_config.json\"\n",
        "if tokenizer_config_path.exists():\n",
        "    try:\n",
        "        tokenizer_config_data = json.loads(tokenizer_config_path.read_text())\n",
        "        if DEBUG_MODE: print(\"→ Loaded tokenizer_config.json (Harmony template)\")\n",
        "    except Exception as exc:\n",
        "        if DEBUG_MODE: print(\"  ✗ Failed to parse tokenizer_config.json:\", exc)\n",
        "\n",
        "llm = None\n",
        "selected_ctx_window = None\n",
        "ctx_errors = []\n",
        "for ctx_candidate in CTX_WINDOW_CANDIDATES:\n",
        "    try:\n",
        "        if DEBUG_MODE:\n",
        "            print(f\"  - Trying ctx_window={ctx_candidate}\")\n",
        "        llm = Llama(\n",
        "            model_path=gguf_path,\n",
        "            n_ctx=ctx_candidate,\n",
        "            n_gpu_layers=-1,\n",
        "            seed=0,\n",
        "            logits_all=False,\n",
        "            verbose=True  # Display the actual chat format used\n",
        "        )\n",
        "        selected_ctx_window = ctx_candidate\n",
        "        break\n",
        "    except Exception as exc:\n",
        "        ctx_errors.append((ctx_candidate, exc))\n",
        "        if DEBUG_MODE:\n",
        "            print(f\"  ✗ Failed ctx_window={ctx_candidate}: {exc}\")\n",
        "        gc.collect()\n",
        "\n",
        "if llm is None:\n",
        "    msgs = \", \".join(f\"{cand}→{err}\" for cand, err in ctx_errors[-3:])\n",
        "    raise RuntimeError(f\"無法載入 GGUF（ctx candidates={CTX_WINDOW_CANDIDATES}）：{msgs}\")\n",
        "\n",
        "ctx_window = selected_ctx_window or ctx_window\n",
        "print(f\"→ Selected ctx_window: {ctx_window}\")\n",
        "if DEBUG_MODE:\n",
        "    print(\"→ Model loaded successfully (GPU)\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    ensure_harmony_formatter()\n",
        "    if DEBUG_MODE: print(\"→ Harmony formatter prepared\")\n",
        "except Exception as exc:\n",
        "    raise RuntimeError(f\"Failed to prepare Harmony formatter: {exc}\")\n",
        "\n",
        "\n",
        "# ===== Summary 4/6) Token-aware Segmentation (Summary) - Uses ctx_window, map_max_new_tokens, prompt_overhead\n",
        "if DEBUG_MODE: print(\"[Summary 4/6] Generating segments (token-aware; single segment ≤ safety limit) ...\")\n",
        "\n",
        "def count_tokens_text(text: str) -> int:\n",
        "    # Check if llm is initialized before using it\n",
        "    if 'llm' not in globals() or llm is None:\n",
        "         raise RuntimeError(\"LLM model is not loaded. Cannot count tokens.\")\n",
        "    return len(llm.tokenize(text.encode(\"utf-8\")))\n",
        "\n",
        "SYSTEM_INSTR = (\n",
        "  \"你是一個會議總結機器人。根據使用者提供的逐字稿（可能雜訊、重複、錯字），\"\n",
        "  \"請去除雜訊與重複、嚴守事實、不腦補。遇到不明確資訊以「待補充／未明確」標註。\"\n",
        "  \"輸出為 Markdown（繁體中文），不要輸出任何系統／思考標記。\"\n",
        ")\n",
        "\n",
        "# — Segment Summary Prompt: More concise request, avoid verbosity and system language - Uses 'topic_hint'\n",
        "MAP_USER_TMPL = textwrap.dedent(\"\"\"\\\n",
        "主題（可留空）：{topic}\n",
        "\n",
        "以下是逐字稿片段（非完整全文）：\n",
        "{chunk}\n",
        "\n",
        "請就此片段輸出「條列式重點摘要」（500–900 字，繁體中文），注意：\n",
        "- 只寫最終內容，不要寫解題想法、不要出現任何系統提示或中英括號標記。\n",
        "- 聚焦可驗證事實（時間、人物、任務、結論、未決事項、行動）。\n",
        "- 結構：可用小標題＋項目符號，語句務必短、準確、無贅詞。\n",
        "\"\"\")\n",
        "\n",
        "# — Summary Prompt: Maintain your three-section output structure - Uses 'topic_hint'\n",
        "REDUCE_USER_TMPL = textwrap.dedent(\"\"\"\\\n",
        "主題（可留空）：{topic}\n",
        "\n",
        "以下是所有片段的重點摘要彙整（仍可能有重疊）：\n",
        "{maps}\n",
        "\n",
        "請整合為一份會議筆記（Markdown，繁體）：\n",
        "1) **整體提要**（3–6 句，避免冗言）\n",
        "2) **章節要點（含時間脈絡）**：條列呈現，每點一行，可附粗略時間，**不得忽略任何片段**，每條尾端標註對應片段編號 (片段 i) 或時間。\n",
        "3) **可執行重點**：具體待辦（每條以動詞開頭）\n",
        "請確保所有片段至少納入 1–2 個重點，若資訊不足請註明「待補充」。\n",
        "請只輸出最終筆記，不要出現系統或思考標記，不要加入未出現的新資訊。\n",
        "\"\"\")\n",
        "\n",
        "# Single segment token budget (reserve space for prompt and generation)\n",
        "prompt_overhead = 700\n",
        "chunk_target    = max(1024, min(2048, ctx_window - prompt_overhead - map_max_new_tokens))\n",
        "SENTENCE_END_RE = re.compile(r\"[。！？?!…]+[\\\"”』】）]*$\")\n",
        "\n",
        "def build_semantic_segments(raw_segments: List[Tuple[float, float, str]]) -> List[Tuple[float, float, str, int]]:\n",
        "    semantic_segments: List[Tuple[float, float, str, int]] = []\n",
        "    buffer: List[str] = []\n",
        "    buffer_tokens = 0\n",
        "    buffer_chars = 0\n",
        "    buffer_line_count = 0\n",
        "    lines_since_punct = 0\n",
        "    start_time: Optional[float] = None\n",
        "    buffer_end: Optional[float] = None\n",
        "    last_end: Optional[float] = None\n",
        "\n",
        "    def flush_buffer():\n",
        "        nonlocal buffer, buffer_tokens, buffer_chars, buffer_line_count, lines_since_punct, start_time, buffer_end\n",
        "        if not buffer:\n",
        "            return\n",
        "        text = \"\\n\".join(buffer).strip()\n",
        "        if text:\n",
        "            semantic_segments.append((start_time or 0.0, buffer_end or start_time or 0.0, text, buffer_tokens))\n",
        "        buffer = []\n",
        "        buffer_tokens = 0\n",
        "        buffer_chars = 0\n",
        "        buffer_line_count = 0\n",
        "        lines_since_punct = 0\n",
        "        start_time = None\n",
        "        buffer_end = None\n",
        "\n",
        "    for s, e, txt in raw_segments:\n",
        "        txt = txt.strip()\n",
        "        if not txt:\n",
        "            last_end = e\n",
        "            continue\n",
        "\n",
        "        if buffer and last_end is not None:\n",
        "            gap = max(0.0, s - last_end)\n",
        "            if gap >= SEMANTIC_PAUSE_THRESHOLD:\n",
        "                flush_buffer()\n",
        "\n",
        "        if not buffer:\n",
        "            start_time = s\n",
        "\n",
        "        buffer.append(txt)\n",
        "        piece_tokens = count_tokens_text(txt)\n",
        "        buffer_tokens += piece_tokens\n",
        "        buffer_chars += len(txt)\n",
        "        buffer_line_count += 1\n",
        "        if SENTENCE_END_RE.search(txt):\n",
        "            lines_since_punct = 0\n",
        "        else:\n",
        "            lines_since_punct += 1\n",
        "        buffer_end = e\n",
        "        last_end = e\n",
        "\n",
        "        buffer_duration = 0.0\n",
        "        if start_time is not None and buffer_end is not None:\n",
        "            buffer_duration = max(0.0, buffer_end - start_time)\n",
        "\n",
        "        should_flush = False\n",
        "        if buffer_tokens >= chunk_target:\n",
        "            should_flush = True\n",
        "        elif buffer_tokens >= SEMANTIC_MIN_TOKENS and SENTENCE_END_RE.search(txt):\n",
        "            should_flush = True\n",
        "        elif buffer_chars >= SEMANTIC_MAX_CHARS and SENTENCE_END_RE.search(txt):\n",
        "            should_flush = True\n",
        "        elif lines_since_punct >= SEMANTIC_FORCE_FLUSH_LINES:\n",
        "            should_flush = True\n",
        "        elif buffer_duration >= SEMANTIC_FORCE_FLUSH_SECONDS:\n",
        "            should_flush = True\n",
        "\n",
        "        if should_flush:\n",
        "            flush_buffer()\n",
        "\n",
        "    flush_buffer()\n",
        "    return semantic_segments\n",
        "\n",
        "\n",
        "semantic_segments = build_semantic_segments(segments)\n",
        "\n",
        "chunks: List[Tuple[float, float, str]] = []\n",
        "i = 0\n",
        "while i < len(semantic_segments):\n",
        "    window: List[Tuple[float, float, str, int]] = []\n",
        "    total_tokens = 0\n",
        "    start_ts = semantic_segments[i][0]\n",
        "    end_ts = semantic_segments[i][1]\n",
        "    j = i\n",
        "    while j < len(semantic_segments):\n",
        "        seg = semantic_segments[j]\n",
        "        seg_tokens = seg[3]\n",
        "        # Always include at least one semantic block per chunk\n",
        "        if window and total_tokens + seg_tokens > chunk_target and SENTENCE_END_RE.search(window[-1][2]):\n",
        "            break\n",
        "        window.append(seg)\n",
        "        total_tokens += seg_tokens\n",
        "        end_ts = seg[1]\n",
        "        j += 1\n",
        "        if total_tokens >= chunk_target or (total_tokens >= SEMANTIC_MIN_TOKENS and SENTENCE_END_RE.search(seg[2])):\n",
        "            break\n",
        "\n",
        "    chunk_text = \"\".join(seg[2] for seg in window).strip()\n",
        "    if chunk_text:\n",
        "        chunks.append((start_ts, end_ts, chunk_text))\n",
        "\n",
        "    if j >= len(semantic_segments):\n",
        "        break\n",
        "\n",
        "    overlap_segments = 0\n",
        "    if len(window) > 1:\n",
        "        trailing_tokens = 0\n",
        "        for idx in range(len(window) - 1, -1, -1):\n",
        "            trailing_tokens += window[idx][3]\n",
        "            if trailing_tokens >= SLIDING_OVERLAP_TOKENS:\n",
        "                break\n",
        "            overlap_segments += 1\n",
        "        overlap_segments = min(overlap_segments, len(window) - 1)\n",
        "\n",
        "    next_i = j if overlap_segments == 0 else max(j - overlap_segments, i + 1)\n",
        "    i = max(next_i, i + 1)\n",
        "\n",
        "if DEBUG_MODE:\n",
        "    avg_tokens = (sum(seg[3] for seg in semantic_segments) / max(len(semantic_segments), 1)) if semantic_segments else 0\n",
        "    print(f\"→ Semantic segments: {len(semantic_segments)} (avg tokens ≈ {avg_tokens:.0f}); summary windows: {len(chunks)} (target ~{chunk_target} tokens)\")\n",
        "\n",
        "# ===== Common: Streaming Tools (No regex cleaning; use correct stop sequence) - Uses temperature, top_p, repeat_penalty, map_max_new_tokens, reduce_max_new_tokens\n",
        "\n",
        "\n",
        "STREAM_FALLBACK_USED = False\n",
        "ARTIFACT_TAG_RE = re.compile(r\"<\\|[^|]*\\|>\")\n",
        "def clean_harmony_artifacts(s: str) -> str:\n",
        "    return ARTIFACT_TAG_RE.sub(\"\", s or \"\")\n",
        "\n",
        "def stream_harmony_final_pieces(text_chunks: Iterable[str]) -> Iterator[str]:\n",
        "    \"\"\"Yield Harmony streamed text, preferring the final channel.\n",
        "\n",
        "    Some community models only emit the assistant channel; fall back to it\n",
        "    so we do not drop the actual content.\n",
        "    \"\"\"\n",
        "    global STREAM_FALLBACK_USED\n",
        "    buffer = \"\"\n",
        "    current_channel: Optional[str] = None\n",
        "    pending_channel = False\n",
        "    channel_name_buffer = \"\"\n",
        "    in_message = False\n",
        "    assistant_cache: List[str] = []\n",
        "    final_seen = False\n",
        "    any_text_emitted = False\n",
        "    raw_plain_chunks: List[str] = []\n",
        "\n",
        "    def canonical_channel(name: Optional[str]) -> str:\n",
        "        if not name:\n",
        "            return \"\"\n",
        "        lowered = name.strip().lower()\n",
        "        if not lowered:\n",
        "            return \"\"\n",
        "        if \"final\" in lowered:\n",
        "            return \"final\"\n",
        "        if \"assistant\" in lowered:\n",
        "            return \"assistant\"\n",
        "        return lowered\n",
        "\n",
        "    def emit_text(text: str):\n",
        "        nonlocal final_seen, assistant_cache, any_text_emitted\n",
        "        if not text:\n",
        "            return\n",
        "        channel = canonical_channel(current_channel)\n",
        "        if channel == \"final\":\n",
        "            final_seen = True\n",
        "            if assistant_cache:\n",
        "                cached = assistant_cache[:]\n",
        "                assistant_cache.clear()\n",
        "                for cached_piece in cached:\n",
        "                    if cached_piece:\n",
        "                        any_text_emitted = True\n",
        "                        yield cached_piece\n",
        "            any_text_emitted = True\n",
        "            yield text\n",
        "        elif channel == \"assistant\":\n",
        "            if final_seen:\n",
        "                any_text_emitted = True\n",
        "                yield text\n",
        "            else:\n",
        "                assistant_cache.append(text)\n",
        "        elif final_seen:\n",
        "            any_text_emitted = True\n",
        "            yield text\n",
        "\n",
        "    for piece in text_chunks:\n",
        "        if not piece:\n",
        "            continue\n",
        "        raw_plain_chunks.append(piece)\n",
        "        buffer += piece\n",
        "        while True:\n",
        "            if pending_channel:\n",
        "                idx = buffer.find(\"<|\")\n",
        "                if idx == -1:\n",
        "                    channel_name_buffer += buffer\n",
        "                    buffer = \"\"\n",
        "                    break\n",
        "                channel_name_buffer += buffer[:idx]\n",
        "                buffer = buffer[idx:]\n",
        "                channel = channel_name_buffer.strip()\n",
        "                channel_name_buffer = \"\"\n",
        "                pending_channel = False\n",
        "                current_channel = channel\n",
        "                channel_canonical = canonical_channel(current_channel)\n",
        "                in_message = bool(channel_canonical in {\"assistant\", \"final\"})\n",
        "                continue\n",
        "            tag_start = buffer.find(\"<|\")\n",
        "            if tag_start == -1:\n",
        "                if in_message or canonical_channel(current_channel) in {\"assistant\", \"final\"}:\n",
        "                    for out in emit_text(buffer):\n",
        "                        yield out\n",
        "                buffer = \"\"\n",
        "                break\n",
        "            if tag_start > 0:\n",
        "                text = buffer[:tag_start]\n",
        "                if in_message or canonical_channel(current_channel) in {\"assistant\", \"final\"}:\n",
        "                    for out in emit_text(text):\n",
        "                        yield out\n",
        "                buffer = buffer[tag_start:]\n",
        "            tag_end = buffer.find(\"|>\")\n",
        "            if tag_end == -1:\n",
        "                break\n",
        "            tag = buffer[2:tag_end].strip().lower()\n",
        "            buffer = buffer[tag_end + 2:]\n",
        "            if tag == \"start\":\n",
        "                current_channel = None\n",
        "                in_message = False\n",
        "            elif tag == \"channel\":\n",
        "                pending_channel = True\n",
        "            elif tag == \"message\":\n",
        "                in_message = True\n",
        "            elif tag == \"end\":\n",
        "                in_message = False\n",
        "                current_channel = None\n",
        "            elif tag == \"return\":\n",
        "                if not final_seen and assistant_cache:\n",
        "                    for cached_piece in assistant_cache:\n",
        "                        if cached_piece:\n",
        "                            any_text_emitted = True\n",
        "                            yield cached_piece\n",
        "                    assistant_cache.clear()\n",
        "                return\n",
        "            else:\n",
        "                continue\n",
        "    if (in_message or canonical_channel(current_channel) in {\"assistant\", \"final\"}) and buffer:\n",
        "        for out in emit_text(buffer):\n",
        "            yield out\n",
        "    if not final_seen and assistant_cache:\n",
        "        for cached_piece in assistant_cache:\n",
        "            if cached_piece:\n",
        "                any_text_emitted = True\n",
        "                yield cached_piece\n",
        "    if not any_text_emitted and raw_plain_chunks:\n",
        "        fallback_text = \"\".join(raw_plain_chunks).strip()\n",
        "        if fallback_text:\n",
        "            STREAM_FALLBACK_USED = True\n",
        "            fallback_text = clean_harmony_artifacts(fallback_text)\n",
        "            print(\"stream-flush fallback used\")\n",
        "            yield fallback_text\n",
        "\n",
        "\n",
        "def llm_stream(messages, max_tokens, *, repeat_penalty_override=None):\n",
        "    \"\"\"Stream Harmony-formatted completions and yield only the final channel.\"\"\"\n",
        "    if 'llm' not in globals() or llm is None:\n",
        "        raise RuntimeError(\"LLM model is not loaded. Cannot stream generation.\")\n",
        "    formatter = ensure_harmony_formatter()\n",
        "    chat_response = formatter(\n",
        "        messages=messages,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    completion_kwargs = dict(\n",
        "        prompt=chat_response.prompt,\n",
        "        temperature=float(temperature),\n",
        "        top_p=float(top_p),\n",
        "        repeat_penalty=float(repeat_penalty_override if repeat_penalty_override is not None else repeat_penalty),\n",
        "        max_tokens=int(max_tokens),\n",
        "        stream=True,\n",
        "    )\n",
        "    if chat_response.stop:\n",
        "        completion_kwargs[\"stop\"] = chat_response.stop\n",
        "    if chat_response.stopping_criteria is not None:\n",
        "        completion_kwargs[\"stopping_criteria\"] = chat_response.stopping_criteria\n",
        "\n",
        "    gen = llm.create_completion(**completion_kwargs)\n",
        "\n",
        "    def _iter_text_stream(events):\n",
        "        for ev in events:\n",
        "            yield ev.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
        "\n",
        "    text_stream = _iter_text_stream(gen)\n",
        "\n",
        "    for final_piece in stream_harmony_final_pieces(text_stream):\n",
        "        if final_piece:\n",
        "            yield final_piece\n",
        "\n",
        "# ===== Summary 5/6) Segment Summary (map) - Uses map_max_new_tokens, ctx_window, prompt_overhead, topic_hint\n",
        "if DEBUG_MODE: print(\"[Summary 5/6] Segment summarization (map) ...\")\n",
        "maps: List[str] = []\n",
        "map_stats: List[dict] = []\n",
        "map_debug_payload: List[dict] = []\n",
        "\n",
        "\n",
        "def escape_braces(text: str) -> str:\n",
        "    \"\"\"Escape braces so str.format does not treat user content as placeholders.\"\"\"\n",
        "    return text.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "\n",
        "\n",
        "def format_timestamp(seconds: float) -> str:\n",
        "    seconds = max(0, int(seconds))\n",
        "    h, rem = divmod(seconds, 3600)\n",
        "    m, s = divmod(rem, 60)\n",
        "    if h:\n",
        "        return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
        "    return f\"{m:02d}:{s:02d}\"\n",
        "\n",
        "\n",
        "safe_topic_hint = escape_braces(topic_hint or \"（無）\")\n",
        "map_generation_limit = map_max_new_tokens\n",
        "if len(chunks) > 6:\n",
        "    map_generation_limit = min(map_generation_limit, 600)\n",
        "    if DEBUG_MODE:\n",
        "        print(f\"→ 動態調整 map_max_new_tokens → {map_generation_limit}\")\n",
        "\n",
        "for i, (s, e, body) in enumerate(chunks, 1):\n",
        "    pct = i / max(len(chunks), 1) * 100\n",
        "    print(f\"  - 處理分段 {i}/{len(chunks)}（~{pct:.1f}%）\")\n",
        "\n",
        "    budget_tokens = max(512, ctx_window - map_generation_limit - prompt_overhead)\n",
        "    def shrink_to_budget(text: str, budget_tokens: int) -> str:\n",
        "        cur = text\n",
        "        for _ in range(6):\n",
        "            if count_tokens_text(cur) <= budget_tokens:\n",
        "                return cur\n",
        "            keep = max(800, int(len(cur) * 0.85))\n",
        "            cur = cur[:keep]\n",
        "        return cur\n",
        "    body2 = shrink_to_budget(body, budget_tokens)\n",
        "    input_tokens = count_tokens_text(body2)\n",
        "    input_chars = len(body2)\n",
        "\n",
        "    safe_body = escape_braces(body2)\n",
        "    user_txt = MAP_USER_TMPL.format(topic=safe_topic_hint, chunk=safe_body)\n",
        "    user_txt = user_txt.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_INSTR},\n",
        "        {\"role\": \"user\",   \"content\": user_txt},\n",
        "    ]\n",
        "\n",
        "    live = display(Markdown(\"\"), display_id=True)\n",
        "    part_buf: List[str] = []\n",
        "    for token in llm_stream(messages, map_generation_limit, repeat_penalty_override=map_repeat_penalty):\n",
        "        part_buf.append(token)\n",
        "        if len(part_buf) % 24 == 0:\n",
        "            cur_txt = \"\".join(part_buf)\n",
        "            live.update(Markdown(cur_txt))\n",
        "            print(f\"    ↳ 分段 {i} 已產生字元：{len(cur_txt)}\")\n",
        "    cur_txt = \"\".join(part_buf)\n",
        "    if STREAM_FALLBACK_USED:\n",
        "        cur_txt = clean_harmony_artifacts(cur_txt)\n",
        "    live.update(Markdown(cur_txt))\n",
        "    print(f\"    ↳ 分段 {i} 最終字元：{len(cur_txt)}\")\n",
        "\n",
        "    cleaned_txt = cur_txt.strip()\n",
        "    map_empty = not cleaned_txt\n",
        "    if map_empty:\n",
        "        cleaned_txt = f\"[片段 {i} 無輸出；時間 {format_timestamp(s)}–{format_timestamp(e)}]\"\n",
        "        print(f\"    ⚠️ 片段 {i} 無輸出，已寫入占位訊息\")\n",
        "\n",
        "    output_tokens = count_tokens_text(cleaned_txt)\n",
        "    try:\n",
        "        token_ids = llm.tokenize(cleaned_txt.encode(\"utf-8\"))\n",
        "    except Exception:\n",
        "        token_ids = []\n",
        "    unique_ratio = len(set(token_ids)) / max(1, len(token_ids))\n",
        "    map_entry = {\n",
        "        \"index\": i,\n",
        "        \"input_tokens\": input_tokens,\n",
        "        \"input_chars\": input_chars,\n",
        "        \"output_tokens\": output_tokens,\n",
        "        \"output_chars\": len(cleaned_txt),\n",
        "        \"hit_limit\": output_tokens >= map_generation_limit,\n",
        "        \"start\": s,\n",
        "        \"end\": e,\n",
        "        \"empty\": map_empty,\n",
        "        \"unique_token_ratio\": unique_ratio,\n",
        "        \"max_new_tokens\": map_generation_limit,\n",
        "    }\n",
        "    map_stats.append(map_entry)\n",
        "    map_debug_payload.append({\n",
        "        \"meta\": map_entry,\n",
        "        \"text\": cleaned_txt,\n",
        "    })\n",
        "    maps.append(cleaned_txt)\n",
        "\n",
        "print(f\"[Map Summary] chunks={len(chunks)} / maps={len(maps)}\")\n",
        "if repetition_reports:\n",
        "    for report in repetition_reports:\n",
        "        duration = report[\"duration\"]\n",
        "        start_ts = format_timestamp(report[\"start\"])\n",
        "        end_ts = format_timestamp(report[\"end\"])\n",
        "        print(\n",
        "            f\"  ↳ 重複壓縮：'{report['text']}' x{report['count']} | 時間 {start_ts}–{end_ts} | 持續 {duration:.1f}s\"\n",
        "        )\n",
        "\n",
        "for stat in map_stats:\n",
        "    print(\n",
        "        f\"  - Map {stat['index']:02d}: in={stat['input_tokens']} tok/{stat['input_chars']} chars, \"\n",
        "        f\"out={stat['output_tokens']} tok/{stat['output_chars']} chars, \"\n",
        "        f\"unique_ratio={stat['unique_token_ratio']:.2f}, hit_limit={stat['hit_limit']}, empty={stat['empty']}\"\n",
        "    )\n",
        "\n",
        "if DEBUG_MODE: print(\"→ Segment summarization complete\")\n",
        "\n",
        "summary_stem = Path(summary_srt_path_abs).stem\n",
        "debug_output_dir = out_base_dir\n",
        "debug_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "map_file_paths = []\n",
        "\n",
        "maps_md_parts = []\n",
        "for payload in map_debug_payload:\n",
        "    meta = payload[\"meta\"]\n",
        "    text_body = payload[\"text\"]\n",
        "    seg_header = (\n",
        "        f\"[SEG {meta['index']} | t≈{format_timestamp(meta['start'])}–{format_timestamp(meta['end'])} | \"\n",
        "        f\"len≈{meta['output_tokens']} tok]\"\n",
        "    )\n",
        "    maps_md_parts.append(f\"{seg_header}\\n### 片段 {meta['index']} 要點\\n\\n{text_body}\")\n",
        "    map_path = debug_output_dir / f\"{summary_stem}_map_{meta['index']:02d}.md\"\n",
        "    header_lines = [\n",
        "        f\"# 片段 {meta['index']:02d} | 時間 {format_timestamp(meta['start'])}–{format_timestamp(meta['end'])}\",\n",
        "        f\"> in={meta['input_tokens']} tok/{meta['input_chars']} chars | out={meta['output_tokens']} tok/{meta['output_chars']} chars | unique_ratio={meta['unique_token_ratio']:.2f}\",\n",
        "        \"\",\n",
        "    ]\n",
        "    map_path.write_text(\"\\n\".join(header_lines) + text_body + \"\\n\", encoding=\"utf-8\")\n",
        "    map_file_paths.append(map_path)\n",
        "\n",
        "maps_md_with_headers = \"\\n\\n---\\n\\n\".join(maps_md_parts)\n",
        "reduce_input_path = debug_output_dir / f\"{summary_stem}_reduce_input.md\"\n",
        "reduce_input_content = maps_md_with_headers\n",
        "if repetition_reports:\n",
        "    diag_lines = [\"\", \"<!-- 重複洪水壓縮原始片段 -->\"]\n",
        "    for report in repetition_reports:\n",
        "        diag_lines.append(\n",
        "            f\"<!-- {format_timestamp(report['start'])}–{format_timestamp(report['end'])} | '{report['text']}' x{report['count']} | {report['duration']:.1f}s -->\"\n",
        "        )\n",
        "        for sample in report.get('samples', []):\n",
        "            diag_lines.append(\n",
        "                f\"<!--   sample {format_timestamp(sample['start'])}–{format_timestamp(sample['end'])}: {sample['text']} -->\"\n",
        "            )\n",
        "    reduce_input_content += \"\\n\".join(diag_lines)\n",
        "reduce_input_path.write_text(reduce_input_content + \"\\n\", encoding=\"utf-8\")\n",
        "print(f\"→ Map 輸出已寫入 {len(map_file_paths)} 段；reduce_input: {reduce_input_path}\")\n",
        "\n",
        "if DEBUG_MODE: print(\"[Summary 6/6] Consolidating summary (reduce) ...\")\n",
        "\n",
        "# If combined text exceeds window, truncate proportionally first (without changing text within segments to avoid breaking meaning)\n",
        "def fit_reduce_payload(md_text: str, max_ctx_tokens: int) -> str:\n",
        "    for _ in range(8):\n",
        "        need = count_tokens_text(md_text)\n",
        "        if need + reduce_max_new_tokens + 400 <= max_ctx_tokens:\n",
        "            return md_text\n",
        "        md_text = md_text[: int(len(md_text) * 0.9)]\n",
        "    return md_text\n",
        "\n",
        "md_cur = fit_reduce_payload(maps_md_with_headers, ctx_window)\n",
        "\n",
        "safe_md_cur = escape_braces(md_cur)\n",
        "user_txt = REDUCE_USER_TMPL.format(topic=safe_topic_hint, maps=safe_md_cur)\n",
        "user_txt = user_txt.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
        "messages = [{\"role\":\"system\",\"content\":SYSTEM_INSTR},\n",
        "            {\"role\":\"user\",\"content\":user_txt}]\n",
        "\n",
        "live2 = display(Markdown(\"\"), display_id=True)\n",
        "final_buf = []\n",
        "for token in llm_stream(messages, reduce_max_new_tokens, repeat_penalty_override=reduce_repeat_penalty):\n",
        "    final_buf.append(token)\n",
        "    if len(final_buf) % 24 == 0:\n",
        "        current_text = \"\".join(final_buf)\n",
        "        live2.update(Markdown(current_text))\n",
        "        print(f\"    ↳ 彙整 已產生字元：{len(current_text)}\")\n",
        "live2.update(Markdown(\"\".join(final_buf)))\n",
        "print(f\"    ↳ 彙整 最終字元：{len(\"\".join(final_buf))}\")\n",
        "\n",
        "final_text = \"\".join(final_buf).strip()\n",
        "if STREAM_FALLBACK_USED:\n",
        "    final_text = clean_harmony_artifacts(final_text)\n",
        "reduce_input_tokens = count_tokens_text(md_cur)\n",
        "reduce_output_tokens = count_tokens_text(final_text)\n",
        "reduce_hit_limit = reduce_output_tokens >= reduce_max_new_tokens\n",
        "\n",
        "missing_segments = []\n",
        "for payload in map_debug_payload:\n",
        "    meta = payload[\"meta\"]\n",
        "    idx = meta[\"index\"]\n",
        "    patterns = [\n",
        "        rf\"片段\\s*{idx}\\b\",\n",
        "        re.escape(format_timestamp(meta[\"start\"])),\n",
        "        re.escape(format_timestamp(meta[\"end\"])),\n",
        "    ]\n",
        "    if not any(re.search(pattern, final_text) for pattern in patterns):\n",
        "        missing_segments.append(idx)\n",
        "if missing_segments:\n",
        "    print(f\"⚠️ 章節覆蓋檢查：缺少片段 {', '.join(str(i) for i in missing_segments)}\")\n",
        "else:\n",
        "    print(\"→ 章節覆蓋檢查通過（所有片段皆被提及）\")\n",
        "\n",
        "# Determine and create the summary output directory\n",
        "summary_output_dir_abs = out_base_dir\n",
        "summary_output_dir_abs.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Determine the summary output file path using the stem of the input SRT\n",
        "out_md = summary_output_dir_abs / f\"{Path(summary_srt_path_abs).stem}_summary.md\"\n",
        "\n",
        "\n",
        "with open(out_md, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(final_text)\n",
        "\n",
        "print(f\"→ 完成 ✅  {out_md}\")\n",
        "print(\"[Summary Metrics]\")\n",
        "print(f\"→ ctx_window candidates: {CTX_WINDOW_CANDIDATES} | selected: {ctx_window}\")\n",
        "print(f\"→ Segments: {len(chunks)} | Maps: {len(maps)}\")\n",
        "for stat in map_stats:\n",
        "    ratio = stat[\"output_tokens\"] / max(1, stat.get(\"max_new_tokens\", map_max_new_tokens))\n",
        "    print(\n",
        "        f\"  - Map {stat['index']:02d}: in={stat['input_tokens']} tok/{stat['input_chars']} chars, \"\n",
        "        f\"out={stat['output_tokens']} tok/{stat['output_chars']} chars, \"\n",
        "        f\"unique_ratio={stat['unique_token_ratio']:.2f}, hit_limit={stat['hit_limit']}, empty={stat['empty']} (ratio={ratio:.2f})\"\n",
        "    )\n",
        "print(\n",
        "    f\"  ↳ Reduce: in={reduce_input_tokens} tok, out={reduce_output_tokens} tok, \"\n",
        "    f\"hit_limit={reduce_hit_limit} (ratio={reduce_output_tokens / max(1, reduce_max_new_tokens):.2f})\"\n",
        ")\n",
        "if missing_segments:\n",
        "    print(f\"  ↳ Reduce coverage warning: 未提及片段 {missing_segments}\")\n",
        "else:\n",
        "    print(\"  ↳ Reduce coverage OK (all segments referenced)\")\n",
        "if STREAM_FALLBACK_USED:\n",
        "    print(\"  ↳ stream-flush fallback used\")\n",
        "try:\n",
        "    del llm\n",
        "except Exception:\n",
        "    pass\n",
        "gc.collect()\n",
        "if DEBUG_MODE: print(\"（顯存已釋放，如需重跑可直接再次執行）\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
